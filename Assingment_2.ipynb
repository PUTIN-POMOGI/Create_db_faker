{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PUTIN-POMOGI/Create_db_faker/blob/master/Assingment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "M8ibZkd5Xlo5"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Instructions\n",
        "- Your submission should be the `.ipynb` file with your name,\n",
        "  like `YusufMesbah.ipynb`. it should include the answers to the questions in\n",
        "  markdown cells.\n",
        "- You are expected to follow the best practices for code writing and model\n",
        "training. Poor coding style will be penalized.\n",
        "- You are allowed to discuss ideas with your peers, but no sharing of code.\n",
        "Plagiarism in the code will result in failing. If you use code from the\n",
        "internet, cite it.\n",
        "- If the instructions seem vague, use common sense."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: ANN (30%)\n",
        "For this task, you are required to build a fully connect feed-forward ANN model\n",
        "for a multi-label regression problem.\n",
        "\n",
        "For the given data, you need do proper data preprocessing, design the ANN model,\n",
        "then fine-tune your model architecture (number of layers, number of neurons,\n",
        "activation function, learning rate, momentum, regularization).\n",
        "\n",
        "For evaluating your model, do $80/20$ train test split.\n",
        "\n",
        "### Data\n",
        "You will be working with the data in `Task 1.csv` for predicting students'\n",
        "scores in 3 different exams: math, reading and writing. The columns include:\n",
        " - gender\n",
        " - race\n",
        " - parental level of education\n",
        " - lunch meal plan at school\n",
        " - whether the student undertook the test preparation course"
      ],
      "metadata": {
        "collapsed": false,
        "id": "btXQudxCXlo9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vPM6d9KCXlo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Task 1.csv')\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "df['gender'] = encoder.fit_transform(pd.DataFrame(df['gender']))\n",
        "\n",
        "encoder = OrdinalEncoder()\n",
        "df['race/ethnicity'] = encoder.fit_transform(df['race/ethnicity'].values.reshape(-1,1))\n",
        "df['parental level of education'] = encoder.fit_transform(df['parental level of education'].values.reshape(-1,1))\n",
        "df['lunch'] = encoder.fit_transform(df['lunch'].values.reshape(-1,1))\n",
        "df['test preparation course'] = encoder.fit_transform(df['test preparation course'].values.reshape(-1,1))\n",
        "\n",
        "scaler= StandardScaler()\n",
        "df[['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']]=scaler.fit_transform(df[['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "1afl-H1HX2Mg",
        "outputId": "9ceef2f1-20e9-42f0-8086-8f80f18dd93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       gender  race/ethnicity  parental level of education     lunch  \\\n",
              "0   -0.966559       -1.909268                    -0.314310  0.730577   \n",
              "1    1.034598        0.741266                     1.342861 -1.368782   \n",
              "2   -0.966559        1.624777                     0.790470 -1.368782   \n",
              "3   -0.966559       -1.025756                    -0.314310  0.730577   \n",
              "4   -0.966559        1.624777                    -1.419090  0.730577   \n",
              "..        ...             ...                          ...       ...   \n",
              "995 -0.966559       -0.142245                    -0.314310  0.730577   \n",
              "996 -0.966559        0.741266                    -1.419090 -1.368782   \n",
              "997  1.034598       -0.142245                     1.342861 -1.368782   \n",
              "998  1.034598       -0.142245                     0.790470  0.730577   \n",
              "999 -0.966559       -1.909268                     0.790470  0.730577   \n",
              "\n",
              "     test preparation course  math score  reading score  writing score  \n",
              "0                  -1.408927          67             67             63  \n",
              "1                   0.709760          40             59             55  \n",
              "2                   0.709760          59             60             50  \n",
              "3                   0.709760          77             78             68  \n",
              "4                  -1.408927          78             73             68  \n",
              "..                       ...         ...            ...            ...  \n",
              "995                 0.709760          73             70             65  \n",
              "996                -1.408927          85             91             92  \n",
              "997                 0.709760          32             35             41  \n",
              "998                 0.709760          73             74             82  \n",
              "999                -1.408927          65             60             62  \n",
              "\n",
              "[1000 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-652346c2-c695-487a-9f29-b6ca476c9132\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>race/ethnicity</th>\n",
              "      <th>parental level of education</th>\n",
              "      <th>lunch</th>\n",
              "      <th>test preparation course</th>\n",
              "      <th>math score</th>\n",
              "      <th>reading score</th>\n",
              "      <th>writing score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>-1.909268</td>\n",
              "      <td>-0.314310</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>-1.408927</td>\n",
              "      <td>67</td>\n",
              "      <td>67</td>\n",
              "      <td>63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.034598</td>\n",
              "      <td>0.741266</td>\n",
              "      <td>1.342861</td>\n",
              "      <td>-1.368782</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>40</td>\n",
              "      <td>59</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>1.624777</td>\n",
              "      <td>0.790470</td>\n",
              "      <td>-1.368782</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>-1.025756</td>\n",
              "      <td>-0.314310</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>77</td>\n",
              "      <td>78</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>1.624777</td>\n",
              "      <td>-1.419090</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>-1.408927</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>-0.142245</td>\n",
              "      <td>-0.314310</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>73</td>\n",
              "      <td>70</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>0.741266</td>\n",
              "      <td>-1.419090</td>\n",
              "      <td>-1.368782</td>\n",
              "      <td>-1.408927</td>\n",
              "      <td>85</td>\n",
              "      <td>91</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1.034598</td>\n",
              "      <td>-0.142245</td>\n",
              "      <td>1.342861</td>\n",
              "      <td>-1.368782</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>32</td>\n",
              "      <td>35</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1.034598</td>\n",
              "      <td>-0.142245</td>\n",
              "      <td>0.790470</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>0.709760</td>\n",
              "      <td>73</td>\n",
              "      <td>74</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-0.966559</td>\n",
              "      <td>-1.909268</td>\n",
              "      <td>0.790470</td>\n",
              "      <td>0.730577</td>\n",
              "      <td>-1.408927</td>\n",
              "      <td>65</td>\n",
              "      <td>60</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-652346c2-c695-487a-9f29-b6ca476c9132')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-652346c2-c695-487a-9f29-b6ca476c9132 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-652346c2-c695-487a-9f29-b6ca476c9132');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustomData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        super().__init__()\n",
        "        self.y = torch.tensor(y).float()\n",
        "        self.X = torch.tensor(X).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx, :], self.y[idx, :]"
      ],
      "metadata": {
        "id": "ZpuhHz3ajMNj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batch_size=20\n",
        "\n",
        "\n",
        "y= df.iloc[:, -3:].values\n",
        "X= df.iloc[:, :-3].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=3)\n",
        "\n",
        "train_dataset = CustomData(X_train, y_train)\n",
        "test_dataset = CustomData(X_test, y_test) \n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "DoXdgi1Kblqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, label = next(iter(train_dataloader))\n",
        "label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I_YknuZthq7",
        "outputId": "e6eb02b9-29f5-4a94-a2b9-970a5ab2c0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, model=[]):\n",
        "        super(Net, self).__init__()\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fpqJ_m7Im7v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "functions = [nn.LeakyReLU()]\n",
        "layers = [5, 50, 3]\n",
        "insides =[]\n",
        "\n",
        "for i in range(len(layers)-2):\n",
        "  insides.append(nn.Linear(layers[i], layers[i+1]))\n",
        "  insides.append(functions[i])\n",
        "insides.append(nn.Linear(layers[-2], layers[-1]))\n",
        "\n",
        "\n",
        "model = Net(insides).to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "86c4Iw5Y15ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d506f4-afff-4425-e8b4-29b5db5d7074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=50, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Linear(in_features=50, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 0.01\n",
        "seed = 1\n",
        "log_interval = 2\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss(reduction='mean')"
      ],
      "metadata": {
        "id": "mdjeCQrgl7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train( model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data).squeeze() \n",
        "        loss = loss_fn(output, target) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "Roon5Orpbn-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data).squeeze() \n",
        "            test_loss += loss_fn(output, target).item() \n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
        "        test_loss))"
      ],
      "metadata": {
        "id": "UrmNW_s1b5Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    test(model, device, test_dataloader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzDfo3tkmQY5",
        "outputId": "948098a5-2a43-41b9-cb82-e2a2e27cf71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/700 (0%)]\tLoss: 5080.129883\n",
            "Train Epoch: 1 [40/700 (6%)]\tLoss: 4336.768555\n",
            "Train Epoch: 1 [80/700 (11%)]\tLoss: 4887.481445\n",
            "Train Epoch: 1 [120/700 (17%)]\tLoss: 3748.533447\n",
            "Train Epoch: 1 [160/700 (23%)]\tLoss: 4673.793457\n",
            "Train Epoch: 1 [200/700 (29%)]\tLoss: 4569.145508\n",
            "Train Epoch: 1 [240/700 (34%)]\tLoss: 4203.434082\n",
            "Train Epoch: 1 [280/700 (40%)]\tLoss: 4612.157227\n",
            "Train Epoch: 1 [320/700 (46%)]\tLoss: 4764.685547\n",
            "Train Epoch: 1 [360/700 (51%)]\tLoss: 3467.738525\n",
            "Train Epoch: 1 [400/700 (57%)]\tLoss: 4235.203613\n",
            "Train Epoch: 1 [440/700 (63%)]\tLoss: 3637.958252\n",
            "Train Epoch: 1 [480/700 (69%)]\tLoss: 3889.682373\n",
            "Train Epoch: 1 [520/700 (74%)]\tLoss: 4416.789062\n",
            "Train Epoch: 1 [560/700 (80%)]\tLoss: 3698.027100\n",
            "Train Epoch: 1 [600/700 (86%)]\tLoss: 3537.059326\n",
            "Train Epoch: 1 [640/700 (91%)]\tLoss: 3187.570312\n",
            "Train Epoch: 1 [680/700 (97%)]\tLoss: 2951.754395\n",
            "\n",
            "Test set: Average loss: 171.6115\n",
            "\n",
            "Train Epoch: 2 [0/700 (0%)]\tLoss: 3403.845947\n",
            "Train Epoch: 2 [40/700 (6%)]\tLoss: 3185.508545\n",
            "Train Epoch: 2 [80/700 (11%)]\tLoss: 2888.620850\n",
            "Train Epoch: 2 [120/700 (17%)]\tLoss: 3213.472900\n",
            "Train Epoch: 2 [160/700 (23%)]\tLoss: 2628.923584\n",
            "Train Epoch: 2 [200/700 (29%)]\tLoss: 3552.986084\n",
            "Train Epoch: 2 [240/700 (34%)]\tLoss: 2797.800537\n",
            "Train Epoch: 2 [280/700 (40%)]\tLoss: 2684.770996\n",
            "Train Epoch: 2 [320/700 (46%)]\tLoss: 1916.040894\n",
            "Train Epoch: 2 [360/700 (51%)]\tLoss: 2194.199463\n",
            "Train Epoch: 2 [400/700 (57%)]\tLoss: 1959.932251\n",
            "Train Epoch: 2 [440/700 (63%)]\tLoss: 2080.383545\n",
            "Train Epoch: 2 [480/700 (69%)]\tLoss: 2319.382324\n",
            "Train Epoch: 2 [520/700 (74%)]\tLoss: 1285.568726\n",
            "Train Epoch: 2 [560/700 (80%)]\tLoss: 1002.674377\n",
            "Train Epoch: 2 [600/700 (86%)]\tLoss: 811.850952\n",
            "Train Epoch: 2 [640/700 (91%)]\tLoss: 706.311218\n",
            "Train Epoch: 2 [680/700 (97%)]\tLoss: 797.586670\n",
            "\n",
            "Test set: Average loss: 40.2726\n",
            "\n",
            "Train Epoch: 3 [0/700 (0%)]\tLoss: 745.170288\n",
            "Train Epoch: 3 [40/700 (6%)]\tLoss: 881.232971\n",
            "Train Epoch: 3 [80/700 (11%)]\tLoss: 617.713745\n",
            "Train Epoch: 3 [120/700 (17%)]\tLoss: 617.664673\n",
            "Train Epoch: 3 [160/700 (23%)]\tLoss: 596.524109\n",
            "Train Epoch: 3 [200/700 (29%)]\tLoss: 279.340881\n",
            "Train Epoch: 3 [240/700 (34%)]\tLoss: 223.786224\n",
            "Train Epoch: 3 [280/700 (40%)]\tLoss: 296.741547\n",
            "Train Epoch: 3 [320/700 (46%)]\tLoss: 442.005066\n",
            "Train Epoch: 3 [360/700 (51%)]\tLoss: 202.631165\n",
            "Train Epoch: 3 [400/700 (57%)]\tLoss: 249.241867\n",
            "Train Epoch: 3 [440/700 (63%)]\tLoss: 177.984665\n",
            "Train Epoch: 3 [480/700 (69%)]\tLoss: 197.526611\n",
            "Train Epoch: 3 [520/700 (74%)]\tLoss: 277.921478\n",
            "Train Epoch: 3 [560/700 (80%)]\tLoss: 189.137451\n",
            "Train Epoch: 3 [600/700 (86%)]\tLoss: 146.462448\n",
            "Train Epoch: 3 [640/700 (91%)]\tLoss: 148.193848\n",
            "Train Epoch: 3 [680/700 (97%)]\tLoss: 279.247620\n",
            "\n",
            "Test set: Average loss: 9.7179\n",
            "\n",
            "Train Epoch: 4 [0/700 (0%)]\tLoss: 290.190216\n",
            "Train Epoch: 4 [40/700 (6%)]\tLoss: 267.889801\n",
            "Train Epoch: 4 [80/700 (11%)]\tLoss: 165.265961\n",
            "Train Epoch: 4 [120/700 (17%)]\tLoss: 192.107513\n",
            "Train Epoch: 4 [160/700 (23%)]\tLoss: 187.153366\n",
            "Train Epoch: 4 [200/700 (29%)]\tLoss: 166.225189\n",
            "Train Epoch: 4 [240/700 (34%)]\tLoss: 192.809097\n",
            "Train Epoch: 4 [280/700 (40%)]\tLoss: 204.656143\n",
            "Train Epoch: 4 [320/700 (46%)]\tLoss: 175.837479\n",
            "Train Epoch: 4 [360/700 (51%)]\tLoss: 153.937057\n",
            "Train Epoch: 4 [400/700 (57%)]\tLoss: 187.890625\n",
            "Train Epoch: 4 [440/700 (63%)]\tLoss: 105.192360\n",
            "Train Epoch: 4 [480/700 (69%)]\tLoss: 194.752136\n",
            "Train Epoch: 4 [520/700 (74%)]\tLoss: 252.167130\n",
            "Train Epoch: 4 [560/700 (80%)]\tLoss: 189.267792\n",
            "Train Epoch: 4 [600/700 (86%)]\tLoss: 191.369675\n",
            "Train Epoch: 4 [640/700 (91%)]\tLoss: 198.032318\n",
            "Train Epoch: 4 [680/700 (97%)]\tLoss: 239.262497\n",
            "\n",
            "Test set: Average loss: 9.0468\n",
            "\n",
            "Train Epoch: 5 [0/700 (0%)]\tLoss: 246.505997\n",
            "Train Epoch: 5 [40/700 (6%)]\tLoss: 208.342850\n",
            "Train Epoch: 5 [80/700 (11%)]\tLoss: 105.110527\n",
            "Train Epoch: 5 [120/700 (17%)]\tLoss: 214.641357\n",
            "Train Epoch: 5 [160/700 (23%)]\tLoss: 135.259140\n",
            "Train Epoch: 5 [200/700 (29%)]\tLoss: 238.120789\n",
            "Train Epoch: 5 [240/700 (34%)]\tLoss: 251.071075\n",
            "Train Epoch: 5 [280/700 (40%)]\tLoss: 296.001465\n",
            "Train Epoch: 5 [320/700 (46%)]\tLoss: 192.153778\n",
            "Train Epoch: 5 [360/700 (51%)]\tLoss: 202.392838\n",
            "Train Epoch: 5 [400/700 (57%)]\tLoss: 174.005371\n",
            "Train Epoch: 5 [440/700 (63%)]\tLoss: 234.076004\n",
            "Train Epoch: 5 [480/700 (69%)]\tLoss: 177.899200\n",
            "Train Epoch: 5 [520/700 (74%)]\tLoss: 147.414291\n",
            "Train Epoch: 5 [560/700 (80%)]\tLoss: 149.600098\n",
            "Train Epoch: 5 [600/700 (86%)]\tLoss: 175.509048\n",
            "Train Epoch: 5 [640/700 (91%)]\tLoss: 114.797165\n",
            "Train Epoch: 5 [680/700 (97%)]\tLoss: 273.830994\n",
            "\n",
            "Test set: Average loss: 8.6843\n",
            "\n",
            "Train Epoch: 6 [0/700 (0%)]\tLoss: 244.167068\n",
            "Train Epoch: 6 [40/700 (6%)]\tLoss: 139.358459\n",
            "Train Epoch: 6 [80/700 (11%)]\tLoss: 134.867340\n",
            "Train Epoch: 6 [120/700 (17%)]\tLoss: 185.916122\n",
            "Train Epoch: 6 [160/700 (23%)]\tLoss: 231.350586\n",
            "Train Epoch: 6 [200/700 (29%)]\tLoss: 104.949707\n",
            "Train Epoch: 6 [240/700 (34%)]\tLoss: 219.530960\n",
            "Train Epoch: 6 [280/700 (40%)]\tLoss: 187.242569\n",
            "Train Epoch: 6 [320/700 (46%)]\tLoss: 149.033279\n",
            "Train Epoch: 6 [360/700 (51%)]\tLoss: 172.083008\n",
            "Train Epoch: 6 [400/700 (57%)]\tLoss: 159.841766\n",
            "Train Epoch: 6 [440/700 (63%)]\tLoss: 122.031418\n",
            "Train Epoch: 6 [480/700 (69%)]\tLoss: 207.523102\n",
            "Train Epoch: 6 [520/700 (74%)]\tLoss: 152.960114\n",
            "Train Epoch: 6 [560/700 (80%)]\tLoss: 233.803085\n",
            "Train Epoch: 6 [600/700 (86%)]\tLoss: 168.977448\n",
            "Train Epoch: 6 [640/700 (91%)]\tLoss: 241.426376\n",
            "Train Epoch: 6 [680/700 (97%)]\tLoss: 226.894562\n",
            "\n",
            "Test set: Average loss: 8.6002\n",
            "\n",
            "Train Epoch: 7 [0/700 (0%)]\tLoss: 173.396622\n",
            "Train Epoch: 7 [40/700 (6%)]\tLoss: 175.428635\n",
            "Train Epoch: 7 [80/700 (11%)]\tLoss: 186.457657\n",
            "Train Epoch: 7 [120/700 (17%)]\tLoss: 198.257339\n",
            "Train Epoch: 7 [160/700 (23%)]\tLoss: 117.899216\n",
            "Train Epoch: 7 [200/700 (29%)]\tLoss: 193.987411\n",
            "Train Epoch: 7 [240/700 (34%)]\tLoss: 156.757248\n",
            "Train Epoch: 7 [280/700 (40%)]\tLoss: 251.057678\n",
            "Train Epoch: 7 [320/700 (46%)]\tLoss: 167.750717\n",
            "Train Epoch: 7 [360/700 (51%)]\tLoss: 194.074173\n",
            "Train Epoch: 7 [400/700 (57%)]\tLoss: 233.329819\n",
            "Train Epoch: 7 [440/700 (63%)]\tLoss: 251.542908\n",
            "Train Epoch: 7 [480/700 (69%)]\tLoss: 190.331146\n",
            "Train Epoch: 7 [520/700 (74%)]\tLoss: 195.861710\n",
            "Train Epoch: 7 [560/700 (80%)]\tLoss: 196.854767\n",
            "Train Epoch: 7 [600/700 (86%)]\tLoss: 134.977097\n",
            "Train Epoch: 7 [640/700 (91%)]\tLoss: 267.275879\n",
            "Train Epoch: 7 [680/700 (97%)]\tLoss: 218.179779\n",
            "\n",
            "Test set: Average loss: 8.4408\n",
            "\n",
            "Train Epoch: 8 [0/700 (0%)]\tLoss: 98.247513\n",
            "Train Epoch: 8 [40/700 (6%)]\tLoss: 173.393768\n",
            "Train Epoch: 8 [80/700 (11%)]\tLoss: 202.459427\n",
            "Train Epoch: 8 [120/700 (17%)]\tLoss: 89.242142\n",
            "Train Epoch: 8 [160/700 (23%)]\tLoss: 247.905746\n",
            "Train Epoch: 8 [200/700 (29%)]\tLoss: 166.314148\n",
            "Train Epoch: 8 [240/700 (34%)]\tLoss: 187.455765\n",
            "Train Epoch: 8 [280/700 (40%)]\tLoss: 150.937408\n",
            "Train Epoch: 8 [320/700 (46%)]\tLoss: 195.729294\n",
            "Train Epoch: 8 [360/700 (51%)]\tLoss: 284.751434\n",
            "Train Epoch: 8 [400/700 (57%)]\tLoss: 115.432777\n",
            "Train Epoch: 8 [440/700 (63%)]\tLoss: 228.980392\n",
            "Train Epoch: 8 [480/700 (69%)]\tLoss: 170.711777\n",
            "Train Epoch: 8 [520/700 (74%)]\tLoss: 129.075256\n",
            "Train Epoch: 8 [560/700 (80%)]\tLoss: 182.769913\n",
            "Train Epoch: 8 [600/700 (86%)]\tLoss: 154.883118\n",
            "Train Epoch: 8 [640/700 (91%)]\tLoss: 274.204803\n",
            "Train Epoch: 8 [680/700 (97%)]\tLoss: 164.244843\n",
            "\n",
            "Test set: Average loss: 8.3800\n",
            "\n",
            "Train Epoch: 9 [0/700 (0%)]\tLoss: 209.404099\n",
            "Train Epoch: 9 [40/700 (6%)]\tLoss: 182.951248\n",
            "Train Epoch: 9 [80/700 (11%)]\tLoss: 126.039619\n",
            "Train Epoch: 9 [120/700 (17%)]\tLoss: 305.908691\n",
            "Train Epoch: 9 [160/700 (23%)]\tLoss: 176.313171\n",
            "Train Epoch: 9 [200/700 (29%)]\tLoss: 289.021515\n",
            "Train Epoch: 9 [240/700 (34%)]\tLoss: 110.995087\n",
            "Train Epoch: 9 [280/700 (40%)]\tLoss: 123.018562\n",
            "Train Epoch: 9 [320/700 (46%)]\tLoss: 161.645081\n",
            "Train Epoch: 9 [360/700 (51%)]\tLoss: 113.700974\n",
            "Train Epoch: 9 [400/700 (57%)]\tLoss: 127.220268\n",
            "Train Epoch: 9 [440/700 (63%)]\tLoss: 179.128479\n",
            "Train Epoch: 9 [480/700 (69%)]\tLoss: 182.636642\n",
            "Train Epoch: 9 [520/700 (74%)]\tLoss: 148.875092\n",
            "Train Epoch: 9 [560/700 (80%)]\tLoss: 133.245010\n",
            "Train Epoch: 9 [600/700 (86%)]\tLoss: 165.323151\n",
            "Train Epoch: 9 [640/700 (91%)]\tLoss: 286.033539\n",
            "Train Epoch: 9 [680/700 (97%)]\tLoss: 150.391388\n",
            "\n",
            "Test set: Average loss: 8.3408\n",
            "\n",
            "Train Epoch: 10 [0/700 (0%)]\tLoss: 174.474091\n",
            "Train Epoch: 10 [40/700 (6%)]\tLoss: 139.932907\n",
            "Train Epoch: 10 [80/700 (11%)]\tLoss: 141.102463\n",
            "Train Epoch: 10 [120/700 (17%)]\tLoss: 179.893509\n",
            "Train Epoch: 10 [160/700 (23%)]\tLoss: 103.712578\n",
            "Train Epoch: 10 [200/700 (29%)]\tLoss: 131.502609\n",
            "Train Epoch: 10 [240/700 (34%)]\tLoss: 212.683014\n",
            "Train Epoch: 10 [280/700 (40%)]\tLoss: 199.111191\n",
            "Train Epoch: 10 [320/700 (46%)]\tLoss: 165.621933\n",
            "Train Epoch: 10 [360/700 (51%)]\tLoss: 254.293289\n",
            "Train Epoch: 10 [400/700 (57%)]\tLoss: 197.224808\n",
            "Train Epoch: 10 [440/700 (63%)]\tLoss: 160.700714\n",
            "Train Epoch: 10 [480/700 (69%)]\tLoss: 226.824539\n",
            "Train Epoch: 10 [520/700 (74%)]\tLoss: 138.002243\n",
            "Train Epoch: 10 [560/700 (80%)]\tLoss: 237.407394\n",
            "Train Epoch: 10 [600/700 (86%)]\tLoss: 155.475189\n",
            "Train Epoch: 10 [640/700 (91%)]\tLoss: 165.347214\n",
            "Train Epoch: 10 [680/700 (97%)]\tLoss: 166.335632\n",
            "\n",
            "Test set: Average loss: 8.1162\n",
            "\n",
            "Train Epoch: 11 [0/700 (0%)]\tLoss: 187.440948\n",
            "Train Epoch: 11 [40/700 (6%)]\tLoss: 213.372299\n",
            "Train Epoch: 11 [80/700 (11%)]\tLoss: 149.471970\n",
            "Train Epoch: 11 [120/700 (17%)]\tLoss: 138.660004\n",
            "Train Epoch: 11 [160/700 (23%)]\tLoss: 184.851974\n",
            "Train Epoch: 11 [200/700 (29%)]\tLoss: 81.541908\n",
            "Train Epoch: 11 [240/700 (34%)]\tLoss: 223.526581\n",
            "Train Epoch: 11 [280/700 (40%)]\tLoss: 203.448807\n",
            "Train Epoch: 11 [320/700 (46%)]\tLoss: 252.535843\n",
            "Train Epoch: 11 [360/700 (51%)]\tLoss: 171.351334\n",
            "Train Epoch: 11 [400/700 (57%)]\tLoss: 188.205826\n",
            "Train Epoch: 11 [440/700 (63%)]\tLoss: 173.884674\n",
            "Train Epoch: 11 [480/700 (69%)]\tLoss: 207.787354\n",
            "Train Epoch: 11 [520/700 (74%)]\tLoss: 117.109253\n",
            "Train Epoch: 11 [560/700 (80%)]\tLoss: 224.597244\n",
            "Train Epoch: 11 [600/700 (86%)]\tLoss: 160.559372\n",
            "Train Epoch: 11 [640/700 (91%)]\tLoss: 177.554062\n",
            "Train Epoch: 11 [680/700 (97%)]\tLoss: 168.655930\n",
            "\n",
            "Test set: Average loss: 8.2071\n",
            "\n",
            "Train Epoch: 12 [0/700 (0%)]\tLoss: 172.261398\n",
            "Train Epoch: 12 [40/700 (6%)]\tLoss: 263.406128\n",
            "Train Epoch: 12 [80/700 (11%)]\tLoss: 130.305618\n",
            "Train Epoch: 12 [120/700 (17%)]\tLoss: 187.258591\n",
            "Train Epoch: 12 [160/700 (23%)]\tLoss: 200.067841\n",
            "Train Epoch: 12 [200/700 (29%)]\tLoss: 143.514130\n",
            "Train Epoch: 12 [240/700 (34%)]\tLoss: 247.479660\n",
            "Train Epoch: 12 [280/700 (40%)]\tLoss: 132.231949\n",
            "Train Epoch: 12 [320/700 (46%)]\tLoss: 120.755470\n",
            "Train Epoch: 12 [360/700 (51%)]\tLoss: 161.914841\n",
            "Train Epoch: 12 [400/700 (57%)]\tLoss: 161.553848\n",
            "Train Epoch: 12 [440/700 (63%)]\tLoss: 186.657135\n",
            "Train Epoch: 12 [480/700 (69%)]\tLoss: 142.262558\n",
            "Train Epoch: 12 [520/700 (74%)]\tLoss: 249.219513\n",
            "Train Epoch: 12 [560/700 (80%)]\tLoss: 162.663986\n",
            "Train Epoch: 12 [600/700 (86%)]\tLoss: 248.600357\n",
            "Train Epoch: 12 [640/700 (91%)]\tLoss: 177.548279\n",
            "Train Epoch: 12 [680/700 (97%)]\tLoss: 149.425949\n",
            "\n",
            "Test set: Average loss: 8.1980\n",
            "\n",
            "Train Epoch: 13 [0/700 (0%)]\tLoss: 129.012085\n",
            "Train Epoch: 13 [40/700 (6%)]\tLoss: 198.357437\n",
            "Train Epoch: 13 [80/700 (11%)]\tLoss: 124.140480\n",
            "Train Epoch: 13 [120/700 (17%)]\tLoss: 153.849472\n",
            "Train Epoch: 13 [160/700 (23%)]\tLoss: 236.356705\n",
            "Train Epoch: 13 [200/700 (29%)]\tLoss: 170.897781\n",
            "Train Epoch: 13 [240/700 (34%)]\tLoss: 119.049294\n",
            "Train Epoch: 13 [280/700 (40%)]\tLoss: 219.727478\n",
            "Train Epoch: 13 [320/700 (46%)]\tLoss: 239.138626\n",
            "Train Epoch: 13 [360/700 (51%)]\tLoss: 116.052315\n",
            "Train Epoch: 13 [400/700 (57%)]\tLoss: 140.884430\n",
            "Train Epoch: 13 [440/700 (63%)]\tLoss: 157.313644\n",
            "Train Epoch: 13 [480/700 (69%)]\tLoss: 245.654160\n",
            "Train Epoch: 13 [520/700 (74%)]\tLoss: 190.372055\n",
            "Train Epoch: 13 [560/700 (80%)]\tLoss: 179.687073\n",
            "Train Epoch: 13 [600/700 (86%)]\tLoss: 165.552399\n",
            "Train Epoch: 13 [640/700 (91%)]\tLoss: 196.013474\n",
            "Train Epoch: 13 [680/700 (97%)]\tLoss: 100.923187\n",
            "\n",
            "Test set: Average loss: 8.0742\n",
            "\n",
            "Train Epoch: 14 [0/700 (0%)]\tLoss: 182.338287\n",
            "Train Epoch: 14 [40/700 (6%)]\tLoss: 201.179428\n",
            "Train Epoch: 14 [80/700 (11%)]\tLoss: 133.957794\n",
            "Train Epoch: 14 [120/700 (17%)]\tLoss: 187.075119\n",
            "Train Epoch: 14 [160/700 (23%)]\tLoss: 99.819984\n",
            "Train Epoch: 14 [200/700 (29%)]\tLoss: 173.731339\n",
            "Train Epoch: 14 [240/700 (34%)]\tLoss: 128.097839\n",
            "Train Epoch: 14 [280/700 (40%)]\tLoss: 116.178871\n",
            "Train Epoch: 14 [320/700 (46%)]\tLoss: 188.164490\n",
            "Train Epoch: 14 [360/700 (51%)]\tLoss: 144.211426\n",
            "Train Epoch: 14 [400/700 (57%)]\tLoss: 174.662018\n",
            "Train Epoch: 14 [440/700 (63%)]\tLoss: 189.058044\n",
            "Train Epoch: 14 [480/700 (69%)]\tLoss: 174.349091\n",
            "Train Epoch: 14 [520/700 (74%)]\tLoss: 149.704895\n",
            "Train Epoch: 14 [560/700 (80%)]\tLoss: 225.547989\n",
            "Train Epoch: 14 [600/700 (86%)]\tLoss: 128.639359\n",
            "Train Epoch: 14 [640/700 (91%)]\tLoss: 190.862152\n",
            "Train Epoch: 14 [680/700 (97%)]\tLoss: 150.338394\n",
            "\n",
            "Test set: Average loss: 8.2581\n",
            "\n",
            "Train Epoch: 15 [0/700 (0%)]\tLoss: 91.176033\n",
            "Train Epoch: 15 [40/700 (6%)]\tLoss: 124.675941\n",
            "Train Epoch: 15 [80/700 (11%)]\tLoss: 174.854950\n",
            "Train Epoch: 15 [120/700 (17%)]\tLoss: 267.304718\n",
            "Train Epoch: 15 [160/700 (23%)]\tLoss: 147.552490\n",
            "Train Epoch: 15 [200/700 (29%)]\tLoss: 202.273224\n",
            "Train Epoch: 15 [240/700 (34%)]\tLoss: 74.948326\n",
            "Train Epoch: 15 [280/700 (40%)]\tLoss: 176.763504\n",
            "Train Epoch: 15 [320/700 (46%)]\tLoss: 221.337326\n",
            "Train Epoch: 15 [360/700 (51%)]\tLoss: 206.914810\n",
            "Train Epoch: 15 [400/700 (57%)]\tLoss: 195.635147\n",
            "Train Epoch: 15 [440/700 (63%)]\tLoss: 121.042625\n",
            "Train Epoch: 15 [480/700 (69%)]\tLoss: 226.811951\n",
            "Train Epoch: 15 [520/700 (74%)]\tLoss: 149.529327\n",
            "Train Epoch: 15 [560/700 (80%)]\tLoss: 287.040802\n",
            "Train Epoch: 15 [600/700 (86%)]\tLoss: 121.973061\n",
            "Train Epoch: 15 [640/700 (91%)]\tLoss: 210.870087\n",
            "Train Epoch: 15 [680/700 (97%)]\tLoss: 126.720383\n",
            "\n",
            "Test set: Average loss: 8.0485\n",
            "\n",
            "Train Epoch: 16 [0/700 (0%)]\tLoss: 205.303665\n",
            "Train Epoch: 16 [40/700 (6%)]\tLoss: 192.357132\n",
            "Train Epoch: 16 [80/700 (11%)]\tLoss: 185.415268\n",
            "Train Epoch: 16 [120/700 (17%)]\tLoss: 119.086655\n",
            "Train Epoch: 16 [160/700 (23%)]\tLoss: 160.580261\n",
            "Train Epoch: 16 [200/700 (29%)]\tLoss: 188.338196\n",
            "Train Epoch: 16 [240/700 (34%)]\tLoss: 207.250427\n",
            "Train Epoch: 16 [280/700 (40%)]\tLoss: 207.215546\n",
            "Train Epoch: 16 [320/700 (46%)]\tLoss: 114.523972\n",
            "Train Epoch: 16 [360/700 (51%)]\tLoss: 200.787125\n",
            "Train Epoch: 16 [400/700 (57%)]\tLoss: 140.143311\n",
            "Train Epoch: 16 [440/700 (63%)]\tLoss: 166.069199\n",
            "Train Epoch: 16 [480/700 (69%)]\tLoss: 187.749557\n",
            "Train Epoch: 16 [520/700 (74%)]\tLoss: 88.875359\n",
            "Train Epoch: 16 [560/700 (80%)]\tLoss: 212.687103\n",
            "Train Epoch: 16 [600/700 (86%)]\tLoss: 92.546059\n",
            "Train Epoch: 16 [640/700 (91%)]\tLoss: 277.929535\n",
            "Train Epoch: 16 [680/700 (97%)]\tLoss: 182.275726\n",
            "\n",
            "Test set: Average loss: 8.0029\n",
            "\n",
            "Train Epoch: 17 [0/700 (0%)]\tLoss: 163.041595\n",
            "Train Epoch: 17 [40/700 (6%)]\tLoss: 178.732605\n",
            "Train Epoch: 17 [80/700 (11%)]\tLoss: 123.474510\n",
            "Train Epoch: 17 [120/700 (17%)]\tLoss: 225.160934\n",
            "Train Epoch: 17 [160/700 (23%)]\tLoss: 148.242355\n",
            "Train Epoch: 17 [200/700 (29%)]\tLoss: 139.691574\n",
            "Train Epoch: 17 [240/700 (34%)]\tLoss: 192.005325\n",
            "Train Epoch: 17 [280/700 (40%)]\tLoss: 72.393509\n",
            "Train Epoch: 17 [320/700 (46%)]\tLoss: 177.681503\n",
            "Train Epoch: 17 [360/700 (51%)]\tLoss: 144.023331\n",
            "Train Epoch: 17 [400/700 (57%)]\tLoss: 163.865204\n",
            "Train Epoch: 17 [440/700 (63%)]\tLoss: 115.573341\n",
            "Train Epoch: 17 [480/700 (69%)]\tLoss: 145.495285\n",
            "Train Epoch: 17 [520/700 (74%)]\tLoss: 166.976074\n",
            "Train Epoch: 17 [560/700 (80%)]\tLoss: 131.760193\n",
            "Train Epoch: 17 [600/700 (86%)]\tLoss: 160.089874\n",
            "Train Epoch: 17 [640/700 (91%)]\tLoss: 210.541977\n",
            "Train Epoch: 17 [680/700 (97%)]\tLoss: 272.634491\n",
            "\n",
            "Test set: Average loss: 8.0575\n",
            "\n",
            "Train Epoch: 18 [0/700 (0%)]\tLoss: 181.228424\n",
            "Train Epoch: 18 [40/700 (6%)]\tLoss: 164.728287\n",
            "Train Epoch: 18 [80/700 (11%)]\tLoss: 245.951279\n",
            "Train Epoch: 18 [120/700 (17%)]\tLoss: 181.275879\n",
            "Train Epoch: 18 [160/700 (23%)]\tLoss: 164.246170\n",
            "Train Epoch: 18 [200/700 (29%)]\tLoss: 139.690933\n",
            "Train Epoch: 18 [240/700 (34%)]\tLoss: 140.381546\n",
            "Train Epoch: 18 [280/700 (40%)]\tLoss: 168.867081\n",
            "Train Epoch: 18 [320/700 (46%)]\tLoss: 268.871857\n",
            "Train Epoch: 18 [360/700 (51%)]\tLoss: 133.525787\n",
            "Train Epoch: 18 [400/700 (57%)]\tLoss: 121.828827\n",
            "Train Epoch: 18 [440/700 (63%)]\tLoss: 169.793671\n",
            "Train Epoch: 18 [480/700 (69%)]\tLoss: 133.993835\n",
            "Train Epoch: 18 [520/700 (74%)]\tLoss: 255.521271\n",
            "Train Epoch: 18 [560/700 (80%)]\tLoss: 137.871964\n",
            "Train Epoch: 18 [600/700 (86%)]\tLoss: 218.390274\n",
            "Train Epoch: 18 [640/700 (91%)]\tLoss: 111.055573\n",
            "Train Epoch: 18 [680/700 (97%)]\tLoss: 128.128693\n",
            "\n",
            "Test set: Average loss: 8.0369\n",
            "\n",
            "Train Epoch: 19 [0/700 (0%)]\tLoss: 175.208206\n",
            "Train Epoch: 19 [40/700 (6%)]\tLoss: 173.842545\n",
            "Train Epoch: 19 [80/700 (11%)]\tLoss: 163.277557\n",
            "Train Epoch: 19 [120/700 (17%)]\tLoss: 192.876648\n",
            "Train Epoch: 19 [160/700 (23%)]\tLoss: 111.784645\n",
            "Train Epoch: 19 [200/700 (29%)]\tLoss: 193.285675\n",
            "Train Epoch: 19 [240/700 (34%)]\tLoss: 201.168350\n",
            "Train Epoch: 19 [280/700 (40%)]\tLoss: 151.735062\n",
            "Train Epoch: 19 [320/700 (46%)]\tLoss: 123.464485\n",
            "Train Epoch: 19 [360/700 (51%)]\tLoss: 330.993774\n",
            "Train Epoch: 19 [400/700 (57%)]\tLoss: 216.107819\n",
            "Train Epoch: 19 [440/700 (63%)]\tLoss: 201.254730\n",
            "Train Epoch: 19 [480/700 (69%)]\tLoss: 173.497025\n",
            "Train Epoch: 19 [520/700 (74%)]\tLoss: 148.412048\n",
            "Train Epoch: 19 [560/700 (80%)]\tLoss: 151.990692\n",
            "Train Epoch: 19 [600/700 (86%)]\tLoss: 162.452606\n",
            "Train Epoch: 19 [640/700 (91%)]\tLoss: 246.011887\n",
            "Train Epoch: 19 [680/700 (97%)]\tLoss: 221.417633\n",
            "\n",
            "Test set: Average loss: 8.0811\n",
            "\n",
            "Train Epoch: 20 [0/700 (0%)]\tLoss: 150.605957\n",
            "Train Epoch: 20 [40/700 (6%)]\tLoss: 202.589447\n",
            "Train Epoch: 20 [80/700 (11%)]\tLoss: 126.929420\n",
            "Train Epoch: 20 [120/700 (17%)]\tLoss: 122.718903\n",
            "Train Epoch: 20 [160/700 (23%)]\tLoss: 155.610733\n",
            "Train Epoch: 20 [200/700 (29%)]\tLoss: 145.341766\n",
            "Train Epoch: 20 [240/700 (34%)]\tLoss: 128.644501\n",
            "Train Epoch: 20 [280/700 (40%)]\tLoss: 136.631058\n",
            "Train Epoch: 20 [320/700 (46%)]\tLoss: 194.542023\n",
            "Train Epoch: 20 [360/700 (51%)]\tLoss: 224.731277\n",
            "Train Epoch: 20 [400/700 (57%)]\tLoss: 111.799271\n",
            "Train Epoch: 20 [440/700 (63%)]\tLoss: 166.708893\n",
            "Train Epoch: 20 [480/700 (69%)]\tLoss: 169.218811\n",
            "Train Epoch: 20 [520/700 (74%)]\tLoss: 227.831848\n",
            "Train Epoch: 20 [560/700 (80%)]\tLoss: 164.383652\n",
            "Train Epoch: 20 [600/700 (86%)]\tLoss: 249.065323\n",
            "Train Epoch: 20 [640/700 (91%)]\tLoss: 185.880356\n",
            "Train Epoch: 20 [680/700 (97%)]\tLoss: 189.830780\n",
            "\n",
            "Test set: Average loss: 8.3038\n",
            "\n",
            "Train Epoch: 21 [0/700 (0%)]\tLoss: 243.976913\n",
            "Train Epoch: 21 [40/700 (6%)]\tLoss: 147.408875\n",
            "Train Epoch: 21 [80/700 (11%)]\tLoss: 192.615967\n",
            "Train Epoch: 21 [120/700 (17%)]\tLoss: 150.684982\n",
            "Train Epoch: 21 [160/700 (23%)]\tLoss: 300.597992\n",
            "Train Epoch: 21 [200/700 (29%)]\tLoss: 146.323639\n",
            "Train Epoch: 21 [240/700 (34%)]\tLoss: 296.149628\n",
            "Train Epoch: 21 [280/700 (40%)]\tLoss: 204.088791\n",
            "Train Epoch: 21 [320/700 (46%)]\tLoss: 190.161072\n",
            "Train Epoch: 21 [360/700 (51%)]\tLoss: 152.966721\n",
            "Train Epoch: 21 [400/700 (57%)]\tLoss: 222.696884\n",
            "Train Epoch: 21 [440/700 (63%)]\tLoss: 271.644257\n",
            "Train Epoch: 21 [480/700 (69%)]\tLoss: 166.168945\n",
            "Train Epoch: 21 [520/700 (74%)]\tLoss: 163.831085\n",
            "Train Epoch: 21 [560/700 (80%)]\tLoss: 152.187576\n",
            "Train Epoch: 21 [600/700 (86%)]\tLoss: 87.134842\n",
            "Train Epoch: 21 [640/700 (91%)]\tLoss: 131.890594\n",
            "Train Epoch: 21 [680/700 (97%)]\tLoss: 164.451004\n",
            "\n",
            "Test set: Average loss: 8.1072\n",
            "\n",
            "Train Epoch: 22 [0/700 (0%)]\tLoss: 137.652359\n",
            "Train Epoch: 22 [40/700 (6%)]\tLoss: 158.396561\n",
            "Train Epoch: 22 [80/700 (11%)]\tLoss: 146.451355\n",
            "Train Epoch: 22 [120/700 (17%)]\tLoss: 172.926529\n",
            "Train Epoch: 22 [160/700 (23%)]\tLoss: 150.573013\n",
            "Train Epoch: 22 [200/700 (29%)]\tLoss: 133.974426\n",
            "Train Epoch: 22 [240/700 (34%)]\tLoss: 251.426376\n",
            "Train Epoch: 22 [280/700 (40%)]\tLoss: 164.720413\n",
            "Train Epoch: 22 [320/700 (46%)]\tLoss: 229.737030\n",
            "Train Epoch: 22 [360/700 (51%)]\tLoss: 135.998535\n",
            "Train Epoch: 22 [400/700 (57%)]\tLoss: 133.093048\n",
            "Train Epoch: 22 [440/700 (63%)]\tLoss: 124.642776\n",
            "Train Epoch: 22 [480/700 (69%)]\tLoss: 242.748276\n",
            "Train Epoch: 22 [520/700 (74%)]\tLoss: 159.500351\n",
            "Train Epoch: 22 [560/700 (80%)]\tLoss: 198.341797\n",
            "Train Epoch: 22 [600/700 (86%)]\tLoss: 223.132965\n",
            "Train Epoch: 22 [640/700 (91%)]\tLoss: 198.642029\n",
            "Train Epoch: 22 [680/700 (97%)]\tLoss: 247.056885\n",
            "\n",
            "Test set: Average loss: 8.0872\n",
            "\n",
            "Train Epoch: 23 [0/700 (0%)]\tLoss: 242.673035\n",
            "Train Epoch: 23 [40/700 (6%)]\tLoss: 206.314713\n",
            "Train Epoch: 23 [80/700 (11%)]\tLoss: 216.862900\n",
            "Train Epoch: 23 [120/700 (17%)]\tLoss: 197.134506\n",
            "Train Epoch: 23 [160/700 (23%)]\tLoss: 187.606995\n",
            "Train Epoch: 23 [200/700 (29%)]\tLoss: 131.008408\n",
            "Train Epoch: 23 [240/700 (34%)]\tLoss: 216.255478\n",
            "Train Epoch: 23 [280/700 (40%)]\tLoss: 237.833466\n",
            "Train Epoch: 23 [320/700 (46%)]\tLoss: 161.543594\n",
            "Train Epoch: 23 [360/700 (51%)]\tLoss: 191.757538\n",
            "Train Epoch: 23 [400/700 (57%)]\tLoss: 161.025299\n",
            "Train Epoch: 23 [440/700 (63%)]\tLoss: 122.292450\n",
            "Train Epoch: 23 [480/700 (69%)]\tLoss: 196.027115\n",
            "Train Epoch: 23 [520/700 (74%)]\tLoss: 124.826790\n",
            "Train Epoch: 23 [560/700 (80%)]\tLoss: 235.700668\n",
            "Train Epoch: 23 [600/700 (86%)]\tLoss: 184.737961\n",
            "Train Epoch: 23 [640/700 (91%)]\tLoss: 185.360504\n",
            "Train Epoch: 23 [680/700 (97%)]\tLoss: 152.732498\n",
            "\n",
            "Test set: Average loss: 8.1167\n",
            "\n",
            "Train Epoch: 24 [0/700 (0%)]\tLoss: 121.038490\n",
            "Train Epoch: 24 [40/700 (6%)]\tLoss: 172.647522\n",
            "Train Epoch: 24 [80/700 (11%)]\tLoss: 136.203323\n",
            "Train Epoch: 24 [120/700 (17%)]\tLoss: 130.037445\n",
            "Train Epoch: 24 [160/700 (23%)]\tLoss: 127.930939\n",
            "Train Epoch: 24 [200/700 (29%)]\tLoss: 151.577118\n",
            "Train Epoch: 24 [240/700 (34%)]\tLoss: 188.603851\n",
            "Train Epoch: 24 [280/700 (40%)]\tLoss: 167.628906\n",
            "Train Epoch: 24 [320/700 (46%)]\tLoss: 194.401581\n",
            "Train Epoch: 24 [360/700 (51%)]\tLoss: 162.546539\n",
            "Train Epoch: 24 [400/700 (57%)]\tLoss: 200.501602\n",
            "Train Epoch: 24 [440/700 (63%)]\tLoss: 178.650543\n",
            "Train Epoch: 24 [480/700 (69%)]\tLoss: 112.384033\n",
            "Train Epoch: 24 [520/700 (74%)]\tLoss: 151.121277\n",
            "Train Epoch: 24 [560/700 (80%)]\tLoss: 171.599136\n",
            "Train Epoch: 24 [600/700 (86%)]\tLoss: 92.702972\n",
            "Train Epoch: 24 [640/700 (91%)]\tLoss: 215.702576\n",
            "Train Epoch: 24 [680/700 (97%)]\tLoss: 237.013748\n",
            "\n",
            "Test set: Average loss: 8.0257\n",
            "\n",
            "Train Epoch: 25 [0/700 (0%)]\tLoss: 127.790314\n",
            "Train Epoch: 25 [40/700 (6%)]\tLoss: 194.460785\n",
            "Train Epoch: 25 [80/700 (11%)]\tLoss: 207.928772\n",
            "Train Epoch: 25 [120/700 (17%)]\tLoss: 188.228516\n",
            "Train Epoch: 25 [160/700 (23%)]\tLoss: 173.549622\n",
            "Train Epoch: 25 [200/700 (29%)]\tLoss: 152.113907\n",
            "Train Epoch: 25 [240/700 (34%)]\tLoss: 208.733887\n",
            "Train Epoch: 25 [280/700 (40%)]\tLoss: 249.537125\n",
            "Train Epoch: 25 [320/700 (46%)]\tLoss: 156.258347\n",
            "Train Epoch: 25 [360/700 (51%)]\tLoss: 149.272202\n",
            "Train Epoch: 25 [400/700 (57%)]\tLoss: 129.531311\n",
            "Train Epoch: 25 [440/700 (63%)]\tLoss: 147.142029\n",
            "Train Epoch: 25 [480/700 (69%)]\tLoss: 90.761314\n",
            "Train Epoch: 25 [520/700 (74%)]\tLoss: 242.508698\n",
            "Train Epoch: 25 [560/700 (80%)]\tLoss: 114.272453\n",
            "Train Epoch: 25 [600/700 (86%)]\tLoss: 162.445206\n",
            "Train Epoch: 25 [640/700 (91%)]\tLoss: 186.758148\n",
            "Train Epoch: 25 [680/700 (97%)]\tLoss: 194.231750\n",
            "\n",
            "Test set: Average loss: 8.0683\n",
            "\n",
            "Train Epoch: 26 [0/700 (0%)]\tLoss: 318.277283\n",
            "Train Epoch: 26 [40/700 (6%)]\tLoss: 174.170090\n",
            "Train Epoch: 26 [80/700 (11%)]\tLoss: 261.725403\n",
            "Train Epoch: 26 [120/700 (17%)]\tLoss: 116.250320\n",
            "Train Epoch: 26 [160/700 (23%)]\tLoss: 155.542969\n",
            "Train Epoch: 26 [200/700 (29%)]\tLoss: 132.558777\n",
            "Train Epoch: 26 [240/700 (34%)]\tLoss: 175.174423\n",
            "Train Epoch: 26 [280/700 (40%)]\tLoss: 160.948105\n",
            "Train Epoch: 26 [320/700 (46%)]\tLoss: 155.098618\n",
            "Train Epoch: 26 [360/700 (51%)]\tLoss: 207.488983\n",
            "Train Epoch: 26 [400/700 (57%)]\tLoss: 145.262070\n",
            "Train Epoch: 26 [440/700 (63%)]\tLoss: 109.664558\n",
            "Train Epoch: 26 [480/700 (69%)]\tLoss: 133.318405\n",
            "Train Epoch: 26 [520/700 (74%)]\tLoss: 188.249466\n",
            "Train Epoch: 26 [560/700 (80%)]\tLoss: 172.089737\n",
            "Train Epoch: 26 [600/700 (86%)]\tLoss: 168.996765\n",
            "Train Epoch: 26 [640/700 (91%)]\tLoss: 110.182953\n",
            "Train Epoch: 26 [680/700 (97%)]\tLoss: 206.260696\n",
            "\n",
            "Test set: Average loss: 8.1783\n",
            "\n",
            "Train Epoch: 27 [0/700 (0%)]\tLoss: 162.104202\n",
            "Train Epoch: 27 [40/700 (6%)]\tLoss: 222.510117\n",
            "Train Epoch: 27 [80/700 (11%)]\tLoss: 196.728180\n",
            "Train Epoch: 27 [120/700 (17%)]\tLoss: 134.054688\n",
            "Train Epoch: 27 [160/700 (23%)]\tLoss: 101.784378\n",
            "Train Epoch: 27 [200/700 (29%)]\tLoss: 130.246475\n",
            "Train Epoch: 27 [240/700 (34%)]\tLoss: 129.001144\n",
            "Train Epoch: 27 [280/700 (40%)]\tLoss: 184.969086\n",
            "Train Epoch: 27 [320/700 (46%)]\tLoss: 270.836517\n",
            "Train Epoch: 27 [360/700 (51%)]\tLoss: 211.330078\n",
            "Train Epoch: 27 [400/700 (57%)]\tLoss: 212.205292\n",
            "Train Epoch: 27 [440/700 (63%)]\tLoss: 204.778702\n",
            "Train Epoch: 27 [480/700 (69%)]\tLoss: 163.873093\n",
            "Train Epoch: 27 [520/700 (74%)]\tLoss: 218.251404\n",
            "Train Epoch: 27 [560/700 (80%)]\tLoss: 131.436569\n",
            "Train Epoch: 27 [600/700 (86%)]\tLoss: 112.850143\n",
            "Train Epoch: 27 [640/700 (91%)]\tLoss: 179.315735\n",
            "Train Epoch: 27 [680/700 (97%)]\tLoss: 144.260162\n",
            "\n",
            "Test set: Average loss: 8.1540\n",
            "\n",
            "Train Epoch: 28 [0/700 (0%)]\tLoss: 138.689606\n",
            "Train Epoch: 28 [40/700 (6%)]\tLoss: 139.646011\n",
            "Train Epoch: 28 [80/700 (11%)]\tLoss: 171.332230\n",
            "Train Epoch: 28 [120/700 (17%)]\tLoss: 248.669937\n",
            "Train Epoch: 28 [160/700 (23%)]\tLoss: 159.134003\n",
            "Train Epoch: 28 [200/700 (29%)]\tLoss: 169.586884\n",
            "Train Epoch: 28 [240/700 (34%)]\tLoss: 139.099701\n",
            "Train Epoch: 28 [280/700 (40%)]\tLoss: 136.490341\n",
            "Train Epoch: 28 [320/700 (46%)]\tLoss: 244.232239\n",
            "Train Epoch: 28 [360/700 (51%)]\tLoss: 137.080353\n",
            "Train Epoch: 28 [400/700 (57%)]\tLoss: 197.030701\n",
            "Train Epoch: 28 [440/700 (63%)]\tLoss: 174.493912\n",
            "Train Epoch: 28 [480/700 (69%)]\tLoss: 113.854950\n",
            "Train Epoch: 28 [520/700 (74%)]\tLoss: 142.189209\n",
            "Train Epoch: 28 [560/700 (80%)]\tLoss: 155.257614\n",
            "Train Epoch: 28 [600/700 (86%)]\tLoss: 116.770744\n",
            "Train Epoch: 28 [640/700 (91%)]\tLoss: 125.078598\n",
            "Train Epoch: 28 [680/700 (97%)]\tLoss: 167.190384\n",
            "\n",
            "Test set: Average loss: 8.0866\n",
            "\n",
            "Train Epoch: 29 [0/700 (0%)]\tLoss: 167.114685\n",
            "Train Epoch: 29 [40/700 (6%)]\tLoss: 165.172363\n",
            "Train Epoch: 29 [80/700 (11%)]\tLoss: 207.288864\n",
            "Train Epoch: 29 [120/700 (17%)]\tLoss: 119.096878\n",
            "Train Epoch: 29 [160/700 (23%)]\tLoss: 184.521255\n",
            "Train Epoch: 29 [200/700 (29%)]\tLoss: 172.457596\n",
            "Train Epoch: 29 [240/700 (34%)]\tLoss: 133.067291\n",
            "Train Epoch: 29 [280/700 (40%)]\tLoss: 209.849808\n",
            "Train Epoch: 29 [320/700 (46%)]\tLoss: 133.940018\n",
            "Train Epoch: 29 [360/700 (51%)]\tLoss: 121.672081\n",
            "Train Epoch: 29 [400/700 (57%)]\tLoss: 200.367325\n",
            "Train Epoch: 29 [440/700 (63%)]\tLoss: 212.397324\n",
            "Train Epoch: 29 [480/700 (69%)]\tLoss: 181.224701\n",
            "Train Epoch: 29 [520/700 (74%)]\tLoss: 212.931854\n",
            "Train Epoch: 29 [560/700 (80%)]\tLoss: 80.417984\n",
            "Train Epoch: 29 [600/700 (86%)]\tLoss: 145.926956\n",
            "Train Epoch: 29 [640/700 (91%)]\tLoss: 204.074921\n",
            "Train Epoch: 29 [680/700 (97%)]\tLoss: 118.371452\n",
            "\n",
            "Test set: Average loss: 8.0645\n",
            "\n",
            "Train Epoch: 30 [0/700 (0%)]\tLoss: 185.881195\n",
            "Train Epoch: 30 [40/700 (6%)]\tLoss: 164.001022\n",
            "Train Epoch: 30 [80/700 (11%)]\tLoss: 150.332733\n",
            "Train Epoch: 30 [120/700 (17%)]\tLoss: 202.508698\n",
            "Train Epoch: 30 [160/700 (23%)]\tLoss: 164.652374\n",
            "Train Epoch: 30 [200/700 (29%)]\tLoss: 135.376541\n",
            "Train Epoch: 30 [240/700 (34%)]\tLoss: 118.285301\n",
            "Train Epoch: 30 [280/700 (40%)]\tLoss: 144.513733\n",
            "Train Epoch: 30 [320/700 (46%)]\tLoss: 141.547287\n",
            "Train Epoch: 30 [360/700 (51%)]\tLoss: 207.529648\n",
            "Train Epoch: 30 [400/700 (57%)]\tLoss: 146.521637\n",
            "Train Epoch: 30 [440/700 (63%)]\tLoss: 77.518524\n",
            "Train Epoch: 30 [480/700 (69%)]\tLoss: 220.197327\n",
            "Train Epoch: 30 [520/700 (74%)]\tLoss: 153.483231\n",
            "Train Epoch: 30 [560/700 (80%)]\tLoss: 186.143158\n",
            "Train Epoch: 30 [600/700 (86%)]\tLoss: 178.170563\n",
            "Train Epoch: 30 [640/700 (91%)]\tLoss: 165.895966\n",
            "Train Epoch: 30 [680/700 (97%)]\tLoss: 191.057556\n",
            "\n",
            "Test set: Average loss: 8.2352\n",
            "\n",
            "Train Epoch: 31 [0/700 (0%)]\tLoss: 108.605621\n",
            "Train Epoch: 31 [40/700 (6%)]\tLoss: 190.120956\n",
            "Train Epoch: 31 [80/700 (11%)]\tLoss: 222.018707\n",
            "Train Epoch: 31 [120/700 (17%)]\tLoss: 263.105743\n",
            "Train Epoch: 31 [160/700 (23%)]\tLoss: 237.646210\n",
            "Train Epoch: 31 [200/700 (29%)]\tLoss: 130.199173\n",
            "Train Epoch: 31 [240/700 (34%)]\tLoss: 98.921158\n",
            "Train Epoch: 31 [280/700 (40%)]\tLoss: 103.473709\n",
            "Train Epoch: 31 [320/700 (46%)]\tLoss: 200.482483\n",
            "Train Epoch: 31 [360/700 (51%)]\tLoss: 244.936386\n",
            "Train Epoch: 31 [400/700 (57%)]\tLoss: 176.100754\n",
            "Train Epoch: 31 [440/700 (63%)]\tLoss: 203.719666\n",
            "Train Epoch: 31 [480/700 (69%)]\tLoss: 191.177902\n",
            "Train Epoch: 31 [520/700 (74%)]\tLoss: 119.090103\n",
            "Train Epoch: 31 [560/700 (80%)]\tLoss: 204.980865\n",
            "Train Epoch: 31 [600/700 (86%)]\tLoss: 212.435959\n",
            "Train Epoch: 31 [640/700 (91%)]\tLoss: 132.817627\n",
            "Train Epoch: 31 [680/700 (97%)]\tLoss: 122.935806\n",
            "\n",
            "Test set: Average loss: 8.0936\n",
            "\n",
            "Train Epoch: 32 [0/700 (0%)]\tLoss: 171.447708\n",
            "Train Epoch: 32 [40/700 (6%)]\tLoss: 198.032288\n",
            "Train Epoch: 32 [80/700 (11%)]\tLoss: 164.429657\n",
            "Train Epoch: 32 [120/700 (17%)]\tLoss: 92.182083\n",
            "Train Epoch: 32 [160/700 (23%)]\tLoss: 136.703644\n",
            "Train Epoch: 32 [200/700 (29%)]\tLoss: 269.003754\n",
            "Train Epoch: 32 [240/700 (34%)]\tLoss: 156.673172\n",
            "Train Epoch: 32 [280/700 (40%)]\tLoss: 98.960449\n",
            "Train Epoch: 32 [320/700 (46%)]\tLoss: 308.738831\n",
            "Train Epoch: 32 [360/700 (51%)]\tLoss: 119.309601\n",
            "Train Epoch: 32 [400/700 (57%)]\tLoss: 163.651947\n",
            "Train Epoch: 32 [440/700 (63%)]\tLoss: 130.528046\n",
            "Train Epoch: 32 [480/700 (69%)]\tLoss: 243.633408\n",
            "Train Epoch: 32 [520/700 (74%)]\tLoss: 190.306610\n",
            "Train Epoch: 32 [560/700 (80%)]\tLoss: 261.621033\n",
            "Train Epoch: 32 [600/700 (86%)]\tLoss: 219.582443\n",
            "Train Epoch: 32 [640/700 (91%)]\tLoss: 189.101639\n",
            "Train Epoch: 32 [680/700 (97%)]\tLoss: 89.353760\n",
            "\n",
            "Test set: Average loss: 7.9971\n",
            "\n",
            "Train Epoch: 33 [0/700 (0%)]\tLoss: 173.943939\n",
            "Train Epoch: 33 [40/700 (6%)]\tLoss: 121.548996\n",
            "Train Epoch: 33 [80/700 (11%)]\tLoss: 167.442047\n",
            "Train Epoch: 33 [120/700 (17%)]\tLoss: 177.939194\n",
            "Train Epoch: 33 [160/700 (23%)]\tLoss: 253.997879\n",
            "Train Epoch: 33 [200/700 (29%)]\tLoss: 238.808701\n",
            "Train Epoch: 33 [240/700 (34%)]\tLoss: 159.772278\n",
            "Train Epoch: 33 [280/700 (40%)]\tLoss: 241.807220\n",
            "Train Epoch: 33 [320/700 (46%)]\tLoss: 216.772491\n",
            "Train Epoch: 33 [360/700 (51%)]\tLoss: 132.639252\n",
            "Train Epoch: 33 [400/700 (57%)]\tLoss: 208.197815\n",
            "Train Epoch: 33 [440/700 (63%)]\tLoss: 168.709702\n",
            "Train Epoch: 33 [480/700 (69%)]\tLoss: 218.047165\n",
            "Train Epoch: 33 [520/700 (74%)]\tLoss: 225.319626\n",
            "Train Epoch: 33 [560/700 (80%)]\tLoss: 164.794159\n",
            "Train Epoch: 33 [600/700 (86%)]\tLoss: 148.860306\n",
            "Train Epoch: 33 [640/700 (91%)]\tLoss: 126.896362\n",
            "Train Epoch: 33 [680/700 (97%)]\tLoss: 183.595184\n",
            "\n",
            "Test set: Average loss: 8.1473\n",
            "\n",
            "Train Epoch: 34 [0/700 (0%)]\tLoss: 184.298508\n",
            "Train Epoch: 34 [40/700 (6%)]\tLoss: 176.135193\n",
            "Train Epoch: 34 [80/700 (11%)]\tLoss: 110.520622\n",
            "Train Epoch: 34 [120/700 (17%)]\tLoss: 184.258698\n",
            "Train Epoch: 34 [160/700 (23%)]\tLoss: 153.255157\n",
            "Train Epoch: 34 [200/700 (29%)]\tLoss: 165.446609\n",
            "Train Epoch: 34 [240/700 (34%)]\tLoss: 133.528534\n",
            "Train Epoch: 34 [280/700 (40%)]\tLoss: 192.060822\n",
            "Train Epoch: 34 [320/700 (46%)]\tLoss: 186.324249\n",
            "Train Epoch: 34 [360/700 (51%)]\tLoss: 157.178040\n",
            "Train Epoch: 34 [400/700 (57%)]\tLoss: 194.033859\n",
            "Train Epoch: 34 [440/700 (63%)]\tLoss: 226.526642\n",
            "Train Epoch: 34 [480/700 (69%)]\tLoss: 145.172256\n",
            "Train Epoch: 34 [520/700 (74%)]\tLoss: 159.363907\n",
            "Train Epoch: 34 [560/700 (80%)]\tLoss: 220.733337\n",
            "Train Epoch: 34 [600/700 (86%)]\tLoss: 131.272614\n",
            "Train Epoch: 34 [640/700 (91%)]\tLoss: 161.168106\n",
            "Train Epoch: 34 [680/700 (97%)]\tLoss: 118.656685\n",
            "\n",
            "Test set: Average loss: 8.1473\n",
            "\n",
            "Train Epoch: 35 [0/700 (0%)]\tLoss: 217.350525\n",
            "Train Epoch: 35 [40/700 (6%)]\tLoss: 111.212875\n",
            "Train Epoch: 35 [80/700 (11%)]\tLoss: 181.950653\n",
            "Train Epoch: 35 [120/700 (17%)]\tLoss: 192.793701\n",
            "Train Epoch: 35 [160/700 (23%)]\tLoss: 193.731369\n",
            "Train Epoch: 35 [200/700 (29%)]\tLoss: 109.369438\n",
            "Train Epoch: 35 [240/700 (34%)]\tLoss: 90.301682\n",
            "Train Epoch: 35 [280/700 (40%)]\tLoss: 286.397766\n",
            "Train Epoch: 35 [320/700 (46%)]\tLoss: 246.454880\n",
            "Train Epoch: 35 [360/700 (51%)]\tLoss: 132.029449\n",
            "Train Epoch: 35 [400/700 (57%)]\tLoss: 136.925110\n",
            "Train Epoch: 35 [440/700 (63%)]\tLoss: 104.796669\n",
            "Train Epoch: 35 [480/700 (69%)]\tLoss: 213.061264\n",
            "Train Epoch: 35 [520/700 (74%)]\tLoss: 103.888779\n",
            "Train Epoch: 35 [560/700 (80%)]\tLoss: 199.820221\n",
            "Train Epoch: 35 [600/700 (86%)]\tLoss: 153.352875\n",
            "Train Epoch: 35 [640/700 (91%)]\tLoss: 168.701187\n",
            "Train Epoch: 35 [680/700 (97%)]\tLoss: 171.640274\n",
            "\n",
            "Test set: Average loss: 8.1445\n",
            "\n",
            "Train Epoch: 36 [0/700 (0%)]\tLoss: 178.934769\n",
            "Train Epoch: 36 [40/700 (6%)]\tLoss: 242.192276\n",
            "Train Epoch: 36 [80/700 (11%)]\tLoss: 212.318909\n",
            "Train Epoch: 36 [120/700 (17%)]\tLoss: 212.392731\n",
            "Train Epoch: 36 [160/700 (23%)]\tLoss: 179.460159\n",
            "Train Epoch: 36 [200/700 (29%)]\tLoss: 145.952499\n",
            "Train Epoch: 36 [240/700 (34%)]\tLoss: 195.538254\n",
            "Train Epoch: 36 [280/700 (40%)]\tLoss: 141.250137\n",
            "Train Epoch: 36 [320/700 (46%)]\tLoss: 170.520340\n",
            "Train Epoch: 36 [360/700 (51%)]\tLoss: 144.812759\n",
            "Train Epoch: 36 [400/700 (57%)]\tLoss: 107.017372\n",
            "Train Epoch: 36 [440/700 (63%)]\tLoss: 248.506302\n",
            "Train Epoch: 36 [480/700 (69%)]\tLoss: 181.060349\n",
            "Train Epoch: 36 [520/700 (74%)]\tLoss: 154.548889\n",
            "Train Epoch: 36 [560/700 (80%)]\tLoss: 168.047531\n",
            "Train Epoch: 36 [600/700 (86%)]\tLoss: 198.440506\n",
            "Train Epoch: 36 [640/700 (91%)]\tLoss: 166.413956\n",
            "Train Epoch: 36 [680/700 (97%)]\tLoss: 139.048355\n",
            "\n",
            "Test set: Average loss: 8.0708\n",
            "\n",
            "Train Epoch: 37 [0/700 (0%)]\tLoss: 137.482513\n",
            "Train Epoch: 37 [40/700 (6%)]\tLoss: 155.866867\n",
            "Train Epoch: 37 [80/700 (11%)]\tLoss: 144.369553\n",
            "Train Epoch: 37 [120/700 (17%)]\tLoss: 151.851990\n",
            "Train Epoch: 37 [160/700 (23%)]\tLoss: 112.682114\n",
            "Train Epoch: 37 [200/700 (29%)]\tLoss: 152.850174\n",
            "Train Epoch: 37 [240/700 (34%)]\tLoss: 158.471268\n",
            "Train Epoch: 37 [280/700 (40%)]\tLoss: 115.862091\n",
            "Train Epoch: 37 [320/700 (46%)]\tLoss: 209.392090\n",
            "Train Epoch: 37 [360/700 (51%)]\tLoss: 193.221283\n",
            "Train Epoch: 37 [400/700 (57%)]\tLoss: 201.309418\n",
            "Train Epoch: 37 [440/700 (63%)]\tLoss: 156.466354\n",
            "Train Epoch: 37 [480/700 (69%)]\tLoss: 228.523056\n",
            "Train Epoch: 37 [520/700 (74%)]\tLoss: 136.580048\n",
            "Train Epoch: 37 [560/700 (80%)]\tLoss: 138.136749\n",
            "Train Epoch: 37 [600/700 (86%)]\tLoss: 126.143219\n",
            "Train Epoch: 37 [640/700 (91%)]\tLoss: 298.729431\n",
            "Train Epoch: 37 [680/700 (97%)]\tLoss: 155.928085\n",
            "\n",
            "Test set: Average loss: 8.0059\n",
            "\n",
            "Train Epoch: 38 [0/700 (0%)]\tLoss: 130.171616\n",
            "Train Epoch: 38 [40/700 (6%)]\tLoss: 138.052841\n",
            "Train Epoch: 38 [80/700 (11%)]\tLoss: 174.721588\n",
            "Train Epoch: 38 [120/700 (17%)]\tLoss: 171.075256\n",
            "Train Epoch: 38 [160/700 (23%)]\tLoss: 246.397568\n",
            "Train Epoch: 38 [200/700 (29%)]\tLoss: 169.703323\n",
            "Train Epoch: 38 [240/700 (34%)]\tLoss: 128.219696\n",
            "Train Epoch: 38 [280/700 (40%)]\tLoss: 199.219376\n",
            "Train Epoch: 38 [320/700 (46%)]\tLoss: 152.717499\n",
            "Train Epoch: 38 [360/700 (51%)]\tLoss: 149.324615\n",
            "Train Epoch: 38 [400/700 (57%)]\tLoss: 219.521133\n",
            "Train Epoch: 38 [440/700 (63%)]\tLoss: 147.735397\n",
            "Train Epoch: 38 [480/700 (69%)]\tLoss: 129.394485\n",
            "Train Epoch: 38 [520/700 (74%)]\tLoss: 179.599915\n",
            "Train Epoch: 38 [560/700 (80%)]\tLoss: 117.011238\n",
            "Train Epoch: 38 [600/700 (86%)]\tLoss: 194.457474\n",
            "Train Epoch: 38 [640/700 (91%)]\tLoss: 176.304764\n",
            "Train Epoch: 38 [680/700 (97%)]\tLoss: 186.186996\n",
            "\n",
            "Test set: Average loss: 8.1642\n",
            "\n",
            "Train Epoch: 39 [0/700 (0%)]\tLoss: 187.977737\n",
            "Train Epoch: 39 [40/700 (6%)]\tLoss: 129.578278\n",
            "Train Epoch: 39 [80/700 (11%)]\tLoss: 135.659119\n",
            "Train Epoch: 39 [120/700 (17%)]\tLoss: 136.171906\n",
            "Train Epoch: 39 [160/700 (23%)]\tLoss: 192.974030\n",
            "Train Epoch: 39 [200/700 (29%)]\tLoss: 131.325500\n",
            "Train Epoch: 39 [240/700 (34%)]\tLoss: 190.469894\n",
            "Train Epoch: 39 [280/700 (40%)]\tLoss: 166.305923\n",
            "Train Epoch: 39 [320/700 (46%)]\tLoss: 232.030746\n",
            "Train Epoch: 39 [360/700 (51%)]\tLoss: 146.636520\n",
            "Train Epoch: 39 [400/700 (57%)]\tLoss: 107.486130\n",
            "Train Epoch: 39 [440/700 (63%)]\tLoss: 142.825043\n",
            "Train Epoch: 39 [480/700 (69%)]\tLoss: 242.168625\n",
            "Train Epoch: 39 [520/700 (74%)]\tLoss: 129.891510\n",
            "Train Epoch: 39 [560/700 (80%)]\tLoss: 150.059601\n",
            "Train Epoch: 39 [600/700 (86%)]\tLoss: 182.735611\n",
            "Train Epoch: 39 [640/700 (91%)]\tLoss: 165.329147\n",
            "Train Epoch: 39 [680/700 (97%)]\tLoss: 244.281006\n",
            "\n",
            "Test set: Average loss: 8.1340\n",
            "\n",
            "Train Epoch: 40 [0/700 (0%)]\tLoss: 110.956024\n",
            "Train Epoch: 40 [40/700 (6%)]\tLoss: 109.858246\n",
            "Train Epoch: 40 [80/700 (11%)]\tLoss: 140.888077\n",
            "Train Epoch: 40 [120/700 (17%)]\tLoss: 108.769676\n",
            "Train Epoch: 40 [160/700 (23%)]\tLoss: 116.142258\n",
            "Train Epoch: 40 [200/700 (29%)]\tLoss: 137.662628\n",
            "Train Epoch: 40 [240/700 (34%)]\tLoss: 254.889755\n",
            "Train Epoch: 40 [280/700 (40%)]\tLoss: 231.539062\n",
            "Train Epoch: 40 [320/700 (46%)]\tLoss: 219.390457\n",
            "Train Epoch: 40 [360/700 (51%)]\tLoss: 126.322395\n",
            "Train Epoch: 40 [400/700 (57%)]\tLoss: 170.049866\n",
            "Train Epoch: 40 [440/700 (63%)]\tLoss: 105.222641\n",
            "Train Epoch: 40 [480/700 (69%)]\tLoss: 169.125809\n",
            "Train Epoch: 40 [520/700 (74%)]\tLoss: 146.952560\n",
            "Train Epoch: 40 [560/700 (80%)]\tLoss: 248.081268\n",
            "Train Epoch: 40 [600/700 (86%)]\tLoss: 149.762695\n",
            "Train Epoch: 40 [640/700 (91%)]\tLoss: 184.582764\n",
            "Train Epoch: 40 [680/700 (97%)]\tLoss: 213.082428\n",
            "\n",
            "Test set: Average loss: 8.1339\n",
            "\n",
            "Train Epoch: 41 [0/700 (0%)]\tLoss: 82.724648\n",
            "Train Epoch: 41 [40/700 (6%)]\tLoss: 230.774841\n",
            "Train Epoch: 41 [80/700 (11%)]\tLoss: 210.693069\n",
            "Train Epoch: 41 [120/700 (17%)]\tLoss: 242.615768\n",
            "Train Epoch: 41 [160/700 (23%)]\tLoss: 193.948776\n",
            "Train Epoch: 41 [200/700 (29%)]\tLoss: 142.628860\n",
            "Train Epoch: 41 [240/700 (34%)]\tLoss: 90.545822\n",
            "Train Epoch: 41 [280/700 (40%)]\tLoss: 150.196884\n",
            "Train Epoch: 41 [320/700 (46%)]\tLoss: 159.696060\n",
            "Train Epoch: 41 [360/700 (51%)]\tLoss: 203.058792\n",
            "Train Epoch: 41 [400/700 (57%)]\tLoss: 157.234695\n",
            "Train Epoch: 41 [440/700 (63%)]\tLoss: 164.295410\n",
            "Train Epoch: 41 [480/700 (69%)]\tLoss: 179.372330\n",
            "Train Epoch: 41 [520/700 (74%)]\tLoss: 120.026176\n",
            "Train Epoch: 41 [560/700 (80%)]\tLoss: 243.809052\n",
            "Train Epoch: 41 [600/700 (86%)]\tLoss: 181.586594\n",
            "Train Epoch: 41 [640/700 (91%)]\tLoss: 214.729355\n",
            "Train Epoch: 41 [680/700 (97%)]\tLoss: 187.256332\n",
            "\n",
            "Test set: Average loss: 7.9641\n",
            "\n",
            "Train Epoch: 42 [0/700 (0%)]\tLoss: 226.571259\n",
            "Train Epoch: 42 [40/700 (6%)]\tLoss: 164.447952\n",
            "Train Epoch: 42 [80/700 (11%)]\tLoss: 163.012863\n",
            "Train Epoch: 42 [120/700 (17%)]\tLoss: 139.435226\n",
            "Train Epoch: 42 [160/700 (23%)]\tLoss: 137.035110\n",
            "Train Epoch: 42 [200/700 (29%)]\tLoss: 119.757111\n",
            "Train Epoch: 42 [240/700 (34%)]\tLoss: 177.133804\n",
            "Train Epoch: 42 [280/700 (40%)]\tLoss: 135.224991\n",
            "Train Epoch: 42 [320/700 (46%)]\tLoss: 157.051758\n",
            "Train Epoch: 42 [360/700 (51%)]\tLoss: 188.921097\n",
            "Train Epoch: 42 [400/700 (57%)]\tLoss: 140.195847\n",
            "Train Epoch: 42 [440/700 (63%)]\tLoss: 107.993805\n",
            "Train Epoch: 42 [480/700 (69%)]\tLoss: 155.873352\n",
            "Train Epoch: 42 [520/700 (74%)]\tLoss: 112.475731\n",
            "Train Epoch: 42 [560/700 (80%)]\tLoss: 125.675201\n",
            "Train Epoch: 42 [600/700 (86%)]\tLoss: 97.736359\n",
            "Train Epoch: 42 [640/700 (91%)]\tLoss: 232.631989\n",
            "Train Epoch: 42 [680/700 (97%)]\tLoss: 89.793694\n",
            "\n",
            "Test set: Average loss: 8.1923\n",
            "\n",
            "Train Epoch: 43 [0/700 (0%)]\tLoss: 169.402481\n",
            "Train Epoch: 43 [40/700 (6%)]\tLoss: 96.759995\n",
            "Train Epoch: 43 [80/700 (11%)]\tLoss: 266.034973\n",
            "Train Epoch: 43 [120/700 (17%)]\tLoss: 171.861755\n",
            "Train Epoch: 43 [160/700 (23%)]\tLoss: 129.561600\n",
            "Train Epoch: 43 [200/700 (29%)]\tLoss: 107.753326\n",
            "Train Epoch: 43 [240/700 (34%)]\tLoss: 273.618866\n",
            "Train Epoch: 43 [280/700 (40%)]\tLoss: 154.411880\n",
            "Train Epoch: 43 [320/700 (46%)]\tLoss: 255.570465\n",
            "Train Epoch: 43 [360/700 (51%)]\tLoss: 152.496689\n",
            "Train Epoch: 43 [400/700 (57%)]\tLoss: 146.722427\n",
            "Train Epoch: 43 [440/700 (63%)]\tLoss: 149.843552\n",
            "Train Epoch: 43 [480/700 (69%)]\tLoss: 215.341415\n",
            "Train Epoch: 43 [520/700 (74%)]\tLoss: 280.191895\n",
            "Train Epoch: 43 [560/700 (80%)]\tLoss: 149.089294\n",
            "Train Epoch: 43 [600/700 (86%)]\tLoss: 193.593277\n",
            "Train Epoch: 43 [640/700 (91%)]\tLoss: 155.999161\n",
            "Train Epoch: 43 [680/700 (97%)]\tLoss: 207.740768\n",
            "\n",
            "Test set: Average loss: 8.2483\n",
            "\n",
            "Train Epoch: 44 [0/700 (0%)]\tLoss: 255.919586\n",
            "Train Epoch: 44 [40/700 (6%)]\tLoss: 143.420273\n",
            "Train Epoch: 44 [80/700 (11%)]\tLoss: 129.822159\n",
            "Train Epoch: 44 [120/700 (17%)]\tLoss: 148.097031\n",
            "Train Epoch: 44 [160/700 (23%)]\tLoss: 139.676270\n",
            "Train Epoch: 44 [200/700 (29%)]\tLoss: 99.291245\n",
            "Train Epoch: 44 [240/700 (34%)]\tLoss: 128.997864\n",
            "Train Epoch: 44 [280/700 (40%)]\tLoss: 177.752792\n",
            "Train Epoch: 44 [320/700 (46%)]\tLoss: 99.262306\n",
            "Train Epoch: 44 [360/700 (51%)]\tLoss: 133.742538\n",
            "Train Epoch: 44 [400/700 (57%)]\tLoss: 164.981659\n",
            "Train Epoch: 44 [440/700 (63%)]\tLoss: 113.904694\n",
            "Train Epoch: 44 [480/700 (69%)]\tLoss: 213.914169\n",
            "Train Epoch: 44 [520/700 (74%)]\tLoss: 133.570663\n",
            "Train Epoch: 44 [560/700 (80%)]\tLoss: 160.951767\n",
            "Train Epoch: 44 [600/700 (86%)]\tLoss: 161.862564\n",
            "Train Epoch: 44 [640/700 (91%)]\tLoss: 196.190918\n",
            "Train Epoch: 44 [680/700 (97%)]\tLoss: 171.390854\n",
            "\n",
            "Test set: Average loss: 8.2261\n",
            "\n",
            "Train Epoch: 45 [0/700 (0%)]\tLoss: 255.468597\n",
            "Train Epoch: 45 [40/700 (6%)]\tLoss: 135.917358\n",
            "Train Epoch: 45 [80/700 (11%)]\tLoss: 199.286346\n",
            "Train Epoch: 45 [120/700 (17%)]\tLoss: 142.523605\n",
            "Train Epoch: 45 [160/700 (23%)]\tLoss: 159.754715\n",
            "Train Epoch: 45 [200/700 (29%)]\tLoss: 115.723473\n",
            "Train Epoch: 45 [240/700 (34%)]\tLoss: 285.442993\n",
            "Train Epoch: 45 [280/700 (40%)]\tLoss: 166.798172\n",
            "Train Epoch: 45 [320/700 (46%)]\tLoss: 109.274605\n",
            "Train Epoch: 45 [360/700 (51%)]\tLoss: 167.826248\n",
            "Train Epoch: 45 [400/700 (57%)]\tLoss: 112.120102\n",
            "Train Epoch: 45 [440/700 (63%)]\tLoss: 145.726547\n",
            "Train Epoch: 45 [480/700 (69%)]\tLoss: 150.279648\n",
            "Train Epoch: 45 [520/700 (74%)]\tLoss: 124.055916\n",
            "Train Epoch: 45 [560/700 (80%)]\tLoss: 203.034286\n",
            "Train Epoch: 45 [600/700 (86%)]\tLoss: 178.093491\n",
            "Train Epoch: 45 [640/700 (91%)]\tLoss: 245.540329\n",
            "Train Epoch: 45 [680/700 (97%)]\tLoss: 146.833633\n",
            "\n",
            "Test set: Average loss: 8.1406\n",
            "\n",
            "Train Epoch: 46 [0/700 (0%)]\tLoss: 244.019958\n",
            "Train Epoch: 46 [40/700 (6%)]\tLoss: 164.339142\n",
            "Train Epoch: 46 [80/700 (11%)]\tLoss: 119.055077\n",
            "Train Epoch: 46 [120/700 (17%)]\tLoss: 185.920609\n",
            "Train Epoch: 46 [160/700 (23%)]\tLoss: 94.053665\n",
            "Train Epoch: 46 [200/700 (29%)]\tLoss: 89.541733\n",
            "Train Epoch: 46 [240/700 (34%)]\tLoss: 317.549866\n",
            "Train Epoch: 46 [280/700 (40%)]\tLoss: 144.852570\n",
            "Train Epoch: 46 [320/700 (46%)]\tLoss: 212.604950\n",
            "Train Epoch: 46 [360/700 (51%)]\tLoss: 145.414124\n",
            "Train Epoch: 46 [400/700 (57%)]\tLoss: 185.813248\n",
            "Train Epoch: 46 [440/700 (63%)]\tLoss: 107.113716\n",
            "Train Epoch: 46 [480/700 (69%)]\tLoss: 164.769958\n",
            "Train Epoch: 46 [520/700 (74%)]\tLoss: 116.595230\n",
            "Train Epoch: 46 [560/700 (80%)]\tLoss: 278.248688\n",
            "Train Epoch: 46 [600/700 (86%)]\tLoss: 156.462097\n",
            "Train Epoch: 46 [640/700 (91%)]\tLoss: 221.745987\n",
            "Train Epoch: 46 [680/700 (97%)]\tLoss: 135.790787\n",
            "\n",
            "Test set: Average loss: 8.2317\n",
            "\n",
            "Train Epoch: 47 [0/700 (0%)]\tLoss: 122.614342\n",
            "Train Epoch: 47 [40/700 (6%)]\tLoss: 147.798019\n",
            "Train Epoch: 47 [80/700 (11%)]\tLoss: 192.889847\n",
            "Train Epoch: 47 [120/700 (17%)]\tLoss: 88.315590\n",
            "Train Epoch: 47 [160/700 (23%)]\tLoss: 198.952530\n",
            "Train Epoch: 47 [200/700 (29%)]\tLoss: 192.315399\n",
            "Train Epoch: 47 [240/700 (34%)]\tLoss: 153.735657\n",
            "Train Epoch: 47 [280/700 (40%)]\tLoss: 135.556458\n",
            "Train Epoch: 47 [320/700 (46%)]\tLoss: 152.041733\n",
            "Train Epoch: 47 [360/700 (51%)]\tLoss: 160.817612\n",
            "Train Epoch: 47 [400/700 (57%)]\tLoss: 160.465073\n",
            "Train Epoch: 47 [440/700 (63%)]\tLoss: 211.899414\n",
            "Train Epoch: 47 [480/700 (69%)]\tLoss: 160.076416\n",
            "Train Epoch: 47 [520/700 (74%)]\tLoss: 150.517609\n",
            "Train Epoch: 47 [560/700 (80%)]\tLoss: 170.651154\n",
            "Train Epoch: 47 [600/700 (86%)]\tLoss: 179.035965\n",
            "Train Epoch: 47 [640/700 (91%)]\tLoss: 161.450104\n",
            "Train Epoch: 47 [680/700 (97%)]\tLoss: 145.725266\n",
            "\n",
            "Test set: Average loss: 8.0504\n",
            "\n",
            "Train Epoch: 48 [0/700 (0%)]\tLoss: 193.944855\n",
            "Train Epoch: 48 [40/700 (6%)]\tLoss: 150.005844\n",
            "Train Epoch: 48 [80/700 (11%)]\tLoss: 160.023346\n",
            "Train Epoch: 48 [120/700 (17%)]\tLoss: 86.382767\n",
            "Train Epoch: 48 [160/700 (23%)]\tLoss: 158.428024\n",
            "Train Epoch: 48 [200/700 (29%)]\tLoss: 165.976334\n",
            "Train Epoch: 48 [240/700 (34%)]\tLoss: 87.973061\n",
            "Train Epoch: 48 [280/700 (40%)]\tLoss: 189.721848\n",
            "Train Epoch: 48 [320/700 (46%)]\tLoss: 215.355972\n",
            "Train Epoch: 48 [360/700 (51%)]\tLoss: 169.777679\n",
            "Train Epoch: 48 [400/700 (57%)]\tLoss: 106.926910\n",
            "Train Epoch: 48 [440/700 (63%)]\tLoss: 226.369568\n",
            "Train Epoch: 48 [480/700 (69%)]\tLoss: 105.547981\n",
            "Train Epoch: 48 [520/700 (74%)]\tLoss: 212.584122\n",
            "Train Epoch: 48 [560/700 (80%)]\tLoss: 149.010391\n",
            "Train Epoch: 48 [600/700 (86%)]\tLoss: 119.583992\n",
            "Train Epoch: 48 [640/700 (91%)]\tLoss: 196.867996\n",
            "Train Epoch: 48 [680/700 (97%)]\tLoss: 269.866730\n",
            "\n",
            "Test set: Average loss: 8.0847\n",
            "\n",
            "Train Epoch: 49 [0/700 (0%)]\tLoss: 206.190582\n",
            "Train Epoch: 49 [40/700 (6%)]\tLoss: 148.904526\n",
            "Train Epoch: 49 [80/700 (11%)]\tLoss: 118.570847\n",
            "Train Epoch: 49 [120/700 (17%)]\tLoss: 197.645203\n",
            "Train Epoch: 49 [160/700 (23%)]\tLoss: 106.344261\n",
            "Train Epoch: 49 [200/700 (29%)]\tLoss: 208.514893\n",
            "Train Epoch: 49 [240/700 (34%)]\tLoss: 124.413544\n",
            "Train Epoch: 49 [280/700 (40%)]\tLoss: 264.484833\n",
            "Train Epoch: 49 [320/700 (46%)]\tLoss: 103.578568\n",
            "Train Epoch: 49 [360/700 (51%)]\tLoss: 109.934662\n",
            "Train Epoch: 49 [400/700 (57%)]\tLoss: 214.817734\n",
            "Train Epoch: 49 [440/700 (63%)]\tLoss: 185.354187\n",
            "Train Epoch: 49 [480/700 (69%)]\tLoss: 161.631332\n",
            "Train Epoch: 49 [520/700 (74%)]\tLoss: 243.841141\n",
            "Train Epoch: 49 [560/700 (80%)]\tLoss: 97.698013\n",
            "Train Epoch: 49 [600/700 (86%)]\tLoss: 206.379898\n",
            "Train Epoch: 49 [640/700 (91%)]\tLoss: 113.421242\n",
            "Train Epoch: 49 [680/700 (97%)]\tLoss: 197.304581\n",
            "\n",
            "Test set: Average loss: 8.0711\n",
            "\n",
            "Train Epoch: 50 [0/700 (0%)]\tLoss: 171.587296\n",
            "Train Epoch: 50 [40/700 (6%)]\tLoss: 136.392319\n",
            "Train Epoch: 50 [80/700 (11%)]\tLoss: 241.695511\n",
            "Train Epoch: 50 [120/700 (17%)]\tLoss: 206.470047\n",
            "Train Epoch: 50 [160/700 (23%)]\tLoss: 153.896683\n",
            "Train Epoch: 50 [200/700 (29%)]\tLoss: 255.592911\n",
            "Train Epoch: 50 [240/700 (34%)]\tLoss: 160.136932\n",
            "Train Epoch: 50 [280/700 (40%)]\tLoss: 134.197174\n",
            "Train Epoch: 50 [320/700 (46%)]\tLoss: 187.635269\n",
            "Train Epoch: 50 [360/700 (51%)]\tLoss: 220.549637\n",
            "Train Epoch: 50 [400/700 (57%)]\tLoss: 178.408600\n",
            "Train Epoch: 50 [440/700 (63%)]\tLoss: 153.327988\n",
            "Train Epoch: 50 [480/700 (69%)]\tLoss: 164.641937\n",
            "Train Epoch: 50 [520/700 (74%)]\tLoss: 113.709572\n",
            "Train Epoch: 50 [560/700 (80%)]\tLoss: 135.991867\n",
            "Train Epoch: 50 [600/700 (86%)]\tLoss: 98.907997\n",
            "Train Epoch: 50 [640/700 (91%)]\tLoss: 146.600815\n",
            "Train Epoch: 50 [680/700 (97%)]\tLoss: 125.186768\n",
            "\n",
            "Test set: Average loss: 8.4402\n",
            "\n",
            "Train Epoch: 51 [0/700 (0%)]\tLoss: 205.423035\n",
            "Train Epoch: 51 [40/700 (6%)]\tLoss: 174.118790\n",
            "Train Epoch: 51 [80/700 (11%)]\tLoss: 202.685638\n",
            "Train Epoch: 51 [120/700 (17%)]\tLoss: 191.112396\n",
            "Train Epoch: 51 [160/700 (23%)]\tLoss: 112.896271\n",
            "Train Epoch: 51 [200/700 (29%)]\tLoss: 240.313507\n",
            "Train Epoch: 51 [240/700 (34%)]\tLoss: 173.748734\n",
            "Train Epoch: 51 [280/700 (40%)]\tLoss: 163.574081\n",
            "Train Epoch: 51 [320/700 (46%)]\tLoss: 208.162064\n",
            "Train Epoch: 51 [360/700 (51%)]\tLoss: 150.902649\n",
            "Train Epoch: 51 [400/700 (57%)]\tLoss: 161.672806\n",
            "Train Epoch: 51 [440/700 (63%)]\tLoss: 106.006920\n",
            "Train Epoch: 51 [480/700 (69%)]\tLoss: 229.181702\n",
            "Train Epoch: 51 [520/700 (74%)]\tLoss: 176.763916\n",
            "Train Epoch: 51 [560/700 (80%)]\tLoss: 188.078705\n",
            "Train Epoch: 51 [600/700 (86%)]\tLoss: 181.412415\n",
            "Train Epoch: 51 [640/700 (91%)]\tLoss: 120.529572\n",
            "Train Epoch: 51 [680/700 (97%)]\tLoss: 164.510254\n",
            "\n",
            "Test set: Average loss: 8.0981\n",
            "\n",
            "Train Epoch: 52 [0/700 (0%)]\tLoss: 150.108109\n",
            "Train Epoch: 52 [40/700 (6%)]\tLoss: 156.736923\n",
            "Train Epoch: 52 [80/700 (11%)]\tLoss: 137.821960\n",
            "Train Epoch: 52 [120/700 (17%)]\tLoss: 102.616783\n",
            "Train Epoch: 52 [160/700 (23%)]\tLoss: 195.902786\n",
            "Train Epoch: 52 [200/700 (29%)]\tLoss: 244.800461\n",
            "Train Epoch: 52 [240/700 (34%)]\tLoss: 209.931732\n",
            "Train Epoch: 52 [280/700 (40%)]\tLoss: 81.102982\n",
            "Train Epoch: 52 [320/700 (46%)]\tLoss: 254.371292\n",
            "Train Epoch: 52 [360/700 (51%)]\tLoss: 222.513153\n",
            "Train Epoch: 52 [400/700 (57%)]\tLoss: 93.959312\n",
            "Train Epoch: 52 [440/700 (63%)]\tLoss: 190.493683\n",
            "Train Epoch: 52 [480/700 (69%)]\tLoss: 176.224838\n",
            "Train Epoch: 52 [520/700 (74%)]\tLoss: 183.483658\n",
            "Train Epoch: 52 [560/700 (80%)]\tLoss: 152.204178\n",
            "Train Epoch: 52 [600/700 (86%)]\tLoss: 211.181000\n",
            "Train Epoch: 52 [640/700 (91%)]\tLoss: 143.864761\n",
            "Train Epoch: 52 [680/700 (97%)]\tLoss: 170.637054\n",
            "\n",
            "Test set: Average loss: 8.1570\n",
            "\n",
            "Train Epoch: 53 [0/700 (0%)]\tLoss: 197.517136\n",
            "Train Epoch: 53 [40/700 (6%)]\tLoss: 142.489441\n",
            "Train Epoch: 53 [80/700 (11%)]\tLoss: 118.800453\n",
            "Train Epoch: 53 [120/700 (17%)]\tLoss: 141.588913\n",
            "Train Epoch: 53 [160/700 (23%)]\tLoss: 169.842468\n",
            "Train Epoch: 53 [200/700 (29%)]\tLoss: 94.104897\n",
            "Train Epoch: 53 [240/700 (34%)]\tLoss: 170.514175\n",
            "Train Epoch: 53 [280/700 (40%)]\tLoss: 156.488434\n",
            "Train Epoch: 53 [320/700 (46%)]\tLoss: 207.341370\n",
            "Train Epoch: 53 [360/700 (51%)]\tLoss: 89.336952\n",
            "Train Epoch: 53 [400/700 (57%)]\tLoss: 133.604645\n",
            "Train Epoch: 53 [440/700 (63%)]\tLoss: 127.546227\n",
            "Train Epoch: 53 [480/700 (69%)]\tLoss: 227.512939\n",
            "Train Epoch: 53 [520/700 (74%)]\tLoss: 128.049255\n",
            "Train Epoch: 53 [560/700 (80%)]\tLoss: 202.683929\n",
            "Train Epoch: 53 [600/700 (86%)]\tLoss: 178.493912\n",
            "Train Epoch: 53 [640/700 (91%)]\tLoss: 105.788704\n",
            "Train Epoch: 53 [680/700 (97%)]\tLoss: 159.787140\n",
            "\n",
            "Test set: Average loss: 8.0181\n",
            "\n",
            "Train Epoch: 54 [0/700 (0%)]\tLoss: 175.189514\n",
            "Train Epoch: 54 [40/700 (6%)]\tLoss: 129.318832\n",
            "Train Epoch: 54 [80/700 (11%)]\tLoss: 235.526566\n",
            "Train Epoch: 54 [120/700 (17%)]\tLoss: 158.818359\n",
            "Train Epoch: 54 [160/700 (23%)]\tLoss: 200.719040\n",
            "Train Epoch: 54 [200/700 (29%)]\tLoss: 189.511383\n",
            "Train Epoch: 54 [240/700 (34%)]\tLoss: 154.465439\n",
            "Train Epoch: 54 [280/700 (40%)]\tLoss: 121.097565\n",
            "Train Epoch: 54 [320/700 (46%)]\tLoss: 107.388519\n",
            "Train Epoch: 54 [360/700 (51%)]\tLoss: 193.110123\n",
            "Train Epoch: 54 [400/700 (57%)]\tLoss: 208.794189\n",
            "Train Epoch: 54 [440/700 (63%)]\tLoss: 128.001755\n",
            "Train Epoch: 54 [480/700 (69%)]\tLoss: 120.859344\n",
            "Train Epoch: 54 [520/700 (74%)]\tLoss: 192.390198\n",
            "Train Epoch: 54 [560/700 (80%)]\tLoss: 130.487625\n",
            "Train Epoch: 54 [600/700 (86%)]\tLoss: 170.714157\n",
            "Train Epoch: 54 [640/700 (91%)]\tLoss: 259.496674\n",
            "Train Epoch: 54 [680/700 (97%)]\tLoss: 137.875870\n",
            "\n",
            "Test set: Average loss: 8.0634\n",
            "\n",
            "Train Epoch: 55 [0/700 (0%)]\tLoss: 125.923515\n",
            "Train Epoch: 55 [40/700 (6%)]\tLoss: 272.897034\n",
            "Train Epoch: 55 [80/700 (11%)]\tLoss: 113.308868\n",
            "Train Epoch: 55 [120/700 (17%)]\tLoss: 163.386200\n",
            "Train Epoch: 55 [160/700 (23%)]\tLoss: 117.180550\n",
            "Train Epoch: 55 [200/700 (29%)]\tLoss: 146.514923\n",
            "Train Epoch: 55 [240/700 (34%)]\tLoss: 161.419922\n",
            "Train Epoch: 55 [280/700 (40%)]\tLoss: 156.626328\n",
            "Train Epoch: 55 [320/700 (46%)]\tLoss: 137.533661\n",
            "Train Epoch: 55 [360/700 (51%)]\tLoss: 118.972069\n",
            "Train Epoch: 55 [400/700 (57%)]\tLoss: 152.788483\n",
            "Train Epoch: 55 [440/700 (63%)]\tLoss: 108.645508\n",
            "Train Epoch: 55 [480/700 (69%)]\tLoss: 192.913956\n",
            "Train Epoch: 55 [520/700 (74%)]\tLoss: 294.843811\n",
            "Train Epoch: 55 [560/700 (80%)]\tLoss: 109.101028\n",
            "Train Epoch: 55 [600/700 (86%)]\tLoss: 190.522369\n",
            "Train Epoch: 55 [640/700 (91%)]\tLoss: 267.347992\n",
            "Train Epoch: 55 [680/700 (97%)]\tLoss: 200.864258\n",
            "\n",
            "Test set: Average loss: 8.2594\n",
            "\n",
            "Train Epoch: 56 [0/700 (0%)]\tLoss: 158.030579\n",
            "Train Epoch: 56 [40/700 (6%)]\tLoss: 123.601395\n",
            "Train Epoch: 56 [80/700 (11%)]\tLoss: 126.989120\n",
            "Train Epoch: 56 [120/700 (17%)]\tLoss: 247.658005\n",
            "Train Epoch: 56 [160/700 (23%)]\tLoss: 171.546387\n",
            "Train Epoch: 56 [200/700 (29%)]\tLoss: 162.370529\n",
            "Train Epoch: 56 [240/700 (34%)]\tLoss: 163.361572\n",
            "Train Epoch: 56 [280/700 (40%)]\tLoss: 141.393494\n",
            "Train Epoch: 56 [320/700 (46%)]\tLoss: 182.880783\n",
            "Train Epoch: 56 [360/700 (51%)]\tLoss: 206.319641\n",
            "Train Epoch: 56 [400/700 (57%)]\tLoss: 130.757278\n",
            "Train Epoch: 56 [440/700 (63%)]\tLoss: 209.358826\n",
            "Train Epoch: 56 [480/700 (69%)]\tLoss: 240.194809\n",
            "Train Epoch: 56 [520/700 (74%)]\tLoss: 263.527313\n",
            "Train Epoch: 56 [560/700 (80%)]\tLoss: 138.188538\n",
            "Train Epoch: 56 [600/700 (86%)]\tLoss: 99.034401\n",
            "Train Epoch: 56 [640/700 (91%)]\tLoss: 127.129227\n",
            "Train Epoch: 56 [680/700 (97%)]\tLoss: 103.924026\n",
            "\n",
            "Test set: Average loss: 8.2230\n",
            "\n",
            "Train Epoch: 57 [0/700 (0%)]\tLoss: 190.638718\n",
            "Train Epoch: 57 [40/700 (6%)]\tLoss: 133.851425\n",
            "Train Epoch: 57 [80/700 (11%)]\tLoss: 207.561081\n",
            "Train Epoch: 57 [120/700 (17%)]\tLoss: 187.756897\n",
            "Train Epoch: 57 [160/700 (23%)]\tLoss: 152.387833\n",
            "Train Epoch: 57 [200/700 (29%)]\tLoss: 198.420959\n",
            "Train Epoch: 57 [240/700 (34%)]\tLoss: 171.378937\n",
            "Train Epoch: 57 [280/700 (40%)]\tLoss: 172.316544\n",
            "Train Epoch: 57 [320/700 (46%)]\tLoss: 213.711700\n",
            "Train Epoch: 57 [360/700 (51%)]\tLoss: 190.512558\n",
            "Train Epoch: 57 [400/700 (57%)]\tLoss: 139.262344\n",
            "Train Epoch: 57 [440/700 (63%)]\tLoss: 215.594818\n",
            "Train Epoch: 57 [480/700 (69%)]\tLoss: 116.005722\n",
            "Train Epoch: 57 [520/700 (74%)]\tLoss: 188.820847\n",
            "Train Epoch: 57 [560/700 (80%)]\tLoss: 199.637024\n",
            "Train Epoch: 57 [600/700 (86%)]\tLoss: 95.813286\n",
            "Train Epoch: 57 [640/700 (91%)]\tLoss: 203.608170\n",
            "Train Epoch: 57 [680/700 (97%)]\tLoss: 180.814041\n",
            "\n",
            "Test set: Average loss: 8.3606\n",
            "\n",
            "Train Epoch: 58 [0/700 (0%)]\tLoss: 135.311661\n",
            "Train Epoch: 58 [40/700 (6%)]\tLoss: 117.706398\n",
            "Train Epoch: 58 [80/700 (11%)]\tLoss: 168.338516\n",
            "Train Epoch: 58 [120/700 (17%)]\tLoss: 163.375229\n",
            "Train Epoch: 58 [160/700 (23%)]\tLoss: 181.421738\n",
            "Train Epoch: 58 [200/700 (29%)]\tLoss: 188.032501\n",
            "Train Epoch: 58 [240/700 (34%)]\tLoss: 228.730576\n",
            "Train Epoch: 58 [280/700 (40%)]\tLoss: 254.101883\n",
            "Train Epoch: 58 [320/700 (46%)]\tLoss: 170.057022\n",
            "Train Epoch: 58 [360/700 (51%)]\tLoss: 167.337326\n",
            "Train Epoch: 58 [400/700 (57%)]\tLoss: 136.874451\n",
            "Train Epoch: 58 [440/700 (63%)]\tLoss: 167.817352\n",
            "Train Epoch: 58 [480/700 (69%)]\tLoss: 116.647194\n",
            "Train Epoch: 58 [520/700 (74%)]\tLoss: 190.444687\n",
            "Train Epoch: 58 [560/700 (80%)]\tLoss: 246.229019\n",
            "Train Epoch: 58 [600/700 (86%)]\tLoss: 131.251114\n",
            "Train Epoch: 58 [640/700 (91%)]\tLoss: 165.264954\n",
            "Train Epoch: 58 [680/700 (97%)]\tLoss: 142.571228\n",
            "\n",
            "Test set: Average loss: 8.1929\n",
            "\n",
            "Train Epoch: 59 [0/700 (0%)]\tLoss: 177.774948\n",
            "Train Epoch: 59 [40/700 (6%)]\tLoss: 103.263458\n",
            "Train Epoch: 59 [80/700 (11%)]\tLoss: 99.236366\n",
            "Train Epoch: 59 [120/700 (17%)]\tLoss: 173.436478\n",
            "Train Epoch: 59 [160/700 (23%)]\tLoss: 129.866898\n",
            "Train Epoch: 59 [200/700 (29%)]\tLoss: 271.616638\n",
            "Train Epoch: 59 [240/700 (34%)]\tLoss: 162.961655\n",
            "Train Epoch: 59 [280/700 (40%)]\tLoss: 164.778290\n",
            "Train Epoch: 59 [320/700 (46%)]\tLoss: 178.673401\n",
            "Train Epoch: 59 [360/700 (51%)]\tLoss: 126.021927\n",
            "Train Epoch: 59 [400/700 (57%)]\tLoss: 191.938202\n",
            "Train Epoch: 59 [440/700 (63%)]\tLoss: 231.024124\n",
            "Train Epoch: 59 [480/700 (69%)]\tLoss: 220.914627\n",
            "Train Epoch: 59 [520/700 (74%)]\tLoss: 151.833633\n",
            "Train Epoch: 59 [560/700 (80%)]\tLoss: 112.760880\n",
            "Train Epoch: 59 [600/700 (86%)]\tLoss: 146.684784\n",
            "Train Epoch: 59 [640/700 (91%)]\tLoss: 205.151901\n",
            "Train Epoch: 59 [680/700 (97%)]\tLoss: 229.387253\n",
            "\n",
            "Test set: Average loss: 8.1842\n",
            "\n",
            "Train Epoch: 60 [0/700 (0%)]\tLoss: 219.344238\n",
            "Train Epoch: 60 [40/700 (6%)]\tLoss: 184.289337\n",
            "Train Epoch: 60 [80/700 (11%)]\tLoss: 160.293274\n",
            "Train Epoch: 60 [120/700 (17%)]\tLoss: 170.824554\n",
            "Train Epoch: 60 [160/700 (23%)]\tLoss: 121.603668\n",
            "Train Epoch: 60 [200/700 (29%)]\tLoss: 125.651939\n",
            "Train Epoch: 60 [240/700 (34%)]\tLoss: 171.018311\n",
            "Train Epoch: 60 [280/700 (40%)]\tLoss: 116.416718\n",
            "Train Epoch: 60 [320/700 (46%)]\tLoss: 199.916824\n",
            "Train Epoch: 60 [360/700 (51%)]\tLoss: 168.382889\n",
            "Train Epoch: 60 [400/700 (57%)]\tLoss: 202.533112\n",
            "Train Epoch: 60 [440/700 (63%)]\tLoss: 156.108047\n",
            "Train Epoch: 60 [480/700 (69%)]\tLoss: 183.713898\n",
            "Train Epoch: 60 [520/700 (74%)]\tLoss: 124.200684\n",
            "Train Epoch: 60 [560/700 (80%)]\tLoss: 143.288864\n",
            "Train Epoch: 60 [600/700 (86%)]\tLoss: 150.520691\n",
            "Train Epoch: 60 [640/700 (91%)]\tLoss: 177.297104\n",
            "Train Epoch: 60 [680/700 (97%)]\tLoss: 180.248550\n",
            "\n",
            "Test set: Average loss: 8.2036\n",
            "\n",
            "Train Epoch: 61 [0/700 (0%)]\tLoss: 140.742157\n",
            "Train Epoch: 61 [40/700 (6%)]\tLoss: 177.515884\n",
            "Train Epoch: 61 [80/700 (11%)]\tLoss: 150.123276\n",
            "Train Epoch: 61 [120/700 (17%)]\tLoss: 179.869980\n",
            "Train Epoch: 61 [160/700 (23%)]\tLoss: 194.366241\n",
            "Train Epoch: 61 [200/700 (29%)]\tLoss: 199.030823\n",
            "Train Epoch: 61 [240/700 (34%)]\tLoss: 163.545349\n",
            "Train Epoch: 61 [280/700 (40%)]\tLoss: 145.983871\n",
            "Train Epoch: 61 [320/700 (46%)]\tLoss: 235.459274\n",
            "Train Epoch: 61 [360/700 (51%)]\tLoss: 179.244614\n",
            "Train Epoch: 61 [400/700 (57%)]\tLoss: 196.895584\n",
            "Train Epoch: 61 [440/700 (63%)]\tLoss: 149.894211\n",
            "Train Epoch: 61 [480/700 (69%)]\tLoss: 176.736267\n",
            "Train Epoch: 61 [520/700 (74%)]\tLoss: 179.547882\n",
            "Train Epoch: 61 [560/700 (80%)]\tLoss: 209.581924\n",
            "Train Epoch: 61 [600/700 (86%)]\tLoss: 162.784836\n",
            "Train Epoch: 61 [640/700 (91%)]\tLoss: 164.257996\n",
            "Train Epoch: 61 [680/700 (97%)]\tLoss: 254.566635\n",
            "\n",
            "Test set: Average loss: 8.1557\n",
            "\n",
            "Train Epoch: 62 [0/700 (0%)]\tLoss: 176.715103\n",
            "Train Epoch: 62 [40/700 (6%)]\tLoss: 123.702682\n",
            "Train Epoch: 62 [80/700 (11%)]\tLoss: 145.994690\n",
            "Train Epoch: 62 [120/700 (17%)]\tLoss: 209.069687\n",
            "Train Epoch: 62 [160/700 (23%)]\tLoss: 214.301666\n",
            "Train Epoch: 62 [200/700 (29%)]\tLoss: 168.611893\n",
            "Train Epoch: 62 [240/700 (34%)]\tLoss: 193.746338\n",
            "Train Epoch: 62 [280/700 (40%)]\tLoss: 181.684464\n",
            "Train Epoch: 62 [320/700 (46%)]\tLoss: 170.978058\n",
            "Train Epoch: 62 [360/700 (51%)]\tLoss: 176.572968\n",
            "Train Epoch: 62 [400/700 (57%)]\tLoss: 192.861786\n",
            "Train Epoch: 62 [440/700 (63%)]\tLoss: 145.337784\n",
            "Train Epoch: 62 [480/700 (69%)]\tLoss: 132.778397\n",
            "Train Epoch: 62 [520/700 (74%)]\tLoss: 168.267090\n",
            "Train Epoch: 62 [560/700 (80%)]\tLoss: 251.956207\n",
            "Train Epoch: 62 [600/700 (86%)]\tLoss: 120.151749\n",
            "Train Epoch: 62 [640/700 (91%)]\tLoss: 223.199875\n",
            "Train Epoch: 62 [680/700 (97%)]\tLoss: 136.789108\n",
            "\n",
            "Test set: Average loss: 8.0247\n",
            "\n",
            "Train Epoch: 63 [0/700 (0%)]\tLoss: 233.179550\n",
            "Train Epoch: 63 [40/700 (6%)]\tLoss: 232.220840\n",
            "Train Epoch: 63 [80/700 (11%)]\tLoss: 158.822571\n",
            "Train Epoch: 63 [120/700 (17%)]\tLoss: 120.303490\n",
            "Train Epoch: 63 [160/700 (23%)]\tLoss: 150.545944\n",
            "Train Epoch: 63 [200/700 (29%)]\tLoss: 169.108398\n",
            "Train Epoch: 63 [240/700 (34%)]\tLoss: 174.779465\n",
            "Train Epoch: 63 [280/700 (40%)]\tLoss: 125.787140\n",
            "Train Epoch: 63 [320/700 (46%)]\tLoss: 199.809021\n",
            "Train Epoch: 63 [360/700 (51%)]\tLoss: 206.400589\n",
            "Train Epoch: 63 [400/700 (57%)]\tLoss: 179.520065\n",
            "Train Epoch: 63 [440/700 (63%)]\tLoss: 152.919174\n",
            "Train Epoch: 63 [480/700 (69%)]\tLoss: 177.230453\n",
            "Train Epoch: 63 [520/700 (74%)]\tLoss: 129.312698\n",
            "Train Epoch: 63 [560/700 (80%)]\tLoss: 190.849838\n",
            "Train Epoch: 63 [600/700 (86%)]\tLoss: 124.998207\n",
            "Train Epoch: 63 [640/700 (91%)]\tLoss: 279.883209\n",
            "Train Epoch: 63 [680/700 (97%)]\tLoss: 112.421349\n",
            "\n",
            "Test set: Average loss: 8.1242\n",
            "\n",
            "Train Epoch: 64 [0/700 (0%)]\tLoss: 185.734467\n",
            "Train Epoch: 64 [40/700 (6%)]\tLoss: 256.669952\n",
            "Train Epoch: 64 [80/700 (11%)]\tLoss: 122.047821\n",
            "Train Epoch: 64 [120/700 (17%)]\tLoss: 247.407669\n",
            "Train Epoch: 64 [160/700 (23%)]\tLoss: 144.602310\n",
            "Train Epoch: 64 [200/700 (29%)]\tLoss: 185.492874\n",
            "Train Epoch: 64 [240/700 (34%)]\tLoss: 166.171677\n",
            "Train Epoch: 64 [280/700 (40%)]\tLoss: 139.370392\n",
            "Train Epoch: 64 [320/700 (46%)]\tLoss: 197.946747\n",
            "Train Epoch: 64 [360/700 (51%)]\tLoss: 138.952835\n",
            "Train Epoch: 64 [400/700 (57%)]\tLoss: 183.628265\n",
            "Train Epoch: 64 [440/700 (63%)]\tLoss: 265.001343\n",
            "Train Epoch: 64 [480/700 (69%)]\tLoss: 185.681076\n",
            "Train Epoch: 64 [520/700 (74%)]\tLoss: 168.273193\n",
            "Train Epoch: 64 [560/700 (80%)]\tLoss: 92.293320\n",
            "Train Epoch: 64 [600/700 (86%)]\tLoss: 153.368134\n",
            "Train Epoch: 64 [640/700 (91%)]\tLoss: 168.135559\n",
            "Train Epoch: 64 [680/700 (97%)]\tLoss: 108.579124\n",
            "\n",
            "Test set: Average loss: 8.4575\n",
            "\n",
            "Train Epoch: 65 [0/700 (0%)]\tLoss: 129.340744\n",
            "Train Epoch: 65 [40/700 (6%)]\tLoss: 190.168503\n",
            "Train Epoch: 65 [80/700 (11%)]\tLoss: 184.800430\n",
            "Train Epoch: 65 [120/700 (17%)]\tLoss: 122.652603\n",
            "Train Epoch: 65 [160/700 (23%)]\tLoss: 61.088482\n",
            "Train Epoch: 65 [200/700 (29%)]\tLoss: 175.362595\n",
            "Train Epoch: 65 [240/700 (34%)]\tLoss: 234.668671\n",
            "Train Epoch: 65 [280/700 (40%)]\tLoss: 180.096512\n",
            "Train Epoch: 65 [320/700 (46%)]\tLoss: 169.960953\n",
            "Train Epoch: 65 [360/700 (51%)]\tLoss: 137.997604\n",
            "Train Epoch: 65 [400/700 (57%)]\tLoss: 161.763763\n",
            "Train Epoch: 65 [440/700 (63%)]\tLoss: 157.340179\n",
            "Train Epoch: 65 [480/700 (69%)]\tLoss: 260.868042\n",
            "Train Epoch: 65 [520/700 (74%)]\tLoss: 206.398056\n",
            "Train Epoch: 65 [560/700 (80%)]\tLoss: 238.143738\n",
            "Train Epoch: 65 [600/700 (86%)]\tLoss: 127.635727\n",
            "Train Epoch: 65 [640/700 (91%)]\tLoss: 161.806305\n",
            "Train Epoch: 65 [680/700 (97%)]\tLoss: 161.604446\n",
            "\n",
            "Test set: Average loss: 8.1257\n",
            "\n",
            "Train Epoch: 66 [0/700 (0%)]\tLoss: 150.196472\n",
            "Train Epoch: 66 [40/700 (6%)]\tLoss: 207.892731\n",
            "Train Epoch: 66 [80/700 (11%)]\tLoss: 129.611862\n",
            "Train Epoch: 66 [120/700 (17%)]\tLoss: 105.934929\n",
            "Train Epoch: 66 [160/700 (23%)]\tLoss: 164.428452\n",
            "Train Epoch: 66 [200/700 (29%)]\tLoss: 147.215500\n",
            "Train Epoch: 66 [240/700 (34%)]\tLoss: 251.381699\n",
            "Train Epoch: 66 [280/700 (40%)]\tLoss: 226.679474\n",
            "Train Epoch: 66 [320/700 (46%)]\tLoss: 203.611725\n",
            "Train Epoch: 66 [360/700 (51%)]\tLoss: 146.840927\n",
            "Train Epoch: 66 [400/700 (57%)]\tLoss: 164.034073\n",
            "Train Epoch: 66 [440/700 (63%)]\tLoss: 136.218323\n",
            "Train Epoch: 66 [480/700 (69%)]\tLoss: 167.074890\n",
            "Train Epoch: 66 [520/700 (74%)]\tLoss: 176.348541\n",
            "Train Epoch: 66 [560/700 (80%)]\tLoss: 192.386749\n",
            "Train Epoch: 66 [600/700 (86%)]\tLoss: 188.538559\n",
            "Train Epoch: 66 [640/700 (91%)]\tLoss: 194.066650\n",
            "Train Epoch: 66 [680/700 (97%)]\tLoss: 160.568359\n",
            "\n",
            "Test set: Average loss: 8.1379\n",
            "\n",
            "Train Epoch: 67 [0/700 (0%)]\tLoss: 90.269562\n",
            "Train Epoch: 67 [40/700 (6%)]\tLoss: 194.361328\n",
            "Train Epoch: 67 [80/700 (11%)]\tLoss: 159.235199\n",
            "Train Epoch: 67 [120/700 (17%)]\tLoss: 171.544235\n",
            "Train Epoch: 67 [160/700 (23%)]\tLoss: 157.038300\n",
            "Train Epoch: 67 [200/700 (29%)]\tLoss: 157.183746\n",
            "Train Epoch: 67 [240/700 (34%)]\tLoss: 341.984070\n",
            "Train Epoch: 67 [280/700 (40%)]\tLoss: 201.813065\n",
            "Train Epoch: 67 [320/700 (46%)]\tLoss: 142.017517\n",
            "Train Epoch: 67 [360/700 (51%)]\tLoss: 163.347900\n",
            "Train Epoch: 67 [400/700 (57%)]\tLoss: 181.220200\n",
            "Train Epoch: 67 [440/700 (63%)]\tLoss: 126.530243\n",
            "Train Epoch: 67 [480/700 (69%)]\tLoss: 142.798111\n",
            "Train Epoch: 67 [520/700 (74%)]\tLoss: 126.642853\n",
            "Train Epoch: 67 [560/700 (80%)]\tLoss: 240.826706\n",
            "Train Epoch: 67 [600/700 (86%)]\tLoss: 96.534828\n",
            "Train Epoch: 67 [640/700 (91%)]\tLoss: 68.089455\n",
            "Train Epoch: 67 [680/700 (97%)]\tLoss: 190.847488\n",
            "\n",
            "Test set: Average loss: 8.4900\n",
            "\n",
            "Train Epoch: 68 [0/700 (0%)]\tLoss: 198.595490\n",
            "Train Epoch: 68 [40/700 (6%)]\tLoss: 173.036972\n",
            "Train Epoch: 68 [80/700 (11%)]\tLoss: 107.726044\n",
            "Train Epoch: 68 [120/700 (17%)]\tLoss: 145.592575\n",
            "Train Epoch: 68 [160/700 (23%)]\tLoss: 128.799210\n",
            "Train Epoch: 68 [200/700 (29%)]\tLoss: 149.502625\n",
            "Train Epoch: 68 [240/700 (34%)]\tLoss: 195.517380\n",
            "Train Epoch: 68 [280/700 (40%)]\tLoss: 226.901169\n",
            "Train Epoch: 68 [320/700 (46%)]\tLoss: 234.408463\n",
            "Train Epoch: 68 [360/700 (51%)]\tLoss: 195.677689\n",
            "Train Epoch: 68 [400/700 (57%)]\tLoss: 239.008270\n",
            "Train Epoch: 68 [440/700 (63%)]\tLoss: 186.701218\n",
            "Train Epoch: 68 [480/700 (69%)]\tLoss: 138.793839\n",
            "Train Epoch: 68 [520/700 (74%)]\tLoss: 174.496201\n",
            "Train Epoch: 68 [560/700 (80%)]\tLoss: 129.384216\n",
            "Train Epoch: 68 [600/700 (86%)]\tLoss: 211.179535\n",
            "Train Epoch: 68 [640/700 (91%)]\tLoss: 184.469589\n",
            "Train Epoch: 68 [680/700 (97%)]\tLoss: 150.878250\n",
            "\n",
            "Test set: Average loss: 8.2422\n",
            "\n",
            "Train Epoch: 69 [0/700 (0%)]\tLoss: 132.309616\n",
            "Train Epoch: 69 [40/700 (6%)]\tLoss: 238.397629\n",
            "Train Epoch: 69 [80/700 (11%)]\tLoss: 192.587784\n",
            "Train Epoch: 69 [120/700 (17%)]\tLoss: 146.039597\n",
            "Train Epoch: 69 [160/700 (23%)]\tLoss: 166.158096\n",
            "Train Epoch: 69 [200/700 (29%)]\tLoss: 130.252121\n",
            "Train Epoch: 69 [240/700 (34%)]\tLoss: 168.457703\n",
            "Train Epoch: 69 [280/700 (40%)]\tLoss: 160.797607\n",
            "Train Epoch: 69 [320/700 (46%)]\tLoss: 268.451874\n",
            "Train Epoch: 69 [360/700 (51%)]\tLoss: 108.209381\n",
            "Train Epoch: 69 [400/700 (57%)]\tLoss: 217.534988\n",
            "Train Epoch: 69 [440/700 (63%)]\tLoss: 207.072830\n",
            "Train Epoch: 69 [480/700 (69%)]\tLoss: 132.040817\n",
            "Train Epoch: 69 [520/700 (74%)]\tLoss: 190.047256\n",
            "Train Epoch: 69 [560/700 (80%)]\tLoss: 105.240814\n",
            "Train Epoch: 69 [600/700 (86%)]\tLoss: 144.630371\n",
            "Train Epoch: 69 [640/700 (91%)]\tLoss: 107.192108\n",
            "Train Epoch: 69 [680/700 (97%)]\tLoss: 206.115860\n",
            "\n",
            "Test set: Average loss: 8.0276\n",
            "\n",
            "Train Epoch: 70 [0/700 (0%)]\tLoss: 115.684601\n",
            "Train Epoch: 70 [40/700 (6%)]\tLoss: 136.934799\n",
            "Train Epoch: 70 [80/700 (11%)]\tLoss: 265.587067\n",
            "Train Epoch: 70 [120/700 (17%)]\tLoss: 153.815323\n",
            "Train Epoch: 70 [160/700 (23%)]\tLoss: 163.897720\n",
            "Train Epoch: 70 [200/700 (29%)]\tLoss: 198.031677\n",
            "Train Epoch: 70 [240/700 (34%)]\tLoss: 196.567490\n",
            "Train Epoch: 70 [280/700 (40%)]\tLoss: 196.781281\n",
            "Train Epoch: 70 [320/700 (46%)]\tLoss: 220.516373\n",
            "Train Epoch: 70 [360/700 (51%)]\tLoss: 170.535873\n",
            "Train Epoch: 70 [400/700 (57%)]\tLoss: 188.754883\n",
            "Train Epoch: 70 [440/700 (63%)]\tLoss: 176.016602\n",
            "Train Epoch: 70 [480/700 (69%)]\tLoss: 198.578827\n",
            "Train Epoch: 70 [520/700 (74%)]\tLoss: 212.406982\n",
            "Train Epoch: 70 [560/700 (80%)]\tLoss: 125.636360\n",
            "Train Epoch: 70 [600/700 (86%)]\tLoss: 216.387955\n",
            "Train Epoch: 70 [640/700 (91%)]\tLoss: 120.400620\n",
            "Train Epoch: 70 [680/700 (97%)]\tLoss: 155.218796\n",
            "\n",
            "Test set: Average loss: 8.1483\n",
            "\n",
            "Train Epoch: 71 [0/700 (0%)]\tLoss: 189.233078\n",
            "Train Epoch: 71 [40/700 (6%)]\tLoss: 127.593391\n",
            "Train Epoch: 71 [80/700 (11%)]\tLoss: 73.347755\n",
            "Train Epoch: 71 [120/700 (17%)]\tLoss: 197.308304\n",
            "Train Epoch: 71 [160/700 (23%)]\tLoss: 158.114487\n",
            "Train Epoch: 71 [200/700 (29%)]\tLoss: 162.522232\n",
            "Train Epoch: 71 [240/700 (34%)]\tLoss: 153.132996\n",
            "Train Epoch: 71 [280/700 (40%)]\tLoss: 138.986542\n",
            "Train Epoch: 71 [320/700 (46%)]\tLoss: 206.436005\n",
            "Train Epoch: 71 [360/700 (51%)]\tLoss: 267.583191\n",
            "Train Epoch: 71 [400/700 (57%)]\tLoss: 161.796967\n",
            "Train Epoch: 71 [440/700 (63%)]\tLoss: 174.420380\n",
            "Train Epoch: 71 [480/700 (69%)]\tLoss: 166.462250\n",
            "Train Epoch: 71 [520/700 (74%)]\tLoss: 179.996399\n",
            "Train Epoch: 71 [560/700 (80%)]\tLoss: 162.008469\n",
            "Train Epoch: 71 [600/700 (86%)]\tLoss: 157.890839\n",
            "Train Epoch: 71 [640/700 (91%)]\tLoss: 157.060242\n",
            "Train Epoch: 71 [680/700 (97%)]\tLoss: 182.433014\n",
            "\n",
            "Test set: Average loss: 8.1958\n",
            "\n",
            "Train Epoch: 72 [0/700 (0%)]\tLoss: 226.594940\n",
            "Train Epoch: 72 [40/700 (6%)]\tLoss: 120.232010\n",
            "Train Epoch: 72 [80/700 (11%)]\tLoss: 137.520493\n",
            "Train Epoch: 72 [120/700 (17%)]\tLoss: 190.901474\n",
            "Train Epoch: 72 [160/700 (23%)]\tLoss: 119.976097\n",
            "Train Epoch: 72 [200/700 (29%)]\tLoss: 164.313812\n",
            "Train Epoch: 72 [240/700 (34%)]\tLoss: 97.713661\n",
            "Train Epoch: 72 [280/700 (40%)]\tLoss: 107.402512\n",
            "Train Epoch: 72 [320/700 (46%)]\tLoss: 144.614334\n",
            "Train Epoch: 72 [360/700 (51%)]\tLoss: 252.843369\n",
            "Train Epoch: 72 [400/700 (57%)]\tLoss: 152.011719\n",
            "Train Epoch: 72 [440/700 (63%)]\tLoss: 130.537247\n",
            "Train Epoch: 72 [480/700 (69%)]\tLoss: 151.827179\n",
            "Train Epoch: 72 [520/700 (74%)]\tLoss: 157.631485\n",
            "Train Epoch: 72 [560/700 (80%)]\tLoss: 177.084610\n",
            "Train Epoch: 72 [600/700 (86%)]\tLoss: 171.612991\n",
            "Train Epoch: 72 [640/700 (91%)]\tLoss: 176.316467\n",
            "Train Epoch: 72 [680/700 (97%)]\tLoss: 118.102074\n",
            "\n",
            "Test set: Average loss: 8.1340\n",
            "\n",
            "Train Epoch: 73 [0/700 (0%)]\tLoss: 215.987488\n",
            "Train Epoch: 73 [40/700 (6%)]\tLoss: 108.729836\n",
            "Train Epoch: 73 [80/700 (11%)]\tLoss: 176.169083\n",
            "Train Epoch: 73 [120/700 (17%)]\tLoss: 213.569427\n",
            "Train Epoch: 73 [160/700 (23%)]\tLoss: 182.970108\n",
            "Train Epoch: 73 [200/700 (29%)]\tLoss: 181.682068\n",
            "Train Epoch: 73 [240/700 (34%)]\tLoss: 225.262268\n",
            "Train Epoch: 73 [280/700 (40%)]\tLoss: 173.530945\n",
            "Train Epoch: 73 [320/700 (46%)]\tLoss: 232.059372\n",
            "Train Epoch: 73 [360/700 (51%)]\tLoss: 166.667374\n",
            "Train Epoch: 73 [400/700 (57%)]\tLoss: 181.706787\n",
            "Train Epoch: 73 [440/700 (63%)]\tLoss: 145.332138\n",
            "Train Epoch: 73 [480/700 (69%)]\tLoss: 235.126602\n",
            "Train Epoch: 73 [520/700 (74%)]\tLoss: 89.263161\n",
            "Train Epoch: 73 [560/700 (80%)]\tLoss: 128.125519\n",
            "Train Epoch: 73 [600/700 (86%)]\tLoss: 277.313416\n",
            "Train Epoch: 73 [640/700 (91%)]\tLoss: 161.610535\n",
            "Train Epoch: 73 [680/700 (97%)]\tLoss: 200.475159\n",
            "\n",
            "Test set: Average loss: 8.2513\n",
            "\n",
            "Train Epoch: 74 [0/700 (0%)]\tLoss: 108.152069\n",
            "Train Epoch: 74 [40/700 (6%)]\tLoss: 177.337555\n",
            "Train Epoch: 74 [80/700 (11%)]\tLoss: 206.375031\n",
            "Train Epoch: 74 [120/700 (17%)]\tLoss: 128.550018\n",
            "Train Epoch: 74 [160/700 (23%)]\tLoss: 173.828857\n",
            "Train Epoch: 74 [200/700 (29%)]\tLoss: 192.685211\n",
            "Train Epoch: 74 [240/700 (34%)]\tLoss: 106.023041\n",
            "Train Epoch: 74 [280/700 (40%)]\tLoss: 166.324448\n",
            "Train Epoch: 74 [320/700 (46%)]\tLoss: 156.163330\n",
            "Train Epoch: 74 [360/700 (51%)]\tLoss: 142.623566\n",
            "Train Epoch: 74 [400/700 (57%)]\tLoss: 154.553970\n",
            "Train Epoch: 74 [440/700 (63%)]\tLoss: 109.631493\n",
            "Train Epoch: 74 [480/700 (69%)]\tLoss: 223.334808\n",
            "Train Epoch: 74 [520/700 (74%)]\tLoss: 224.031662\n",
            "Train Epoch: 74 [560/700 (80%)]\tLoss: 197.305191\n",
            "Train Epoch: 74 [600/700 (86%)]\tLoss: 161.249832\n",
            "Train Epoch: 74 [640/700 (91%)]\tLoss: 270.762726\n",
            "Train Epoch: 74 [680/700 (97%)]\tLoss: 222.804749\n",
            "\n",
            "Test set: Average loss: 8.1786\n",
            "\n",
            "Train Epoch: 75 [0/700 (0%)]\tLoss: 251.343216\n",
            "Train Epoch: 75 [40/700 (6%)]\tLoss: 159.744843\n",
            "Train Epoch: 75 [80/700 (11%)]\tLoss: 140.940826\n",
            "Train Epoch: 75 [120/700 (17%)]\tLoss: 184.052536\n",
            "Train Epoch: 75 [160/700 (23%)]\tLoss: 188.757538\n",
            "Train Epoch: 75 [200/700 (29%)]\tLoss: 250.302017\n",
            "Train Epoch: 75 [240/700 (34%)]\tLoss: 168.436188\n",
            "Train Epoch: 75 [280/700 (40%)]\tLoss: 219.662979\n",
            "Train Epoch: 75 [320/700 (46%)]\tLoss: 133.641998\n",
            "Train Epoch: 75 [360/700 (51%)]\tLoss: 124.925560\n",
            "Train Epoch: 75 [400/700 (57%)]\tLoss: 187.121292\n",
            "Train Epoch: 75 [440/700 (63%)]\tLoss: 148.073883\n",
            "Train Epoch: 75 [480/700 (69%)]\tLoss: 191.870148\n",
            "Train Epoch: 75 [520/700 (74%)]\tLoss: 145.183304\n",
            "Train Epoch: 75 [560/700 (80%)]\tLoss: 75.815727\n",
            "Train Epoch: 75 [600/700 (86%)]\tLoss: 169.424896\n",
            "Train Epoch: 75 [640/700 (91%)]\tLoss: 114.359474\n",
            "Train Epoch: 75 [680/700 (97%)]\tLoss: 124.597237\n",
            "\n",
            "Test set: Average loss: 8.2654\n",
            "\n",
            "Train Epoch: 76 [0/700 (0%)]\tLoss: 183.572311\n",
            "Train Epoch: 76 [40/700 (6%)]\tLoss: 134.752274\n",
            "Train Epoch: 76 [80/700 (11%)]\tLoss: 128.480392\n",
            "Train Epoch: 76 [120/700 (17%)]\tLoss: 138.893616\n",
            "Train Epoch: 76 [160/700 (23%)]\tLoss: 134.336090\n",
            "Train Epoch: 76 [200/700 (29%)]\tLoss: 103.075516\n",
            "Train Epoch: 76 [240/700 (34%)]\tLoss: 161.940826\n",
            "Train Epoch: 76 [280/700 (40%)]\tLoss: 151.933167\n",
            "Train Epoch: 76 [320/700 (46%)]\tLoss: 262.491302\n",
            "Train Epoch: 76 [360/700 (51%)]\tLoss: 185.983841\n",
            "Train Epoch: 76 [400/700 (57%)]\tLoss: 181.672684\n",
            "Train Epoch: 76 [440/700 (63%)]\tLoss: 186.215332\n",
            "Train Epoch: 76 [480/700 (69%)]\tLoss: 191.069397\n",
            "Train Epoch: 76 [520/700 (74%)]\tLoss: 193.418884\n",
            "Train Epoch: 76 [560/700 (80%)]\tLoss: 207.440201\n",
            "Train Epoch: 76 [600/700 (86%)]\tLoss: 153.655121\n",
            "Train Epoch: 76 [640/700 (91%)]\tLoss: 162.307556\n",
            "Train Epoch: 76 [680/700 (97%)]\tLoss: 173.253998\n",
            "\n",
            "Test set: Average loss: 8.0656\n",
            "\n",
            "Train Epoch: 77 [0/700 (0%)]\tLoss: 224.818649\n",
            "Train Epoch: 77 [40/700 (6%)]\tLoss: 165.736496\n",
            "Train Epoch: 77 [80/700 (11%)]\tLoss: 130.388550\n",
            "Train Epoch: 77 [120/700 (17%)]\tLoss: 139.200958\n",
            "Train Epoch: 77 [160/700 (23%)]\tLoss: 128.805771\n",
            "Train Epoch: 77 [200/700 (29%)]\tLoss: 144.639572\n",
            "Train Epoch: 77 [240/700 (34%)]\tLoss: 194.716156\n",
            "Train Epoch: 77 [280/700 (40%)]\tLoss: 157.421417\n",
            "Train Epoch: 77 [320/700 (46%)]\tLoss: 200.950089\n",
            "Train Epoch: 77 [360/700 (51%)]\tLoss: 173.126389\n",
            "Train Epoch: 77 [400/700 (57%)]\tLoss: 203.830139\n",
            "Train Epoch: 77 [440/700 (63%)]\tLoss: 163.348770\n",
            "Train Epoch: 77 [480/700 (69%)]\tLoss: 161.575272\n",
            "Train Epoch: 77 [520/700 (74%)]\tLoss: 183.977676\n",
            "Train Epoch: 77 [560/700 (80%)]\tLoss: 140.995926\n",
            "Train Epoch: 77 [600/700 (86%)]\tLoss: 161.027130\n",
            "Train Epoch: 77 [640/700 (91%)]\tLoss: 229.609161\n",
            "Train Epoch: 77 [680/700 (97%)]\tLoss: 97.071632\n",
            "\n",
            "Test set: Average loss: 8.1684\n",
            "\n",
            "Train Epoch: 78 [0/700 (0%)]\tLoss: 127.563461\n",
            "Train Epoch: 78 [40/700 (6%)]\tLoss: 177.285965\n",
            "Train Epoch: 78 [80/700 (11%)]\tLoss: 256.917480\n",
            "Train Epoch: 78 [120/700 (17%)]\tLoss: 114.122406\n",
            "Train Epoch: 78 [160/700 (23%)]\tLoss: 195.985352\n",
            "Train Epoch: 78 [200/700 (29%)]\tLoss: 237.149338\n",
            "Train Epoch: 78 [240/700 (34%)]\tLoss: 180.679062\n",
            "Train Epoch: 78 [280/700 (40%)]\tLoss: 116.693810\n",
            "Train Epoch: 78 [320/700 (46%)]\tLoss: 139.925842\n",
            "Train Epoch: 78 [360/700 (51%)]\tLoss: 165.023895\n",
            "Train Epoch: 78 [400/700 (57%)]\tLoss: 202.806412\n",
            "Train Epoch: 78 [440/700 (63%)]\tLoss: 223.061142\n",
            "Train Epoch: 78 [480/700 (69%)]\tLoss: 173.620941\n",
            "Train Epoch: 78 [520/700 (74%)]\tLoss: 147.849991\n",
            "Train Epoch: 78 [560/700 (80%)]\tLoss: 170.646896\n",
            "Train Epoch: 78 [600/700 (86%)]\tLoss: 142.543381\n",
            "Train Epoch: 78 [640/700 (91%)]\tLoss: 139.930237\n",
            "Train Epoch: 78 [680/700 (97%)]\tLoss: 173.338547\n",
            "\n",
            "Test set: Average loss: 8.1475\n",
            "\n",
            "Train Epoch: 79 [0/700 (0%)]\tLoss: 216.267441\n",
            "Train Epoch: 79 [40/700 (6%)]\tLoss: 122.668686\n",
            "Train Epoch: 79 [80/700 (11%)]\tLoss: 192.129745\n",
            "Train Epoch: 79 [120/700 (17%)]\tLoss: 143.977844\n",
            "Train Epoch: 79 [160/700 (23%)]\tLoss: 146.279190\n",
            "Train Epoch: 79 [200/700 (29%)]\tLoss: 113.317589\n",
            "Train Epoch: 79 [240/700 (34%)]\tLoss: 113.797638\n",
            "Train Epoch: 79 [280/700 (40%)]\tLoss: 191.319427\n",
            "Train Epoch: 79 [320/700 (46%)]\tLoss: 160.366898\n",
            "Train Epoch: 79 [360/700 (51%)]\tLoss: 167.893097\n",
            "Train Epoch: 79 [400/700 (57%)]\tLoss: 226.368912\n",
            "Train Epoch: 79 [440/700 (63%)]\tLoss: 231.170441\n",
            "Train Epoch: 79 [480/700 (69%)]\tLoss: 190.467361\n",
            "Train Epoch: 79 [520/700 (74%)]\tLoss: 108.146095\n",
            "Train Epoch: 79 [560/700 (80%)]\tLoss: 202.353745\n",
            "Train Epoch: 79 [600/700 (86%)]\tLoss: 181.542053\n",
            "Train Epoch: 79 [640/700 (91%)]\tLoss: 197.799896\n",
            "Train Epoch: 79 [680/700 (97%)]\tLoss: 156.817703\n",
            "\n",
            "Test set: Average loss: 8.0866\n",
            "\n",
            "Train Epoch: 80 [0/700 (0%)]\tLoss: 121.708664\n",
            "Train Epoch: 80 [40/700 (6%)]\tLoss: 211.623962\n",
            "Train Epoch: 80 [80/700 (11%)]\tLoss: 169.540741\n",
            "Train Epoch: 80 [120/700 (17%)]\tLoss: 126.281563\n",
            "Train Epoch: 80 [160/700 (23%)]\tLoss: 145.471710\n",
            "Train Epoch: 80 [200/700 (29%)]\tLoss: 165.487198\n",
            "Train Epoch: 80 [240/700 (34%)]\tLoss: 242.425186\n",
            "Train Epoch: 80 [280/700 (40%)]\tLoss: 199.978516\n",
            "Train Epoch: 80 [320/700 (46%)]\tLoss: 167.315933\n",
            "Train Epoch: 80 [360/700 (51%)]\tLoss: 186.825851\n",
            "Train Epoch: 80 [400/700 (57%)]\tLoss: 129.479141\n",
            "Train Epoch: 80 [440/700 (63%)]\tLoss: 127.411331\n",
            "Train Epoch: 80 [480/700 (69%)]\tLoss: 142.804474\n",
            "Train Epoch: 80 [520/700 (74%)]\tLoss: 205.831467\n",
            "Train Epoch: 80 [560/700 (80%)]\tLoss: 186.954483\n",
            "Train Epoch: 80 [600/700 (86%)]\tLoss: 265.301117\n",
            "Train Epoch: 80 [640/700 (91%)]\tLoss: 190.434250\n",
            "Train Epoch: 80 [680/700 (97%)]\tLoss: 277.409668\n",
            "\n",
            "Test set: Average loss: 8.3291\n",
            "\n",
            "Train Epoch: 81 [0/700 (0%)]\tLoss: 99.772003\n",
            "Train Epoch: 81 [40/700 (6%)]\tLoss: 155.186310\n",
            "Train Epoch: 81 [80/700 (11%)]\tLoss: 171.795700\n",
            "Train Epoch: 81 [120/700 (17%)]\tLoss: 201.244125\n",
            "Train Epoch: 81 [160/700 (23%)]\tLoss: 201.911118\n",
            "Train Epoch: 81 [200/700 (29%)]\tLoss: 227.193420\n",
            "Train Epoch: 81 [240/700 (34%)]\tLoss: 144.009995\n",
            "Train Epoch: 81 [280/700 (40%)]\tLoss: 196.253204\n",
            "Train Epoch: 81 [320/700 (46%)]\tLoss: 129.004883\n",
            "Train Epoch: 81 [360/700 (51%)]\tLoss: 184.920319\n",
            "Train Epoch: 81 [400/700 (57%)]\tLoss: 216.800476\n",
            "Train Epoch: 81 [440/700 (63%)]\tLoss: 145.566788\n",
            "Train Epoch: 81 [480/700 (69%)]\tLoss: 171.512451\n",
            "Train Epoch: 81 [520/700 (74%)]\tLoss: 129.968369\n",
            "Train Epoch: 81 [560/700 (80%)]\tLoss: 162.121445\n",
            "Train Epoch: 81 [600/700 (86%)]\tLoss: 93.807808\n",
            "Train Epoch: 81 [640/700 (91%)]\tLoss: 177.412354\n",
            "Train Epoch: 81 [680/700 (97%)]\tLoss: 206.244003\n",
            "\n",
            "Test set: Average loss: 8.1112\n",
            "\n",
            "Train Epoch: 82 [0/700 (0%)]\tLoss: 240.271057\n",
            "Train Epoch: 82 [40/700 (6%)]\tLoss: 161.427872\n",
            "Train Epoch: 82 [80/700 (11%)]\tLoss: 170.286407\n",
            "Train Epoch: 82 [120/700 (17%)]\tLoss: 160.683853\n",
            "Train Epoch: 82 [160/700 (23%)]\tLoss: 166.657822\n",
            "Train Epoch: 82 [200/700 (29%)]\tLoss: 125.422890\n",
            "Train Epoch: 82 [240/700 (34%)]\tLoss: 232.594177\n",
            "Train Epoch: 82 [280/700 (40%)]\tLoss: 271.103790\n",
            "Train Epoch: 82 [320/700 (46%)]\tLoss: 254.505707\n",
            "Train Epoch: 82 [360/700 (51%)]\tLoss: 147.828674\n",
            "Train Epoch: 82 [400/700 (57%)]\tLoss: 112.081268\n",
            "Train Epoch: 82 [440/700 (63%)]\tLoss: 127.782623\n",
            "Train Epoch: 82 [480/700 (69%)]\tLoss: 202.409851\n",
            "Train Epoch: 82 [520/700 (74%)]\tLoss: 106.269356\n",
            "Train Epoch: 82 [560/700 (80%)]\tLoss: 167.970734\n",
            "Train Epoch: 82 [600/700 (86%)]\tLoss: 181.503311\n",
            "Train Epoch: 82 [640/700 (91%)]\tLoss: 200.352112\n",
            "Train Epoch: 82 [680/700 (97%)]\tLoss: 164.108978\n",
            "\n",
            "Test set: Average loss: 8.1469\n",
            "\n",
            "Train Epoch: 83 [0/700 (0%)]\tLoss: 189.065582\n",
            "Train Epoch: 83 [40/700 (6%)]\tLoss: 238.574234\n",
            "Train Epoch: 83 [80/700 (11%)]\tLoss: 117.282021\n",
            "Train Epoch: 83 [120/700 (17%)]\tLoss: 175.292572\n",
            "Train Epoch: 83 [160/700 (23%)]\tLoss: 112.040993\n",
            "Train Epoch: 83 [200/700 (29%)]\tLoss: 153.568375\n",
            "Train Epoch: 83 [240/700 (34%)]\tLoss: 175.281967\n",
            "Train Epoch: 83 [280/700 (40%)]\tLoss: 198.101746\n",
            "Train Epoch: 83 [320/700 (46%)]\tLoss: 162.140076\n",
            "Train Epoch: 83 [360/700 (51%)]\tLoss: 120.955132\n",
            "Train Epoch: 83 [400/700 (57%)]\tLoss: 187.868134\n",
            "Train Epoch: 83 [440/700 (63%)]\tLoss: 220.754166\n",
            "Train Epoch: 83 [480/700 (69%)]\tLoss: 95.849808\n",
            "Train Epoch: 83 [520/700 (74%)]\tLoss: 121.860458\n",
            "Train Epoch: 83 [560/700 (80%)]\tLoss: 130.386658\n",
            "Train Epoch: 83 [600/700 (86%)]\tLoss: 168.292923\n",
            "Train Epoch: 83 [640/700 (91%)]\tLoss: 123.762169\n",
            "Train Epoch: 83 [680/700 (97%)]\tLoss: 159.942276\n",
            "\n",
            "Test set: Average loss: 8.1148\n",
            "\n",
            "Train Epoch: 84 [0/700 (0%)]\tLoss: 176.663864\n",
            "Train Epoch: 84 [40/700 (6%)]\tLoss: 155.248703\n",
            "Train Epoch: 84 [80/700 (11%)]\tLoss: 98.777710\n",
            "Train Epoch: 84 [120/700 (17%)]\tLoss: 253.201889\n",
            "Train Epoch: 84 [160/700 (23%)]\tLoss: 149.576706\n",
            "Train Epoch: 84 [200/700 (29%)]\tLoss: 104.185806\n",
            "Train Epoch: 84 [240/700 (34%)]\tLoss: 96.131462\n",
            "Train Epoch: 84 [280/700 (40%)]\tLoss: 152.947556\n",
            "Train Epoch: 84 [320/700 (46%)]\tLoss: 205.806793\n",
            "Train Epoch: 84 [360/700 (51%)]\tLoss: 177.665390\n",
            "Train Epoch: 84 [400/700 (57%)]\tLoss: 159.728653\n",
            "Train Epoch: 84 [440/700 (63%)]\tLoss: 119.314056\n",
            "Train Epoch: 84 [480/700 (69%)]\tLoss: 193.708115\n",
            "Train Epoch: 84 [520/700 (74%)]\tLoss: 134.357056\n",
            "Train Epoch: 84 [560/700 (80%)]\tLoss: 154.419174\n",
            "Train Epoch: 84 [600/700 (86%)]\tLoss: 167.751923\n",
            "Train Epoch: 84 [640/700 (91%)]\tLoss: 167.352386\n",
            "Train Epoch: 84 [680/700 (97%)]\tLoss: 243.180115\n",
            "\n",
            "Test set: Average loss: 8.4840\n",
            "\n",
            "Train Epoch: 85 [0/700 (0%)]\tLoss: 203.955734\n",
            "Train Epoch: 85 [40/700 (6%)]\tLoss: 179.893875\n",
            "Train Epoch: 85 [80/700 (11%)]\tLoss: 172.814453\n",
            "Train Epoch: 85 [120/700 (17%)]\tLoss: 224.476700\n",
            "Train Epoch: 85 [160/700 (23%)]\tLoss: 170.752670\n",
            "Train Epoch: 85 [200/700 (29%)]\tLoss: 104.949066\n",
            "Train Epoch: 85 [240/700 (34%)]\tLoss: 227.873459\n",
            "Train Epoch: 85 [280/700 (40%)]\tLoss: 205.996140\n",
            "Train Epoch: 85 [320/700 (46%)]\tLoss: 130.918015\n",
            "Train Epoch: 85 [360/700 (51%)]\tLoss: 227.139786\n",
            "Train Epoch: 85 [400/700 (57%)]\tLoss: 121.929161\n",
            "Train Epoch: 85 [440/700 (63%)]\tLoss: 121.083168\n",
            "Train Epoch: 85 [480/700 (69%)]\tLoss: 155.874557\n",
            "Train Epoch: 85 [520/700 (74%)]\tLoss: 196.180878\n",
            "Train Epoch: 85 [560/700 (80%)]\tLoss: 159.200363\n",
            "Train Epoch: 85 [600/700 (86%)]\tLoss: 109.818504\n",
            "Train Epoch: 85 [640/700 (91%)]\tLoss: 180.801910\n",
            "Train Epoch: 85 [680/700 (97%)]\tLoss: 252.612579\n",
            "\n",
            "Test set: Average loss: 8.0118\n",
            "\n",
            "Train Epoch: 86 [0/700 (0%)]\tLoss: 268.508270\n",
            "Train Epoch: 86 [40/700 (6%)]\tLoss: 152.188828\n",
            "Train Epoch: 86 [80/700 (11%)]\tLoss: 137.717224\n",
            "Train Epoch: 86 [120/700 (17%)]\tLoss: 151.958267\n",
            "Train Epoch: 86 [160/700 (23%)]\tLoss: 109.289833\n",
            "Train Epoch: 86 [200/700 (29%)]\tLoss: 217.153305\n",
            "Train Epoch: 86 [240/700 (34%)]\tLoss: 180.611221\n",
            "Train Epoch: 86 [280/700 (40%)]\tLoss: 155.429626\n",
            "Train Epoch: 86 [320/700 (46%)]\tLoss: 255.834549\n",
            "Train Epoch: 86 [360/700 (51%)]\tLoss: 133.729965\n",
            "Train Epoch: 86 [400/700 (57%)]\tLoss: 98.052223\n",
            "Train Epoch: 86 [440/700 (63%)]\tLoss: 204.128525\n",
            "Train Epoch: 86 [480/700 (69%)]\tLoss: 154.366211\n",
            "Train Epoch: 86 [520/700 (74%)]\tLoss: 110.794144\n",
            "Train Epoch: 86 [560/700 (80%)]\tLoss: 200.341125\n",
            "Train Epoch: 86 [600/700 (86%)]\tLoss: 194.870926\n",
            "Train Epoch: 86 [640/700 (91%)]\tLoss: 177.904129\n",
            "Train Epoch: 86 [680/700 (97%)]\tLoss: 130.413071\n",
            "\n",
            "Test set: Average loss: 8.1829\n",
            "\n",
            "Train Epoch: 87 [0/700 (0%)]\tLoss: 204.012512\n",
            "Train Epoch: 87 [40/700 (6%)]\tLoss: 113.862076\n",
            "Train Epoch: 87 [80/700 (11%)]\tLoss: 198.594727\n",
            "Train Epoch: 87 [120/700 (17%)]\tLoss: 215.157028\n",
            "Train Epoch: 87 [160/700 (23%)]\tLoss: 190.646881\n",
            "Train Epoch: 87 [200/700 (29%)]\tLoss: 211.733887\n",
            "Train Epoch: 87 [240/700 (34%)]\tLoss: 132.599564\n",
            "Train Epoch: 87 [280/700 (40%)]\tLoss: 99.315903\n",
            "Train Epoch: 87 [320/700 (46%)]\tLoss: 177.141312\n",
            "Train Epoch: 87 [360/700 (51%)]\tLoss: 155.303894\n",
            "Train Epoch: 87 [400/700 (57%)]\tLoss: 157.604340\n",
            "Train Epoch: 87 [440/700 (63%)]\tLoss: 179.804703\n",
            "Train Epoch: 87 [480/700 (69%)]\tLoss: 180.555710\n",
            "Train Epoch: 87 [520/700 (74%)]\tLoss: 139.175873\n",
            "Train Epoch: 87 [560/700 (80%)]\tLoss: 192.251404\n",
            "Train Epoch: 87 [600/700 (86%)]\tLoss: 260.795105\n",
            "Train Epoch: 87 [640/700 (91%)]\tLoss: 220.466400\n",
            "Train Epoch: 87 [680/700 (97%)]\tLoss: 186.270370\n",
            "\n",
            "Test set: Average loss: 8.2731\n",
            "\n",
            "Train Epoch: 88 [0/700 (0%)]\tLoss: 73.149162\n",
            "Train Epoch: 88 [40/700 (6%)]\tLoss: 140.075974\n",
            "Train Epoch: 88 [80/700 (11%)]\tLoss: 171.911636\n",
            "Train Epoch: 88 [120/700 (17%)]\tLoss: 154.863770\n",
            "Train Epoch: 88 [160/700 (23%)]\tLoss: 125.810379\n",
            "Train Epoch: 88 [200/700 (29%)]\tLoss: 185.218567\n",
            "Train Epoch: 88 [240/700 (34%)]\tLoss: 220.720566\n",
            "Train Epoch: 88 [280/700 (40%)]\tLoss: 122.085594\n",
            "Train Epoch: 88 [320/700 (46%)]\tLoss: 87.947929\n",
            "Train Epoch: 88 [360/700 (51%)]\tLoss: 241.215088\n",
            "Train Epoch: 88 [400/700 (57%)]\tLoss: 175.311615\n",
            "Train Epoch: 88 [440/700 (63%)]\tLoss: 242.179977\n",
            "Train Epoch: 88 [480/700 (69%)]\tLoss: 207.677643\n",
            "Train Epoch: 88 [520/700 (74%)]\tLoss: 221.304459\n",
            "Train Epoch: 88 [560/700 (80%)]\tLoss: 183.119720\n",
            "Train Epoch: 88 [600/700 (86%)]\tLoss: 170.593750\n",
            "Train Epoch: 88 [640/700 (91%)]\tLoss: 206.436966\n",
            "Train Epoch: 88 [680/700 (97%)]\tLoss: 167.561707\n",
            "\n",
            "Test set: Average loss: 8.1396\n",
            "\n",
            "Train Epoch: 89 [0/700 (0%)]\tLoss: 151.332550\n",
            "Train Epoch: 89 [40/700 (6%)]\tLoss: 231.242996\n",
            "Train Epoch: 89 [80/700 (11%)]\tLoss: 168.143600\n",
            "Train Epoch: 89 [120/700 (17%)]\tLoss: 128.837128\n",
            "Train Epoch: 89 [160/700 (23%)]\tLoss: 147.093613\n",
            "Train Epoch: 89 [200/700 (29%)]\tLoss: 158.178421\n",
            "Train Epoch: 89 [240/700 (34%)]\tLoss: 147.224136\n",
            "Train Epoch: 89 [280/700 (40%)]\tLoss: 168.818680\n",
            "Train Epoch: 89 [320/700 (46%)]\tLoss: 139.234802\n",
            "Train Epoch: 89 [360/700 (51%)]\tLoss: 165.308044\n",
            "Train Epoch: 89 [400/700 (57%)]\tLoss: 146.736801\n",
            "Train Epoch: 89 [440/700 (63%)]\tLoss: 163.871719\n",
            "Train Epoch: 89 [480/700 (69%)]\tLoss: 186.659698\n",
            "Train Epoch: 89 [520/700 (74%)]\tLoss: 200.016663\n",
            "Train Epoch: 89 [560/700 (80%)]\tLoss: 298.594543\n",
            "Train Epoch: 89 [600/700 (86%)]\tLoss: 83.101288\n",
            "Train Epoch: 89 [640/700 (91%)]\tLoss: 216.984421\n",
            "Train Epoch: 89 [680/700 (97%)]\tLoss: 251.849670\n",
            "\n",
            "Test set: Average loss: 8.3972\n",
            "\n",
            "Train Epoch: 90 [0/700 (0%)]\tLoss: 297.224274\n",
            "Train Epoch: 90 [40/700 (6%)]\tLoss: 224.845078\n",
            "Train Epoch: 90 [80/700 (11%)]\tLoss: 228.567001\n",
            "Train Epoch: 90 [120/700 (17%)]\tLoss: 143.496658\n",
            "Train Epoch: 90 [160/700 (23%)]\tLoss: 165.584976\n",
            "Train Epoch: 90 [200/700 (29%)]\tLoss: 213.028641\n",
            "Train Epoch: 90 [240/700 (34%)]\tLoss: 187.422379\n",
            "Train Epoch: 90 [280/700 (40%)]\tLoss: 222.220901\n",
            "Train Epoch: 90 [320/700 (46%)]\tLoss: 108.091141\n",
            "Train Epoch: 90 [360/700 (51%)]\tLoss: 94.981476\n",
            "Train Epoch: 90 [400/700 (57%)]\tLoss: 190.181686\n",
            "Train Epoch: 90 [440/700 (63%)]\tLoss: 187.321259\n",
            "Train Epoch: 90 [480/700 (69%)]\tLoss: 272.919098\n",
            "Train Epoch: 90 [520/700 (74%)]\tLoss: 164.543640\n",
            "Train Epoch: 90 [560/700 (80%)]\tLoss: 186.486649\n",
            "Train Epoch: 90 [600/700 (86%)]\tLoss: 148.685455\n",
            "Train Epoch: 90 [640/700 (91%)]\tLoss: 155.511917\n",
            "Train Epoch: 90 [680/700 (97%)]\tLoss: 113.762680\n",
            "\n",
            "Test set: Average loss: 8.1782\n",
            "\n",
            "Train Epoch: 91 [0/700 (0%)]\tLoss: 113.738640\n",
            "Train Epoch: 91 [40/700 (6%)]\tLoss: 215.960587\n",
            "Train Epoch: 91 [80/700 (11%)]\tLoss: 197.666840\n",
            "Train Epoch: 91 [120/700 (17%)]\tLoss: 136.124496\n",
            "Train Epoch: 91 [160/700 (23%)]\tLoss: 75.658203\n",
            "Train Epoch: 91 [200/700 (29%)]\tLoss: 231.672501\n",
            "Train Epoch: 91 [240/700 (34%)]\tLoss: 136.194641\n",
            "Train Epoch: 91 [280/700 (40%)]\tLoss: 230.216034\n",
            "Train Epoch: 91 [320/700 (46%)]\tLoss: 174.217072\n",
            "Train Epoch: 91 [360/700 (51%)]\tLoss: 240.973740\n",
            "Train Epoch: 91 [400/700 (57%)]\tLoss: 153.404724\n",
            "Train Epoch: 91 [440/700 (63%)]\tLoss: 81.743523\n",
            "Train Epoch: 91 [480/700 (69%)]\tLoss: 112.956299\n",
            "Train Epoch: 91 [520/700 (74%)]\tLoss: 130.372452\n",
            "Train Epoch: 91 [560/700 (80%)]\tLoss: 165.382980\n",
            "Train Epoch: 91 [600/700 (86%)]\tLoss: 224.060150\n",
            "Train Epoch: 91 [640/700 (91%)]\tLoss: 250.011642\n",
            "Train Epoch: 91 [680/700 (97%)]\tLoss: 140.501556\n",
            "\n",
            "Test set: Average loss: 8.2674\n",
            "\n",
            "Train Epoch: 92 [0/700 (0%)]\tLoss: 98.456581\n",
            "Train Epoch: 92 [40/700 (6%)]\tLoss: 187.718948\n",
            "Train Epoch: 92 [80/700 (11%)]\tLoss: 142.097305\n",
            "Train Epoch: 92 [120/700 (17%)]\tLoss: 152.153641\n",
            "Train Epoch: 92 [160/700 (23%)]\tLoss: 122.900230\n",
            "Train Epoch: 92 [200/700 (29%)]\tLoss: 232.413055\n",
            "Train Epoch: 92 [240/700 (34%)]\tLoss: 97.728973\n",
            "Train Epoch: 92 [280/700 (40%)]\tLoss: 210.927872\n",
            "Train Epoch: 92 [320/700 (46%)]\tLoss: 229.267517\n",
            "Train Epoch: 92 [360/700 (51%)]\tLoss: 125.544922\n",
            "Train Epoch: 92 [400/700 (57%)]\tLoss: 176.906555\n",
            "Train Epoch: 92 [440/700 (63%)]\tLoss: 202.885712\n",
            "Train Epoch: 92 [480/700 (69%)]\tLoss: 186.936554\n",
            "Train Epoch: 92 [520/700 (74%)]\tLoss: 173.951111\n",
            "Train Epoch: 92 [560/700 (80%)]\tLoss: 183.859283\n",
            "Train Epoch: 92 [600/700 (86%)]\tLoss: 152.124527\n",
            "Train Epoch: 92 [640/700 (91%)]\tLoss: 169.508469\n",
            "Train Epoch: 92 [680/700 (97%)]\tLoss: 169.388535\n",
            "\n",
            "Test set: Average loss: 8.1074\n",
            "\n",
            "Train Epoch: 93 [0/700 (0%)]\tLoss: 110.187775\n",
            "Train Epoch: 93 [40/700 (6%)]\tLoss: 75.508171\n",
            "Train Epoch: 93 [80/700 (11%)]\tLoss: 113.985977\n",
            "Train Epoch: 93 [120/700 (17%)]\tLoss: 242.901627\n",
            "Train Epoch: 93 [160/700 (23%)]\tLoss: 167.048920\n",
            "Train Epoch: 93 [200/700 (29%)]\tLoss: 168.253647\n",
            "Train Epoch: 93 [240/700 (34%)]\tLoss: 176.316330\n",
            "Train Epoch: 93 [280/700 (40%)]\tLoss: 262.183807\n",
            "Train Epoch: 93 [320/700 (46%)]\tLoss: 174.936768\n",
            "Train Epoch: 93 [360/700 (51%)]\tLoss: 223.849030\n",
            "Train Epoch: 93 [400/700 (57%)]\tLoss: 175.376877\n",
            "Train Epoch: 93 [440/700 (63%)]\tLoss: 175.954102\n",
            "Train Epoch: 93 [480/700 (69%)]\tLoss: 196.109772\n",
            "Train Epoch: 93 [520/700 (74%)]\tLoss: 210.833801\n",
            "Train Epoch: 93 [560/700 (80%)]\tLoss: 170.551910\n",
            "Train Epoch: 93 [600/700 (86%)]\tLoss: 219.782623\n",
            "Train Epoch: 93 [640/700 (91%)]\tLoss: 87.215775\n",
            "Train Epoch: 93 [680/700 (97%)]\tLoss: 156.869537\n",
            "\n",
            "Test set: Average loss: 8.1465\n",
            "\n",
            "Train Epoch: 94 [0/700 (0%)]\tLoss: 145.692123\n",
            "Train Epoch: 94 [40/700 (6%)]\tLoss: 147.164886\n",
            "Train Epoch: 94 [80/700 (11%)]\tLoss: 204.811371\n",
            "Train Epoch: 94 [120/700 (17%)]\tLoss: 234.120865\n",
            "Train Epoch: 94 [160/700 (23%)]\tLoss: 170.448196\n",
            "Train Epoch: 94 [200/700 (29%)]\tLoss: 196.505692\n",
            "Train Epoch: 94 [240/700 (34%)]\tLoss: 121.145866\n",
            "Train Epoch: 94 [280/700 (40%)]\tLoss: 108.428299\n",
            "Train Epoch: 94 [320/700 (46%)]\tLoss: 184.143097\n",
            "Train Epoch: 94 [360/700 (51%)]\tLoss: 100.694420\n",
            "Train Epoch: 94 [400/700 (57%)]\tLoss: 143.175674\n",
            "Train Epoch: 94 [440/700 (63%)]\tLoss: 172.024307\n",
            "Train Epoch: 94 [480/700 (69%)]\tLoss: 150.095734\n",
            "Train Epoch: 94 [520/700 (74%)]\tLoss: 210.382324\n",
            "Train Epoch: 94 [560/700 (80%)]\tLoss: 140.976700\n",
            "Train Epoch: 94 [600/700 (86%)]\tLoss: 138.786316\n",
            "Train Epoch: 94 [640/700 (91%)]\tLoss: 134.184784\n",
            "Train Epoch: 94 [680/700 (97%)]\tLoss: 197.066315\n",
            "\n",
            "Test set: Average loss: 8.1614\n",
            "\n",
            "Train Epoch: 95 [0/700 (0%)]\tLoss: 199.175125\n",
            "Train Epoch: 95 [40/700 (6%)]\tLoss: 199.708954\n",
            "Train Epoch: 95 [80/700 (11%)]\tLoss: 171.602219\n",
            "Train Epoch: 95 [120/700 (17%)]\tLoss: 139.490280\n",
            "Train Epoch: 95 [160/700 (23%)]\tLoss: 242.332733\n",
            "Train Epoch: 95 [200/700 (29%)]\tLoss: 97.776649\n",
            "Train Epoch: 95 [240/700 (34%)]\tLoss: 168.292038\n",
            "Train Epoch: 95 [280/700 (40%)]\tLoss: 126.591400\n",
            "Train Epoch: 95 [320/700 (46%)]\tLoss: 328.520172\n",
            "Train Epoch: 95 [360/700 (51%)]\tLoss: 170.837204\n",
            "Train Epoch: 95 [400/700 (57%)]\tLoss: 175.184525\n",
            "Train Epoch: 95 [440/700 (63%)]\tLoss: 253.930084\n",
            "Train Epoch: 95 [480/700 (69%)]\tLoss: 153.013046\n",
            "Train Epoch: 95 [520/700 (74%)]\tLoss: 144.756744\n",
            "Train Epoch: 95 [560/700 (80%)]\tLoss: 190.830429\n",
            "Train Epoch: 95 [600/700 (86%)]\tLoss: 281.527344\n",
            "Train Epoch: 95 [640/700 (91%)]\tLoss: 103.681831\n",
            "Train Epoch: 95 [680/700 (97%)]\tLoss: 209.531784\n",
            "\n",
            "Test set: Average loss: 8.1400\n",
            "\n",
            "Train Epoch: 96 [0/700 (0%)]\tLoss: 210.530869\n",
            "Train Epoch: 96 [40/700 (6%)]\tLoss: 98.733917\n",
            "Train Epoch: 96 [80/700 (11%)]\tLoss: 205.333923\n",
            "Train Epoch: 96 [120/700 (17%)]\tLoss: 189.956863\n",
            "Train Epoch: 96 [160/700 (23%)]\tLoss: 186.171616\n",
            "Train Epoch: 96 [200/700 (29%)]\tLoss: 200.363007\n",
            "Train Epoch: 96 [240/700 (34%)]\tLoss: 142.669601\n",
            "Train Epoch: 96 [280/700 (40%)]\tLoss: 142.330948\n",
            "Train Epoch: 96 [320/700 (46%)]\tLoss: 146.936752\n",
            "Train Epoch: 96 [360/700 (51%)]\tLoss: 161.284683\n",
            "Train Epoch: 96 [400/700 (57%)]\tLoss: 122.040123\n",
            "Train Epoch: 96 [440/700 (63%)]\tLoss: 142.844894\n",
            "Train Epoch: 96 [480/700 (69%)]\tLoss: 127.981705\n",
            "Train Epoch: 96 [520/700 (74%)]\tLoss: 154.399170\n",
            "Train Epoch: 96 [560/700 (80%)]\tLoss: 147.938446\n",
            "Train Epoch: 96 [600/700 (86%)]\tLoss: 298.705627\n",
            "Train Epoch: 96 [640/700 (91%)]\tLoss: 122.854446\n",
            "Train Epoch: 96 [680/700 (97%)]\tLoss: 125.402946\n",
            "\n",
            "Test set: Average loss: 8.1767\n",
            "\n",
            "Train Epoch: 97 [0/700 (0%)]\tLoss: 136.346802\n",
            "Train Epoch: 97 [40/700 (6%)]\tLoss: 226.856522\n",
            "Train Epoch: 97 [80/700 (11%)]\tLoss: 139.768936\n",
            "Train Epoch: 97 [120/700 (17%)]\tLoss: 194.342117\n",
            "Train Epoch: 97 [160/700 (23%)]\tLoss: 217.614304\n",
            "Train Epoch: 97 [200/700 (29%)]\tLoss: 238.745209\n",
            "Train Epoch: 97 [240/700 (34%)]\tLoss: 112.770035\n",
            "Train Epoch: 97 [280/700 (40%)]\tLoss: 224.129440\n",
            "Train Epoch: 97 [320/700 (46%)]\tLoss: 130.381973\n",
            "Train Epoch: 97 [360/700 (51%)]\tLoss: 194.933563\n",
            "Train Epoch: 97 [400/700 (57%)]\tLoss: 135.193466\n",
            "Train Epoch: 97 [440/700 (63%)]\tLoss: 169.679749\n",
            "Train Epoch: 97 [480/700 (69%)]\tLoss: 158.670135\n",
            "Train Epoch: 97 [520/700 (74%)]\tLoss: 159.805161\n",
            "Train Epoch: 97 [560/700 (80%)]\tLoss: 152.008316\n",
            "Train Epoch: 97 [600/700 (86%)]\tLoss: 175.942200\n",
            "Train Epoch: 97 [640/700 (91%)]\tLoss: 115.584328\n",
            "Train Epoch: 97 [680/700 (97%)]\tLoss: 120.997993\n",
            "\n",
            "Test set: Average loss: 8.2609\n",
            "\n",
            "Train Epoch: 98 [0/700 (0%)]\tLoss: 172.121140\n",
            "Train Epoch: 98 [40/700 (6%)]\tLoss: 132.068329\n",
            "Train Epoch: 98 [80/700 (11%)]\tLoss: 220.085876\n",
            "Train Epoch: 98 [120/700 (17%)]\tLoss: 127.456367\n",
            "Train Epoch: 98 [160/700 (23%)]\tLoss: 191.841415\n",
            "Train Epoch: 98 [200/700 (29%)]\tLoss: 164.746689\n",
            "Train Epoch: 98 [240/700 (34%)]\tLoss: 252.885742\n",
            "Train Epoch: 98 [280/700 (40%)]\tLoss: 127.299950\n",
            "Train Epoch: 98 [320/700 (46%)]\tLoss: 173.820450\n",
            "Train Epoch: 98 [360/700 (51%)]\tLoss: 154.500427\n",
            "Train Epoch: 98 [400/700 (57%)]\tLoss: 157.455994\n",
            "Train Epoch: 98 [440/700 (63%)]\tLoss: 166.942047\n",
            "Train Epoch: 98 [480/700 (69%)]\tLoss: 210.438782\n",
            "Train Epoch: 98 [520/700 (74%)]\tLoss: 247.151993\n",
            "Train Epoch: 98 [560/700 (80%)]\tLoss: 161.298920\n",
            "Train Epoch: 98 [600/700 (86%)]\tLoss: 186.008850\n",
            "Train Epoch: 98 [640/700 (91%)]\tLoss: 100.584831\n",
            "Train Epoch: 98 [680/700 (97%)]\tLoss: 171.108109\n",
            "\n",
            "Test set: Average loss: 8.2510\n",
            "\n",
            "Train Epoch: 99 [0/700 (0%)]\tLoss: 161.590073\n",
            "Train Epoch: 99 [40/700 (6%)]\tLoss: 124.944290\n",
            "Train Epoch: 99 [80/700 (11%)]\tLoss: 135.408066\n",
            "Train Epoch: 99 [120/700 (17%)]\tLoss: 165.085785\n",
            "Train Epoch: 99 [160/700 (23%)]\tLoss: 90.445580\n",
            "Train Epoch: 99 [200/700 (29%)]\tLoss: 83.206024\n",
            "Train Epoch: 99 [240/700 (34%)]\tLoss: 186.812027\n",
            "Train Epoch: 99 [280/700 (40%)]\tLoss: 205.879166\n",
            "Train Epoch: 99 [320/700 (46%)]\tLoss: 269.923645\n",
            "Train Epoch: 99 [360/700 (51%)]\tLoss: 121.336029\n",
            "Train Epoch: 99 [400/700 (57%)]\tLoss: 142.935486\n",
            "Train Epoch: 99 [440/700 (63%)]\tLoss: 213.916550\n",
            "Train Epoch: 99 [480/700 (69%)]\tLoss: 100.021149\n",
            "Train Epoch: 99 [520/700 (74%)]\tLoss: 170.685867\n",
            "Train Epoch: 99 [560/700 (80%)]\tLoss: 132.983063\n",
            "Train Epoch: 99 [600/700 (86%)]\tLoss: 185.751251\n",
            "Train Epoch: 99 [640/700 (91%)]\tLoss: 180.936020\n",
            "Train Epoch: 99 [680/700 (97%)]\tLoss: 187.737503\n",
            "\n",
            "Test set: Average loss: 8.2302\n",
            "\n",
            "Train Epoch: 100 [0/700 (0%)]\tLoss: 110.951691\n",
            "Train Epoch: 100 [40/700 (6%)]\tLoss: 256.432129\n",
            "Train Epoch: 100 [80/700 (11%)]\tLoss: 179.994202\n",
            "Train Epoch: 100 [120/700 (17%)]\tLoss: 103.273964\n",
            "Train Epoch: 100 [160/700 (23%)]\tLoss: 189.243515\n",
            "Train Epoch: 100 [200/700 (29%)]\tLoss: 103.214508\n",
            "Train Epoch: 100 [240/700 (34%)]\tLoss: 222.111877\n",
            "Train Epoch: 100 [280/700 (40%)]\tLoss: 110.016205\n",
            "Train Epoch: 100 [320/700 (46%)]\tLoss: 197.977081\n",
            "Train Epoch: 100 [360/700 (51%)]\tLoss: 174.503036\n",
            "Train Epoch: 100 [400/700 (57%)]\tLoss: 162.555984\n",
            "Train Epoch: 100 [440/700 (63%)]\tLoss: 253.313034\n",
            "Train Epoch: 100 [480/700 (69%)]\tLoss: 115.344681\n",
            "Train Epoch: 100 [520/700 (74%)]\tLoss: 128.468826\n",
            "Train Epoch: 100 [560/700 (80%)]\tLoss: 186.153549\n",
            "Train Epoch: 100 [600/700 (86%)]\tLoss: 182.813065\n",
            "Train Epoch: 100 [640/700 (91%)]\tLoss: 134.784943\n",
            "Train Epoch: 100 [680/700 (97%)]\tLoss: 234.524872\n",
            "\n",
            "Test set: Average loss: 8.3079\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_nn = model(torch.tensor(X_test).float()) \n",
        "print(\"MSE neural network\", mean_squared_error(y_test, y_pred_nn.detach().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdFnGWt_zUi9",
        "outputId": "6dfca13d-6cb3-4181-d38d-56e72fd081f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE neural network 166.1575547703153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. What preprocessing techniques did you use? Why?\n",
        "    - *Answer*\n",
        "2. Describe the fine-tuning process and how you reached your model architecture.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0fbjTZnFXlpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: CNN (40%)\n",
        "For this task, you will be doing image classification:\n",
        "- First, adapt your best model from Task 1 to work on this task, and\n",
        "fit it on the new data. Then, evaluate its performance.\n",
        "- After that, build a CNN model for image classification.\n",
        "- Compare both models in terms of accuracy, number of parameters and speed of\n",
        "inference (the time the model takes to predict 50 samples).\n",
        "\n",
        "For the given data, you need to do proper data preprocessing and augmentation,\n",
        "data loaders.\n",
        "Then fine-tune your model architecture (number of layers, number of filters,\n",
        "activation function, learning rate, momentum, regularization).\n",
        "\n",
        "### Data\n",
        "You will be working with the data in `triple_mnist.zip` for predicting 3-digit\n",
        "numbers writen in the image. Each image contains 3 digits similar to the\n",
        "following example (whose label is `039`):\n",
        "\n",
        "![example](https://github.com/shaohua0116/MultiDigitMNIST/blob/master/asset/examples/039/0_039.png?raw=true)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0Hgc5qoSXlpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Ws0TxSl6YM",
        "outputId": "d72f5a2c-08f3-465f-fd24-68137d5a9be9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/ML/triple_mnist.zip\" "
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "i2--rgTPXlpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "metadata": {
        "id": "36b3mJWXksYF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyImageFolder(torchvision.datasets.ImageFolder):\n",
        "   def __init__(self, root,transform=None, target_transform=None):\n",
        "       super().__init__(root,transform,target_transform)\n",
        "\n",
        "   def find_classes(self, directory: str):\n",
        "       idx_list, idx_dict = super().find_classes(directory)\n",
        "       idx_dict = {i: int(i) for i in idx_dict}\n",
        "       return idx_list, idx_dict\n"
      ],
      "metadata": {
        "id": "bxUwEF3yyvsX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageData(Dataset):\n",
        "    def __init__(self, images, targets):\n",
        "        super().__init__()\n",
        "        \n",
        "        for idx, image in enumerate(images):\n",
        "          gray_img = np.array(image)\n",
        "          images[idx] = rgb2gray(gray_img)\n",
        "        \n",
        "        self.images = images\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "s7ppT1w5aAY4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyImageFolder(root='./triple_mnist/train')\n",
        "\n",
        "images =[]\n",
        "classes =[]\n",
        "for image, label in train_dataset:\n",
        "  images.append(image)\n",
        "  classes.append(label)\n",
        "\n",
        "train_dataset = CustomImageData(images, classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "Zdj6vbXQsylh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageRestorator:\n",
        "    def __init__(self, dataset: CustomImageData):\n",
        "      self.digits=[]\n",
        "      for i in range(10):\n",
        "        self.digits.append(get_img_cut(dataset, i))\n",
        "      \n",
        "\n",
        "\n",
        "    def recollect_digits(self, dataset: CustomImageData):\n",
        "      self.digits=[]\n",
        "      for i in range(10):\n",
        "        self.digits.append(get_img_cut(dataset, i))\n",
        "\n",
        "    def get_img_cut(dataset: CustomImageData, digit):\n",
        "      length = len(dataset.targets)\n",
        "      cuted_part =[]\n",
        "      for i in range(100):\n",
        "        idx = random.randint(0, length-1)\n",
        "        img, target = dataset[idx]\n",
        "        while (target % 10 != digit):\n",
        "          idx+=100\n",
        "          if idx >= length:\n",
        "            idx = idx % length\n",
        "          img, target = dataset[idx]\n",
        "        cuted_part.append(img[:, 56:84])\n",
        "      return cuted_part\n",
        "\n",
        "    def class_restoration(self, res_cl: int):\n",
        "      fir_d = res_cl // 100 #first digit of the class\n",
        "      sec_d = (res_cl // 10) % 10\n",
        "      thir_d = res_cl % 10\n",
        "      fir_part = self.digits[fir_d]\n",
        "      sec_part = self.digits[sec_d]\n",
        "      thir_part = self.digits[thir_d]\n",
        "      new_class_img =[]\n",
        "      for i in range(len(fir_part)):\n",
        "        image = np.concatenate((fir_part[i], sec_part[i], thir_part[i]), axis=1)\n",
        "        new_class_img.append(image)\n",
        "      return new_class_img\n",
        "\n",
        "    def dataset_restoration(dataset: CustomImageData):\n",
        "        classes = np.unique(dataset.targets)\n",
        "        possible_classes = [i for i in range(1000)]\n",
        "        lost_classes = list(set(possible_classes) - set(classes))\n",
        "        images = []\n",
        "        classes =[]\n",
        "        for lost_class in lost_classes:\n",
        "          images+= class_restoration(dataset, lost_class)\n",
        "          classes+= [lost_class for i in range(100)]\n",
        "        return CustomImageData(images, classes)"
      ],
      "metadata": {
        "id": "n7PykYya6gyN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_restoration(dataset: CustomImageData):\n",
        "    classes = np.unique(dataset.targets)\n",
        "    possible_classes = [i for i in range(1000)]\n",
        "    lost_classes = list(set(possible_classes) - set(classes))\n",
        "    images = []\n",
        "    classes =[]\n",
        "    for lost_class in lost_classes:\n",
        "      images+= class_restoration(dataset, lost_class)\n",
        "      classes+= [lost_class for i in range(100)]\n",
        "    return CustomImageData(images, classes)\n",
        "\n",
        "def get_img_cut(dataset: CustomImageData, digit):\n",
        "    length = len(dataset.targets)\n",
        "    cuted_part =[]\n",
        "    for i in range(100):\n",
        "      idx = random.randint(0, length-1)\n",
        "      img, target = dataset[idx]\n",
        "      while (target % 10 != digit):\n",
        "        idx+=100\n",
        "        if idx >= length:\n",
        "          idx = idx % length\n",
        "        img, target = dataset[idx]\n",
        "      cuted_part.append(img[:, 56:84])\n",
        "    return cuted_part\n",
        "\n",
        "\n",
        "def class_restoration(dataset: CustomImageData, res_cl: int):\n",
        "    fir_d = res_cl // 100 #first digit of the class\n",
        "    sec_d = (res_cl // 10) % 10\n",
        "    thir_d = res_cl % 10\n",
        "    fir_part = get_img_cut(dataset, fir_d)\n",
        "    sec_part = get_img_cut(dataset, sec_d)\n",
        "    thir_part = get_img_cut(dataset, thir_d)\n",
        "    new_class_img =[]\n",
        "    for i in range(len(fir_part)):\n",
        "      image = np.concatenate((fir_part[i], sec_part[i], thir_part[i]), axis=1)\n",
        "      new_class_img.append(image)\n",
        "    return new_class_img\n",
        "\n",
        "#res_data = dataset_restoration(train_dataset)\n",
        "imgres = ImageRestorator(train_dataset)\n",
        "\n",
        "resu = imgres.class_restoration(176)\n",
        "print(resu[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "MYchiHgrT1BL",
        "outputId": "5006d871-3ff2-4be5-8de0-b1c8f605e22e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a0fcffb7ff5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mimgres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageRestorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mresu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_restoration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m176\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4aef7621902c>\u001b[0m in \u001b[0;36mclass_restoration\u001b[0;34m(self, res_cl)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mthir_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthir_d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mnew_class_img\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfir_part\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfir_part\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msec_part\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthir_part\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mnew_class_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(resu[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "RFYL1T386Hb_",
        "outputId": "a23e474f-f0d7-409f-a78a-a8d3cd3bf559"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZBc53nf++9zlt57enr2GWCwgyApbpK4SJRiy6JFUUu8JLZjWU7ZKeXqprLZcaoi+d6kstS9VXZVyrb+uFGVykoi3zimZNmKVIxKEUVrX7iTIgmC2JcZYPa1u6e7T5/z5I9uADPgAGgQmPU8n6qpmV4G/fbM/HDOec97nkdUFWPM9uds9ACMMevDwm5MTFjYjYkJC7sxMWFhNyYmLOzGxMRNhV1EHhORN0TkuIh8+lYNyhhz68lbPc8uIi5wFPgAMAI8C3xMVQ/fuuEZY24V7ya+90HguKqeBBCRx4FfBK4a9oQkNUX2Jl7SGHMtVcrUtSarPXYzYd8BnFt2ewR46FrfkCLLQ/LITbykMeZantanrvrYzYS9LSLySeCTACkya/1yxpiruJkJulFgeNntna37VlDVz6nq/ap6v0/yJl7OGHMzbibszwIHRWSviCSAXwe+dmuGZYy51d7ybryqNkTknwL/C3CB/6yqr92ykRljbqmbOmZX1a8DX79FYzHGrCFbQWdMTFjYjYmJNT/1Zsy2I4IkEojrIp4HydZZpiiEMESDBlGlApusCpSF3Zh2OS7iCE6xSO2eXVS7fRZ3OiweaIADiWkXryR0nIkoPnmCcHJyo0e8goXdmDaJI80teWeemUNJyjuV/N1T/Le3PY4vIV+ceYiXZndy5sUdFJ/JgYXdmC3k4i675yE7Bgj6O1jqT7C4V9HBKge7puh0ajgoaTcg6TZQd3Ptvl9kYTfmGpxkEqenG82mufDzfcw/VKVQmOe3977E2zOn2eHOs9ODxSgi71bJ+TXUU5BVr0XZUBZ2Y67FddFUgiiTpNoL9+weZU9umo/mX+aOhEPzhJZDhZBQHRqRg0SbL+hgYTfmmiSdotHXQb2YoDrQ4JGeIwz5s/S4AZBkMaozGQpvBAN8/qcPk/lpmv6zEczOb/TQ38TCbsw1SDrNUn+KpW6H4tAMv5J/jbzj4UvzdNt8pBwLevn+wm0Uv5ui97+/DGFIWKtt8MjfzMJuzNWIoKkE1U6HWpfQk66ScVx8cS895Vyjg+8u3M7z07tIzitRubyBA742C7sxqxA/gbgO9R2dTD0Ykh9c5AP9R3C5fDweqvK5sZ/lpa/fQXpK6TsyS7SBY74eC7sxq3EEfJ8g79E5tMB7hk5xZ3oU54oV5sdme+l7MSA1XkEuTG/QYNtjYTfmCuIncA7sod6XZe6Ax9t6xrknd44hbxZXhMWozqv1PBNhnqnJPMXZOu5sGd2Ex+nLWdiNuYKTTjF7b5GZO4Rwf4WP9/+Yh5LTpMQFXCZD4UszD/L6XD/pE0n8sxcIp6bRoLHRQ78mC7sxV3JdGimhkVdymRrdTpmCk7r0cE1dRiqdjM11kCiDVqtovb7pLny5koXdmFVEHqivJLwQX0KaxZiaTje6eeW1XXQc9Si+EaBL1U0fdGjjenYR+c8iMiEiry67r0tEnhSRY63PxbUdpjHryBHUBfUiPDfEZWWQJxod5E949D1XIXNiprlV3wLaKV7xX4HHrrjv08BTqnoQeKp125gtTfwETj6PdOSpF4RksUp/poQvzRNqpajGhXCJ0XoRdwncpQCpB2i0+bfq0MZuvKp+T0T2XHH3LwLva339BeA7wKdu4biMWXdORw4Geqn1ZSnvafC39x3hUGaMghMSaMixhs/h2g6endlNejrCnVpAS2XQzXx2/bK3WpaqX1UvtL4eA/pv0XiM2TCSSNAopKgXPLyOOgfTE+zwZ0m0rmCbCzOM1LuYrabxahFaraH1YINH3b6bnqBTVRWRq+7HWEcYs1VU79jB2UcTNLobPHrgDd6RPkWnUyMpDgEh31m8g/9x4h6qozkOTFbRUhltNLbE5By89bCPi8igql4QkUFg4mpPVNXPAZ8D6JCurfFTMfEjwuxtCX7jQ9/jwewJbven2OkluRiR+ajOj6f2wksdFCcUb3yecBOvg1/NW92N/xrwW62vfwv46q0ZjjHrTzwPJ5kkSgg9Xoled5GMcGlpbCUKmItgupwhOQvJeUVqW2f3/aLrbtlF5C9oTsb1iMgI8G+BPwC+JCKfAM4Av7aWgzRmrYifwO3pgnSKeh4KbpmsNPBbx+nzUZ3D9Tyngx7KpwoceL6Eu1Almtt816tfTzuz8R+7ykPWe9lsfY5AOkWUTxMllZQT4Et06eq2cqSMNoqcqfWQmHXwzkyg5Qq6tLTBA79xtoLOxJokEjR68tS7UgQdSqdTIS+64pr1QF1qkYfTAK3Xmx9tnlt3UikklURSKaL+LqK0f/m1ayHOuTHCqfW5Ws7CbmJNUikqOzKUBxykt8oOb4GCk7gU9gChHCWpRAmcALRcIarV2puBF0HyeSh2EHTnmL4nQ7Xr8vXwfhkGvxuBhd2YdeAIkS+ECcFxQ3yJ8CXRfGhZoYpIBRRU9dpBF7ncKcb3oatA0Juj1pNgqVeodV1egNPICNWBLJmJAbRaI5xfaHaVWSMWdmNW4SC44lxaF+9cfSnJJZJMIq6L09dDdV8vQd5l+m0elYM1ktkl7hk6z1D68sTefJDmmTt3UZkapuOIz87HT9K4MLZm78nCbkzLxVLvF4Pe/Brci8WmrlUhWprdYiSRICzmmd+foNoluA/O8u9vf5IBb567E7P0uOlL31LTgDMDymSY5R93/wb6jTxY2I1ZGyJC5EHkg+tGuCgRChpdCnx4cTnKKht3J5PB6cg3T93t6qLalaDS5zB/mxJ2NHigZ4IBb55ut0xKVi5rcXBwaOBLA9eNUN9F/ATaCNZkVZ6F3cSb6xKkhUYW0onLC2UuBv7iEXakq2/Wnf5eKof6qHa5jP2tiH0HL7A3O8f7i0fo9RYY9uYYckNchKS8OW6uKL6E+G5ImE3hdeSak4DV6i1/qxZ2E28iqAeRr3juyqvXIpTwmvvuoKkEtYJLtejQOTTHLw2+zO7EJA+nJsk7CZpFLy6fxouuqD/ros0PR9GEgyQSaHVtatlZ2E28JXyWeoT6jhp7CjMkb7BzU6OYYXG3Q7VHeaBnnAfSJyk4tVa9umtzRSg4QlLqDHfMMrK3m7y7g9SpKaJTt37dvYXdxJomfWq9EcNDM9yRH7t0OWtbRKgXEpR3hXg9S7yneJy3JyPAv+63QvOYveCkKAAH85O8vvsgYSJB72IHnJZbftxuYTfxFka4VWFuKcVskCFsJ2COi5PNIAmfWtHF6arS21mi2y2tqCsfETEZ1piJPFIS0u86pJYdty/fpY9UkBAkZM0umbWwm1iTSpX8SSg1OvmJv4dK7/W/x81lCW/fTb0rxdS9wv9x9w85lLrAvYkx4PKptao2eKJ0iG9M3sXOzBz/sOd73JFY+W9VtUGkylQ9R3IO0jMRTrnGWiytsbCbWNMgIDWnNNIOc4vp607IAZDwqXWnWOr1CPvrfDD3Knv9iFSr2eNFkSonqn0cnexlvpBirisN1N70nKpGlBsJ3KriVUII1qYghoXdmBsk2Qzz+31Ku5ShgVkyTgN32Yx7KaoxFsJMlOKFmWFqIznO1XwmdudZHvaqNnglyHAu6ObIVB/FsZDUhRIsrk1RDAu7MTcoKmSZuyfggTtP8reKx+lywBf30vH6XBTxQm03Z2o9nDzbR9frQqmU4vx9RWDq0r9TiUK+V7qd52Z3Uz5dYOexKaKTZ4nCtVkfb2E38RaGuPUIb0kIA5dK5FHTAF9cPFx8lJQEpN2AMAlusZN6Rwo3FzCQWqDLK+Eil4LuIFTU5UythzPVLggEdUDd5jn15RN4VYVTlR5OznaRmHOQShUN1q4GvYXdxJqWyuRenSSbS1MZ6uT5+3cTcpbdXp0eN0uX6/Jw+iT7ExN87b67Of939lPvhP6uMZJOc6mr0zpdd3FN/Uu1nXzh9YeozyfBV+beVaO7u8Sh5PkVr30+zPDtp+9i6PtKeqyy5tVv2ilLNQz8Gc1y0Qp8TlU/IyJdwBeBPcBp4NdUdXbthmrMrRdVq3D8FIiQeehdnKr10ust0Os2a6gWnDQ5P2LIK/Pw8Cn+5t47cDINBrML+BK2VsDJiotnztR60FNZcjNC+W1V3nXgFHsy0wx788DlSbzpMEfxsJD98k+aY1nj99rOlr0B/EtVfUFE8sDzIvIk8Ns0u8L8gYh8mmZXGGsUYbYmVRKLyrfHDnKhs0Ci51kKzlxrTbuPi3BbdpzXhgcAmK1leCEYphQmCdSl062w25ul123Oogcdzf5whc4Kh3Lj7EzMkGp1lgk0pKYN5sL+5nn1ddJODboLwIXW14si8jqwA+sKY7aZ/Jkqo98b4Ds9fUQPCwP93yLvBPS7zQm4jxVe5Odyh3lhaQ9//MojBONp3sgN8u3iQXKpGo8OHeE92aNECF3Dc9QCnw/tOsxvdj5DUqDT8YiImI/qjIc+I/VunHXs8nxDx+ytNlBvB56mza4w1iTCbBVuqUZ6Mg3qcLZUZLSnQC+LdDnNCbt+N0m/C4vRGBoJ7pJDKFB2U9TrHpM9eebSzb/xnkyFeuSyKznNoJvAXbYMt6owF6WZD9PIOnaOajvsIpID/gr4XVVdkGWDv1ZXGGsSYbYKZ2KWrlcTBIUEF5yd/NO9v8mOnTN85tDj3LVs5duwt8Av3/4yx3b0Ug6SzNdSpLwGnV4FgF2JKT460Kw+e1/q7IqgA7xcH+DxiQc5PNlP5/z67ce3FXYR8WkG/c9V9a9bd7fdFcaYraAxNo6MT5JKp9g1tZelgQzjD/Zzel8PdyUuF4Xc7SX4N70/IeiJ+H61h69OvwOAHr8EwJ7EFLf7Z8hcOvd+OeyhKi9WdvPM8T04E0n6Z279detX005/dgE+D7yuqn+07CHrCmO2F1WIQrQe4CxWSczVScwLz5f38GLNYTy8vPrNF5eM49PtltiZnmUwNU+XV6LTqZCVOklx8MUlIqISBUyGNV6uww+qWZ6f3YUzmSA5I7hL69dZRvQ6a3BF5L3A94FXuHx24P+iedz+JWAXra4wqjpzrX+rQ7r0IbHeEmaTE8EtdCCpFOV37uLMLyu9g/P8+u7n+WTn4RU15eejOpOhQ4iQlwYpgZQ4JMXDFWE8rDEWJjlc28Gfnn4v4zMdeEcz9D3XILEYkDh64ZYWmXxan2JBZ1Zd4N/ObPwPuHqpPUuu2X5UiUplKC+RnOwjeT7HZNjJG739hJ2vrbhaveAkKLT2j51lj0REhKosRi4zYY5TtV5Gz3eRuOCTP61kT84h5SWihcV1e1u2gs6YVWgYIi54U4t0Hc6yNObzTfdtNCKXnelZHs2/wj6vwqIKM2EKRyIG3CW6HI+TDfh+5SATQQdPT+/hzHQX1dkUhVd90pMRufM1ZL6EVqvNls/rxMJuzGpU0UaD8Nx5OucXKKZSZKZ28YOxu6n3NeAh+EjHS5wOejhaHcSXkIcyJ3D8BX5UOcRnj/wM5dk0uTcS9BwLSSw0SB0/RzQzB0FAox6ARuva293Cbsw1aCMgml9AqjVS0wGpySTqeDw/u4ucW2W0VuRkqQdPQqqRz2hqnKcX9lGeyOLPuaSmlPREDbdUQ2fniRbXb7f9ShZ2Y65FFQ1DqNVIHR1ncK5AmEkw/epuHs/twQnArSsInE0dIEwIiZKyZ6KBt1TFn6ogM/NoEBAtrd9pttVY2I25ntYufePcCJwbwQE62vzWdVz6fl0W9i3OyWSQfA7xfTSbRtMri5yF2QSNtEvkOzSyDpF3g7WSb5BXjUiN13ArdaRSQ0oVtBESLSygtbWph27aY2Hf4pzebmr7egmyHgu7PWrFy4+pC9XBBtm+Mp2ZJd43cIxdybVtD/yDuQP84Jk7SU1kyYwr+ZEGXinAPyaE47bIciNZ2Lc4TSaod3jUcw7VHqj1Xt5xVFcpDs3z9r5R9mam+HjhOfb6uTUdzy5/mmeHdlN1Mkjk4i25JJKCP5bFWcygYWhb+A1iYd/iws4MizubW3T33nke2XH60mO+E3IgPcG+5ARdboku9/pdSm7WQX+ajx96llPDPYxWCowv5pmpJOHUAMmZQTpPhOSfep1wYWHNx2JWsrBvZSIEHQnKO5Sgt8E/vu0n/PPikRVPaXYKbR6nu5Je7V+5pfb7OT7V/RpRd3NldajKeFjnz+58kFcWhnj5hwfpeCYHFvZ1F+uwSzKJJBI4uSzhQDdRygNHUKfZesdpREgjwp1eJDw3uq6rndrmSHMxc6sbaFJWth6aj5ZYjEJqCtNRs6rKrZKVgIIT4LcKM+ScFEBr7XjrdQQ6tcHe5AS1nMcL2f3gXPf6K7MGYht28TzcwX7C7jzTt+eZ+nCN4b4Z8okanYkKlUaCswtFytUEzo93MPxfS4RTazu5dasFGvJcLcfT5QOcr3Xy3OQwi0vJ639jm/o6SjzYc4Yef5H3Z1/nnVf5p3NOkvdnTvNA6ix/1Xcf+LH9s9tQ8f2pi4NmUgSFFOVBh1++4yV+tfgMQ26NQTfDQlTl+9UeztR7+eMLjyHJWxeSWyrSZhlQFSJ1CPVy6ZOaBowG/Rwp9zNa7mR8tIizeOu27Gd6kmT9OoPpLHenRgi0suyQ4fLW2xeXnV5zYjCbrqGubdk3QmzDLr7H0nAHs7c1u3DuT03Q69RItaqK+OKwy5sl69TQVAg30t1zvaiSOr9Iz8td1Ds8/tP8o/ynwZ+9/HAEOpMkMS+4S0L3lOLewkVcQS7JyWN7OZZQvtX9NtxCQCZb5YGBcwym5vmZ3BF+Ll1dcUmo2TjxDXvCZ36vz/zb6wwNzXB36hz9rVphEUpSfG7zQwJK+Lk6bNKtUXTsNIUzo4jr0pdMIollx+xRhEYRhGHzgougwfXqF9wIcR1wXRBBPK/Zwnj/AD/8wD3UBgNG7+nk4Z1PWtg3idiGHXGIfMFNhmT8gJQ0Lv1ROjQD74vbrAnu6ObcsgMa1Ne0i8iN8ro7gQx4iree1RTNdcU37Fdx8ZjzYuDNjWl0p8m/c4pP7nmO+zMn33R2wGycdjrCpIDv0Wxl4QFfVtV/KyJ7gceBbuB54O+r6ubZxLwFzhUFeZq3N+fu+2YV5Dw+uPN1fq/rZOse24XfLNr5S64B71fVe4H7gMdE5F3AHwJ/rKoHgFngE2s3zDXgCGESMtkqxWQFX6I3bckjlGjNm/JsAyK4nQW8wQGqnS4ZZ0v/n79tXTfs2lRq3fRbHwq8H/hy6/4vAL+0JiNcK45LkIfbuie5LTdBVpoLZi4G3nbh2yeuC0P9LN29k/IOhy6vdP1vMuuurX1UEXFF5CWateGfBE4Ac6p6cUnZCM2WUKt97ydF5DkReS5gc10AEflKd7JMl1de9QcRaEhVG0SRrGv5oC3FcZFEgkYhzVK3Rz2vpJyV5ZFDjZiPlpgKy9QCz36WG6StCTpVDYH7RKQT+Apwe7svsFk7wojnEhRDPlJ8mT53kU7ncn9tgFJU44V6ntGgSDCXgtB256/kdhZgsI9GZ4Zzj2ZJvWOGe7sneSB1Bri8Dv90o8K/OP0rHB4dIP1CBhYnN27QMXZDs/GqOici3wbeDXSKiNfauu8ERtdigGvGdfEKdT6YmW917Vi5Qq6sES8s7eFIaRB/zoXNuC5+g0k+T2VXgaVej653j/H1u/6clHgkr7jg5nSjwOFn9tL/TER2tIwu2m7+RminI0xva4uOiKSBDwCvA98GfqX1tC3TEUaSSdyebqLuDhLJYMVVYctFwGyQZbqWxalzSxejbGkiONksbrFI2FegNORTHnTozyySk+SKU20jjRI/rEZ8r3Q7yRkhNd3AXag1a7qZddfOln0Q+IKIuDT/c/iSqj4hIoeBx0Xk/wFepNkiatNzB/qo3DFAud9jqHMEWDkZF6E4CIuRy9FSH8emevAXpbkKzeAkk7B3mHpPhpnbk8w9XKO7q8Rjva+teF6oEX+xcC9/duwhSuM5dr8UkPzpabRWR+s2W78R2ukI81OabZqvvP8k8OBaDGotaSbFUo9LtVvYlVha9TkRSqAO8/U01aUE+Ro2qXSR6xIWUlS7fao9wt6hKQ4VJtifGF9x8UuEcmqpl/JonvS4S2psfstdNbjdxG4FXdCXY+YuIeitc3fHeWDlarlAQyIiRsNOjp/pJ3MsQX6k2ezPgNNZYOTdGUp31untn+GxgdcY9mcY9haA7KXnRUS8OLWDrpccMlMN3JkFbNZjY8Uu7OWBJDveeZ53dJ3j5/KH37Q8NiCkEoUcqw2QO5Jg4MdL+BOLRNXNddpwo2hnHv9npvnufZ8nJUJGXHxx8Vg5KReqMjbSxZ3fOEs0M0vDfn4bLnZhVxfyiRpFv0JKAq5czhloxKIK840MXgW8hSpSrTWvF40xJ5/HKXSwNJhjqOM8u7zVC1fOR0u8Wk8y2ujFm/bQhUWiSmWdR2tWE7uwX83FpbFnGj6v1Hby4twwmYkIGRknqsZ8Btlxqb7ndkbf5xH0BXyq76dXfeoXF/fzH7/6i3ScgD2vL214FxRzWezDvnwmPlRlLkpzqtbLWDlPeiEknL5my/lYEEdY2OVx4KEz3NExxgPpU0Bi1ee+VNrF8DfreH/zPIAtOt5E4hF2x8XtyEEySbXTYV9uittSF+h06jitY01bC/9mbrFI8Lbd1IsJ5g8pH+wcYX9qgi6nzvKw1zTgK6U+frBwG988egcHFmr209yEYhF2J+HDUD+NzjSVHcpjhVe4PzlDZtkCEAfB3aQFKjaK7h7kxK+mKO6b4e/uPMo/6v4ReXHIOSsn42bCGv/hlY+Q/E4H/eMh7ugFm3nfhGIRdlyXKO3TyHiESaXLLVF0Vq+hHqhHJUxQb7hkonhuny6W2A46U3j9FR4aOMs7s6fod1eukKtpwGJU51yYZGkqQ89Ig+RUHbWZ900pFmEX36PemaLS7xPmIlz00kq5i5oLaUKeq+zlG2fuoDyap2cxfpNLTirF0iN3M3W3T2Uo5O/e9iKPdBxmtzeLR2rFc79cGuAPX3+UxaksPU+75N6YQipVQpt935RiEXZcl3rBo1Z0cLI1fIkA91LgLx6vhyiHFwepnO4gM+7glmuxK10hqSST9/jc8eGj7M9N8YmuH7LfS+NK5k3P/dbsnbjfLDJ8vkHu8AThsZO20nATi0XYJZWiNOCyuDeir2eBlIQ0a3A0RURUtcFiFDJdzZKYdUjMK1KLz5Gn+AmcbBopdhIUlP25KfanJsiLrlgGW4qq/Kia53TQyzOjuyhMRySnA2SpZkHf5GIR9qjYQeVnS/zre7/BvsQEQ+7lKrLQXCI7FYaMhRmOX+hl6NWQ5EwAc/HpR+Z0FmgcGKLWk8TZV+K3iz8i70R0uSsv/T0eCP/kmY+TeDVD/mxE8dkxdH6BqGy77ptdLMKuSZeD/ZP8/fxY656V54hDlMXIZzFKE5U9UpN1vNkliFFrYUklqXcmqBZdujvKHPCTq9Z7n44yMJKm+3CD9FiVaGzCVshtEbEIO0CkKyfj4PKW/XxD+friPZyo9JK64ONPzCKlSqzWw4d9BSbv86n2Rnyw+/ybrvE/2yhxLCjwzYW7yVwQsqdLOPNlwiA+hzpbXWzCftFq166faxT4+vm3MTZdoHhW0XPnCas1iOKzRLban0EemOeB/gs82vnqiuN0gGNBga/OvoOfjO+h43SIvnqURqSx+hltdds77CIgDjgOjlx98ijAZSnwCGsuTkBzHXwc/ohFcPN5SKeoFl168yX2ZabocleWjQo1YqzRzdGFPmbmswxXws3Zvtpc07YOu5NMItkM9ZxPxmn+cS4/1XZ5gs6jUk0iSy5uXZudUWPAzeepvPcQi8Mes/dEfHrnszyYOsWQ12DltenK/5y6h9Pf301+ElKjM7E7JbkdtN3upFVO+kUReaJ1e6+IPC0ix0XkiyKy+pURG0gSCSSdJky6JNzLW2oHWXFMGqpDI3CRmuCE8Qg6AMkki8Mec7crPXtmeF/mGPclk/S52RVPi4g4Od9N59GIzmN1nLnFDRqwuRk30tvod2gWmrxoc3eEEUH3DDH7nmGm7vbZnbl89VrUWkF30XSYI5xOkh53SCyEsbl2XTyXeoeg3XUGcov4V1y+MhWW+atSB5+dO8j42S6yYwHJiSV0afVyXmZza2s3XkR2Ah8B/l/g90REaHaE+Y3WU74A/Dvgs2swxrdEXJeZ+4rMfaTMzu45Hsk3CyKuNkF3utpD/rhL8VhA6tw8YUx240n4VHuVu3af552dZ8k6K2fgDwdZ/uDoY0yNddD9nEvyp6fQxUVCK9G1JbV7zP4nwL8C8q3b3dxARxjgkwAp3rzkck20upTU88Le3mkO5Kfodsssf7sXV80FGjEdZPFLSmI+QKr1bb9lFz+Bk04R5bM0ciGD6Xl6/EX8K063VSOf+VIKd94jUVJ0cZGoGr/rBbaLdrq4fhSYUNXnReR9N/oC690RxkmlcAb6iHIZSsPK/znwEnsSUwy5dcC7NEFXiQL+snSAH8wd4AdHD7D3dB3/7BS6uLi9l32KED50J6MPZ6h2Kw/d+wa/3PUCA+4CGcfaK29n7WzZ3wP8goh8GEgBHcBn2KQdYSSdptFXoF5MEg7U+WjuDfrdNM6yvQoHoaoR35q+g2eP7iVzPEHyzAUa50Y2cOTrRBzmbkvT9+gIdxTG+c3uH/GulMuVHXHM9tNOF9ffV9WdqroH+HXgb1T142zWjjAJn3pXimq3h58OSKxSkCJCCYFS0Dzd5tRBGjE4r97SSAl7cjMcyIxTcK6+SrCqPkElgb/o4FW396FNHNzMefZPsRk7whTyzN7mUxlQ9vVOX/rf7MrLWWsK46UcyQmX5KxCjJZ91gvw93qe5s7ELF3O1c+YTjY6SIz6dJxQUuM11Jpbbmk32tjxO8B3Wl9vzo4wvke9A4JiSE+qjLtqH7eIEKFa9/GWwKtqPNo7iSC+R5SA/f4sO69SDjrUiAYh82EGvywkFxq4lTq6zScut7tts4LuUgOzPOMAAAq2SURBVCmlngxLwwEDu2a4Oz+KLytbMQcaMh/VOdcosDSSZ9crAcnpKlrZ3ueO3WKRxp27qRYTVHfVSV6l3F4pqvInM/fxxMhdjI8UGT7SIHtqAWd6gUYc/kPcxrZH2EVw0ikkm2WpJ8Hwngl+acfL3J85iS9uq5hkM/S1qMF46HOi3k/+tEP62y+hYUQYbO9mg1IsMH5/hsoO5bY958nK6tM1i1GDP3vtIYpfz7B7pkH2hbOEk1PNoG/nsxQxsE3C7iDpNFrIEWQdBlMV+v15OqTGlXOQM1HEkfoODleG8CpKVItJhRVVRIEIIoTwitVygYaUohrjoU+wkCA93SAxW0eXluyil21iW4RdfI9g3wDz+9LMHYK/132Mh1NnyDsCJJqTcq3jzacqt/Enr72fpakMu0ca8Qg6wFKV/EiIW3MY2d9JcMX7PttY4juVAxyuDJE95ZN97RxaqRKVt/fhTZxsj7C7LrVigsqgUO8JOJAcY6e3slT0xVn4E9U+6qdzZCcdErPljRjuhtAgIDkToOIzW/G58uh7LkrwSmUnR+b7SU0rjXPn43GZb4xsi7ATRTgNxamBNBwinEsTchd7uIXa/Hyh2kFqyiE9obil+FSP1WoNf6qCU0+SeznLhzo+STpxeY37fClNMJHGKznsOBts+yXDcbQ9wg64tRC/4uFUhVAdXHEIl/3B1rRBVSNOL3TTeTwkc76KM70Qm7BHlQpy7BSu6zJ8Mo8+kQP38nxGT6OEBHMQRkRz80RxObyJkW0RdlXFqUe4VXBqwmQjz2w4hi8OSfEJNOR8KExHOWbKGXpLEW6phsaooCSqaK3Zgy2qVGBsfKNHZNbZ9gh70MA/M0nXYh633skf7XuEbw7dyWO9r/Eb+ZMcDlw+8dJvsXSyg8JRIX1iHOYW0VJ8jtmN2RZhJwppjIzCCHSGh5g72M2LM3tJ3dvgV3PHOVkfRH/Syf7vlnCnS0SnR9Btfl7dmCttj7AvI5UqmQuK0/D4cXof/0w+zNHZXrIXFHd+CalUbfLJxNK2C3t0YZz+b4Zo0ifKZxjv2EtnPcI/e4Zodo4otMqoJp62X9irVaJl16VfnG+2eJu4u5GCk8aYLczCbkxMWNiNiQkLuzEx0W7d+NPAIhACDVW9X0S6gC8Ce4DTwK+p6uzaDNMYc7NuZMv+c6p6n6re37r9aeApVT0IPNW6bYzZpG5mN/4XaXaCofX5l25+OMaYtdJu2BX4pog83+rwAtCvqhdaX48B/at9o4h8UkSeE5HnAmJ04Ykxm0y7i2req6qjItIHPCkiR5Y/qKoqsnoD9PXuCGOMWV1bW3ZVHW19ngC+QrOE9LiIDAK0Pk+s1SCNMTfvumEXkayI5C9+DTwKvAp8jWYnGNhMHWGMMatqZze+H/hKs0szHvDfVfUbIvIs8CUR+QRwBvi1tRumMeZmXTfsrc4v965y/zTwyFoMyhhz69kKOmNiwsJuTExY2I2JCQu7MTFhYTcmJizsxsSEhd2YmLCwGxMTFnZjYsLCbkxMWNiNiQkLuzExYWE3JiYs7MbEhIXdmJiwsBsTExZ2Y2KirbCLSKeIfFlEjojI6yLybhHpEpEnReRY63NxrQdrjHnr2t2yfwb4hqreTrNE1etYRxhjtpR2qssWgJ8BPg+gqnVVncM6whizpbSzZd8LTAL/RUReFJE/bZWUto4wxmwh7YTdA94BfFZV3w6UuWKXXVWVZouoN1HVz6nq/ap6v0/yZsdrjHmL2gn7CDCiqk+3bn+ZZvitI4wxW8h1w66qY8A5ETnUuusR4DDWEcaYLaXdxo7/DPhzEUkAJ4F/QPM/CusIY8wW0VbYVfUl4P5VHrKOMMZsEbaCzpiYsLAbExMWdmNiwsJuTExY2I2JCQu7MTFhYTcmJizsxsSEhd2YmLCwGxMTFnZjYsLCbkxMWNiNiQkLuzExYWE3JiYs7MbERDulpA+JyEvLPhZE5HetSYQxW0s7NejeUNX7VPU+4J1ABfgK1iTCmC3lRnfjHwFOqOoZrEmEMVvKjYb914G/aH3dVpMIY8zm0HbYW5VlfwH4yysfu1aTCOsIY8zmcCNb9g8BL6jqeOt2W00irCOMMZvDjYT9Y1zehQdrEmHMltJuf/Ys8AHgr5fd/QfAB0TkGPDzrdvGmE2q3SYRZaD7ivumsSYRxmwZtoLOmJiwsBsTExZ2Y2LCwm5MTFjYjYkJC7sxMWFhNyYmLOzGxISF3ZiYsLAbExMWdmNiwsJuTExY2I2JCQu7MTFhYTcmJizsxsSEhd2YmGi3LNW/EJHXRORVEfkLEUmJyF4ReVpEjovIF1vVZ40xm1Q77Z92AP8cuF9V7wJcmvXj/xD4Y1U9AMwCn1jLgRpjbk67u/EekBYRD8gAF4D3A19uPW4dYYzZ5Nrp9TYK/EfgLM2QzwPPA3Oq2mg9bQTYsVaDNMbcvHZ244s0+7rtBYaALPBYuy9gHWGM2Rza2Y3/eeCUqk6qakCzdvx7gM7Wbj3ATmB0tW+2jjDGbA7thP0s8C4RyYiI0KwVfxj4NvArredYRxhjNrl2jtmfpjkR9wLwSut7Pgd8Cvg9ETlOs4HE59dwnMaYmyTNBqzro0O69CGxJjLGrJWn9SkWdEZWe8xW0BkTExZ2Y2LCwm5MTFjYjYmJdZ2gE5FJoAxMrduLrr0e7P1sVtvpvUB772e3qvau9sC6hh1ARJ5T1fvX9UXXkL2fzWs7vRe4+fdju/HGxISF3ZiY2Iiwf24DXnMt2fvZvLbTe4GbfD/rfsxujNkYthtvTEysa9hF5DEReaNVt+7T6/naN0tEhkXk2yJyuFWP73da93eJyJMicqz1ubjRY70RIuKKyIsi8kTr9patLSginSLyZRE5IiKvi8i7t/Lv51bXfly3sIuIC/x/wIeAO4GPicid6/X6t0AD+JeqeifwLuCftMb/aeApVT0IPNW6vZX8DvD6sttbubbgZ4BvqOrtwL0039eW/P2sSe1HVV2XD+DdwP9advv3gd9fr9dfg/fzVeADwBvAYOu+QeCNjR7bDbyHnTQD8H7gCUBoLtrwVvudbeYPoACcojUPtez+Lfn7oVnm7RzQRbMG5BPAB2/m97Oeu/EXB3/Rlq1bJyJ7gLcDTwP9qnqh9dAY0L9Bw3or/gT4V0DUut3N1q0tuBeYBP5L67DkT0Ukyxb9/ega1H60CbobJCI54K+A31XVheWPafO/2y1xekNEPgpMqOrzGz2WW8QD3gF8VlXfTnNZ9opd9i32+7mp2o+rWc+wjwLDy25ftW7dZiUiPs2g/7mq/nXr7nERGWw9PghMbNT4btB7gF8QkdPA4zR35T9Dm7UFN6ERYESblZWgWV3pHWzd389N1X5czXqG/VngYGs2MUFzsuFr6/j6N6VVf+/zwOuq+kfLHvoazRp8sIVq8anq76vqTlXdQ/N38Teq+nG2aG1BVR0DzonIodZdF2slbsnfD2tR+3GdJx0+DBwFTgD/90ZPgtzg2N9Lcxfwp8BLrY8P0zzOfQo4BnwL6Nrosb6F9/Y+4InW1/uAZ4DjwF8CyY0e3w28j/uA51q/o/8BFLfy7wf498AR4FXg/weSN/P7sRV0xsSETdAZExMWdmNiwsJuTExY2I2JCQu7MTFhYTcmJizsxsSEhd2YmPjfdySdqIPaibAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = class_restoration(train_dataset, 456)\n",
        "\n",
        "figure = plt.figure(figsize=(10, 8))\n",
        "columns =25\n",
        "rows = 4\n",
        "for i in range(1, columns * rows ):\n",
        "    figure.add_subplot(rows, columns, i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img[i], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jGoJL-QecUQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(16, 32, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = self.out(x)\n",
        "        return output, x    # return x for visualization"
      ],
      "metadata": {
        "id": "YQMNqwCWPEcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. What preprocessing techniques did you use? Why?\n",
        "    - *Answer*\n",
        "2. What data augmentation techniques did you use?\n",
        "    - *Answer*\n",
        "3. Describe the fine-tuning process and how you reached your final CNN model.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0ZplsaeMXlpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Decision Trees and Ensemble Learning (15%)\n",
        "\n",
        "For the `loan_data.csv` data, predict if the bank should give a loan or not.\n",
        "You need to do the following:\n",
        "- Fine-tune a decision tree on the data\n",
        "- Fine-tune a random forest on the data\n",
        "- Compare their performance\n",
        "- Visualize your DT and one of the trees from the RF\n",
        "\n",
        "For evaluating your models, do $80/20$ train test split.\n",
        "\n",
        "### Data\n",
        "- `credit.policy`: Whether the customer meets the credit underwriting criteria.\n",
        "- `purpose`: The purpose of the loan.\n",
        "- `int.rate`: The interest rate of the loan.\n",
        "- `installment`: The monthly installments owed by the borrower if the loan is funded.\n",
        "- `log.annual.inc`: The natural logarithm of the self-reported annual income of the borrower.\n",
        "- `dti`: The debt-to-income ratio of the borrower.\n",
        "- `fico`: The FICO credit score of the borrower.\n",
        "- `days.with.cr.line`: The number of days the borrower has had a credit line.\n",
        "- `revol.bal`: The borrower's revolving balance.\n",
        "- `revol.util`: The borrower's revolving line utilization rate."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Qq127Lo8XlpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_text\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "z_CPIM9IXlpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv('loan_data.csv')\n",
        "\n",
        "encoder = OrdinalEncoder()\n",
        "df['purpose'] = encoder.fit_transform(df['purpose'].values.reshape(-1,1))\n",
        "\n",
        "y = df.iloc[:,0]\n",
        "X = df.iloc[:, 1:]\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(criterion = \"entropy\")\n",
        "dt_clf = dt_clf.fit(train_x, train_y)\n",
        "\n",
        "\n",
        "score = dt_clf.score(test_x, test_y)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ripMiU197wYU",
        "outputId": "853855c9-1782-457d-94ba-5ffeb5e7e0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9900835073068893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. How did the DT compare to the RF in performance? Why?\n",
        "    - *Answer*\n",
        "2. After fine-tuning, how does the max depth in DT compare to RF? Why?\n",
        "    - *Answer*\n",
        "3. What is ensemble learning? What are its pros and cons?\n",
        "    - *Answer*\n",
        "4. Briefly explain 2 types of boosting methods and 2 types of bagging methods.\n",
        "Which of these categories does RF fall under?\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "id": "h0KhDTFrXlpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Domain Gap (15%)\n",
        "\n",
        "Evaluate your CNN model from task 2 on SVHN data without retraining your model."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "R4s3vCAQXlpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: Implement task 4"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ioGoCkTyXlpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. How did your model perform? Why is it better/worse?\n",
        "    - *Answer*\n",
        "2. What is domain gap in the context of ML?\n",
        "    - *Answer*\n",
        "3. Suggest two ways through which the problem of domain gap can be tackled.\n",
        "    - *Answer*"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WHrIi5V9XlpH"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}